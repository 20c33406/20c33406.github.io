(function(){function r(e,n,t){function o(i,f){if(!n[i]){if(!e[i]){var c="function"==typeof require&&require;if(!f&&c)return c(i,!0);if(u)return u(i,!0);var a=new Error("Cannot find module '"+i+"'");throw a.code="MODULE_NOT_FOUND",a}var p=n[i]={exports:{}};e[i][0].call(p.exports,function(r){var n=e[i][1][r];return o(n||r)},p,p.exports,r,e,n,t)}return n[i].exports}for(var u="function"==typeof require&&require,i=0;i<t.length;i++)o(t[i]);return o}return r})()({1:[function(require,module,exports){

},{}],2:[function(require,module,exports){




class space {
        constructor(id, smallid, bigid, player, available){
            this.id = id
            this.smallid = smallid
            this.bigid = bigid
            this.player = player
            this.available = available
            this.lastPlaced = false
        }
    }
class Game {

    constructor(){
        this.on = true;
        this.playerX = 1;
        this.playerO = 2;
        this.board = Array(9).fill(null).map(() => Array(9).fill(0));
        this.winningCombos = [
        [0, 1, 2],
        [3, 4, 5],
        [6, 7, 8],
        [0, 3, 6],
        [1, 4, 7],
        [2, 5, 8],
        [0, 4, 8],
        [2, 4, 6]
        ];
        let k = 0
        for(let i=0;i<9;i++){
            for(let j=0;j<9;j++){
                this.board[i][j] = new space(k,j,i,0,true)
                k++
            }
        }
    }
    
    getAvailableIds(){
        let ids = []
        for(let i=0;i<9;i++){
            for(let j=0;j<9;j++){
                if(this.board[i][j].available == true){
                    ids.push(this.board[i][j].id)
                }
            }
        }
        return ids
    }



     
    displayBoard = () => {
        let state = []
        for(let i=0;i<9;i+=3){ 
          for(let j=0;j<9;j+=3){
          state.push([this.board[i][0+j].player,this.board[i][1+j].player,this.board[i][2+j].player,this.board[i+1][0+j].player,this.board[i+1][1+j].player,this.board[i+1][2+j].player,this.board[i+2][0+j].player,this.board[i+2][1+j].player,this.board[i+2][2+j].player])
        }}
        
        for(let i=0;i<9;i++){
          for(let j=0;j<9;j++){
            if(state[i][j]==1){
              state[i][j] = "X"
            } else if(state[i][j]==2){
              state[i][j] = "O"
            } else {
              state[i][j] = "-"
              
            }
        }
        
        }
        for(let i=0;i<9;i++){
          console.log(state[i][0],state[i][1],state[i][2],state[i][3],state[i][4],state[i][5],state[i][6],state[i][7],state[i][8],)
        }
    }
    checkBoard = () => {
        let state = []
        for(let i=0;i<9;i+=3){ 
          for(let j=0;j<9;j+=3){
          state.push([this.board[i][0+j].smallid,this.board[i][1+j].smallid,this.board[i][2+j].smallid,this.board[i+1][0+j].smallid,this.board[i+1][1+j].smallid,this.board[i+1][2+j].smallid,this.board[i+2][0+j].smallid,this.board[i+2][1+j].smallid,this.board[i+2][2+j].smallid])
        }}
  
        
        
        for(let i=0;i<9;i++){
          console.log(state[i][0],state[i][1],state[i][2],state[i][3],state[i][4],state[i][5],state[i][6],state[i][7],state[i][8],)
        }
    }


    

    getSpaceById = (e) => {
        for(let i=0;i<9;i++){
            for(let j=0;j<9;j++){
                if(this.board[i][j].id == e){
                    
                    return this.board[i][j]
                }
            }
        }
    }

    

    highlightAll = () => {
        for (let j = 0; j < 81; j++) {
            this.getSpaceById(j).available = true
        }

    }

    clearAll = () => {
        for (let j = 0; j < 81; j++) {
            this.getSpaceById(j).available = false
        }

    }



    markSquareAsWon = (playerId, big) => {
        
        for (let i = 0; i < 81; i++) {
            let tspace = this.getSpaceById(i)

            if(tspace.bigid == big){
                tspace.player = playerId
            }
        }
        

        
    };

    highlightAvailableMoves = (id) => {
        
        this.clearAll()
        for(let i=0;i<81;i++){
            let lastspace = this.getSpaceById(id)
            let newspace = this.getSpaceById(i)
    
            if((newspace.player==0) && (lastspace.smallid == newspace.bigid || this.board[lastspace.smallid].every(val => val.player !== 0))){
                newspace.available=true
                
            }
        }
        

            

        
    };
    checkForWin = () => {
        let winIndicator = 0
        this.winningCombos.some(([a, b, c]) => {
            if(this.board[a].every((val, idx) => val.player !== 0 && val.player === this.board[b][idx].player && val.player === this.board[c][idx].player)){
                winIndicator = (this.board[a][0].player)
            }
        });
        if(winIndicator){
            return winIndicator
        } else if(this.boardIsFilled()) {
            return -1
        }   else {
            return 0
        }
    };

    playerHasWonSquare = (num) => {
        let winIndicator= this.winningCombos.some(([a, b, c]) => {
            if(this.board[num][a].player!=0 && this.board[num][a].player === this.board[num][b].player && this.board[num][a].player === this.board[num][c].player){
                return(this.board[num][a].player)
            }
        });
        if(winIndicator){
            return winIndicator
        }   else {
            return false
        }
    };

    boardIsFilled = () => {
      
      for(let i=0;i<9;i++){
        if(!this.isFilled(i)){
          return false
        }
      }
      return true
    }

    isFilled = (num) => {
      return this.board[num].every(val => val.player !== 0);

    };

    chooseSpace = (playerId, tempid) => {
        
        
        let tempspace = this.getSpaceById(tempid)
   


        if (tempspace.available) {

  
            tempspace.player = playerId

            if (this.playerHasWonSquare(tempspace.bigid)) {
                
                this.markSquareAsWon(playerId, tempspace.bigid);
              
                
            }
            this.highlightAvailableMoves(tempspace.id);
        }
        return false
   
    }

    get1DArrayFormatted(playerId){
        return this.board.reduce((array, line) => array.concat(
      line.map((cellValue) => {
        if (cellValue.player === 0) return 0;
        else if (cellValue.player === playerId) return 1;
        return -1;
      })
    ), []);
    };

    get1DArrayFiltered(playerId){
    // this function returns the board in a single array with
    // only the playerId chips appearing
    return this.board.reduce((array, line) => array.concat(
      line.map((cellValue) => {
        if (cellValue.player === playerId) return 1;
        else return 0;
      })
    ), []);
  }
    
    getConvolutionalVol(playerId){
    // this function aims to return a 3D array : 6*7*2 for the 2 players' chips
    // The first unit in the depth is the playerId game
    const opponentId = playerId === 1 ? 2 : 1;
    const vol = {
      sx: 9,
      sy: 9,
      depth: 2,
      w: new Float64Array(this.get1DArrayFiltered(playerId).concat(this.get1DArrayFiltered(opponentId))),
      dw: new Float64Array(9 * 9 * 2).fill(0),
    }
    return vol;
  }

  setNewLastSpace(num){
    for(let i=0;i<81;i++){
      this.getSpaceById(i).lastPlaced=false
      
    }
    this.getSpaceById(num).lastPlaced=true
  }
}




module.exports.Game = Game
},{}],3:[function(require,module,exports){
   
const noughtsAndCrosses = require('./Game');

exports.randomChoice = (choices) => {
  const index = Math.floor(Math.random() * choices.length);
  return choices[index];
}

exports.getArrayFromIndex = (columnIndex, value) => (
  new Array(7).fill(0).map((_, index) => (index === columnIndex ? value : 0))
)

exports.boardToConvolutionalVol = (board, playerId) => {
  const opponentId = playerId === 1 ? 2 : 1;
    const vol = {
      sx: 6,
      sy: 7,
      depth: 2,
      w: new Float64Array(boardTo1DArrayFiltered(board, playerId)
        .concat(boardTo1DArrayFiltered(board, opponentId))),
      dw: new Float64Array(6 * 7 * 2).fill(0),
    }
    return vol;
}

const boardTo1DArrayFiltered = (board, playerId) => {
  // this function returns the board in a single array with
  // only the playerId chips appearing
  return board.reduce((array, line) => array.concat(
    line.map((cellValue) => {
      if (cellValue === playerId) return 1;
      else return 0;
    })
  ), []);
}

exports.boardTo1DArrayFormatted = (board, playerId) => {
  return board.reduce((array, line) => array.concat(
    line.map((cellValue) => {
      if (cellValue === 0) return 0;
      else if (cellValue === playerId) return 1;
      return -1;
    })
  ), []);
}

exports.evaluateLearning = (network) => {
  const benchMark = [];
  const game1 = new noughtsAndCrosses.Game();
  game1.playChip(1, 1);
  game1.playChip(2, 0);
  game1.playChip(1, 2);
  game1.playChip(2, 6);
  game1.playChip(1, 3);
  game1.playChip(2, 1);
  benchMark.push(network.activate(game1.get1DArrayFormatted(1)));

  const game2 = new noughtsAndCrosses.Game();
  game2.playChip(2, 0);
  game2.playChip(1, 1);
  game2.playChip(2, 0);
  game2.playChip(1, 1);
  game2.playChip(2, 6);
  game2.playChip(1, 1);
  game2.playChip(2, 2);
  benchMark.push(network.activate(game2.get1DArrayFormatted(1)));

  const game3 = new noughtsAndCrosses.Game();
  game3.playChip(1, 0);
  game3.playChip(2, 0);
  game3.playChip(1, 1);
  game3.playChip(2, 6);
  game3.playChip(1, 6);
  game3.playChip(2, 2);
  game3.playChip(1, 3);
  game3.playChip(2, 1);
  game3.playChip(1, 1);
  game3.playChip(2, 3);
  game3.playChip(1, 2);
  game3.playChip(2, 0);
  benchMark.push(network.activate(game3.get1DArrayFormatted(1)));

  return benchMark;
}

exports.evaluateLearningCNN = (network) => {
  const benchMark = [];
  const game1 = new noughtsAndCrosses.Game();
  game1.chooseSpace(1, 1);
  game1.chooseSpace(2, 0);
  game1.chooseSpace(1, 2);
  game1.chooseSpace(2, 6);
  game1.chooseSpace(1, 3);
  game1.chooseSpace(2, 1);
  benchMark.push(network.forward(game1.getConvolutionalVol(1)).w);

  const game2 = new noughtsAndCrosses.Game();
  game2.chooseSpace(2, 0);
  game2.chooseSpace(1, 1);
  game2.chooseSpace(2, 0);
  game2.chooseSpace(1, 1);
  game2.chooseSpace(2, 6);
  game2.chooseSpace(1, 1);
  game2.chooseSpace(2, 2);
  benchMark.push(network.forward(game2.getConvolutionalVol(1)).w);

  const game3 = new noughtsAndCrosses.Game();
  game3.chooseSpace(1, 0);
  game3.chooseSpace(2, 0);
  game3.chooseSpace(1, 1);
  game3.chooseSpace(2, 6);
  game3.chooseSpace(1, 6);
  game3.chooseSpace(2, 2);
  game3.chooseSpace(1, 3);
  game3.chooseSpace(2, 1);
  game3.chooseSpace(1, 1);
  game3.chooseSpace(2, 3);
  game3.chooseSpace(1, 2);
  game3.chooseSpace(2, 0);
  benchMark.push(network.forward(game3.getConvolutionalVol(1)).w);

  return benchMark;
}
module.exports = exports;
},{"./Game":2}],4:[function(require,module,exports){
//      
// @flow
const convnetjs = require("convnetjs");
const deepqlearn = require('convnetjs/build/deepqlearn');
const Helper = require('./Helper');

const synaptic = require('synaptic');

exports.initialize = (networkType, config) => {
  let myNetwork
  
  if (networkType === 'CNN') {
    const filters_number = config[networkType]['filters_number'];
    const padding = config[networkType]['padding'];
    const stride = config[networkType]['stride'];
    const size = config[networkType]['size'];
    const layers = config[networkType]['layers'];
    
    const layer_defs = [];
    layer_defs.push({type:'input', out_sx:7, out_sy:6, out_depth:2});
    layer_defs.push({
      type: 'conv',
      sx: size,
      filters: filters_number,
      stride: stride,
      pad: padding,
      activation: 'relu'
    });
    for (let layer = 0; layer < layers.length; layer++) {
      layer_defs.push({
        type: 'fc',
        num_neurons: layers[layer],
        activation: 'relu'
      });
    }
    layer_defs.push({type:'regression', num_neurons:7});
    myNetwork = new convnetjs.Net();
    myNetwork.makeLayers(layer_defs);
  } else if (networkType === 'NN') {
    const layers = config[networkType]['layers'];

    const inputLayer = new synaptic.Layer(7 * 6);

    const hiddenLayers = [];
    for (let layer = 0; layer < layers.length; layer++) {
      hiddenLayers.push(new synaptic.Layer(layers[layer]));
      hiddenLayers[layer].set({
        squash: synaptic.Neuron.squash.RELU,
      });
    }

    const outputLayer = new synaptic.Layer(7);
    outputLayer.set({
      squash: synaptic.Neuron.squash.IDENTITY,
    });

    inputLayer.project(hiddenLayers[0]);
    for (let layer = 0; layer < layers.length - 1; layer++) {
      hiddenLayers[layer].project(hiddenLayers[layer + 1])
    }
    hiddenLayers[hiddenLayers.length - 1].project(outputLayer);

    myNetwork = new synaptic.Network({
      input: inputLayer,
      hidden: hiddenLayers,
      output: outputLayer,
    });
  }
  return myNetwork;
}

exports.getTrainer = (networkType, myNetwork) => {
  let trainer;
  if (networkType === 'CNN') {
    trainer = new convnetjs.Trainer(
      myNetwork,
      {
        method: 'sgd',
        learning_rate: 0.00001,
        momentum: 0.5,
        l2_decay: 0.001,
        l1_decay: 0.001,
        batch_size: 1,
      }
    );
  } else if (networkType === 'NN') {
    trainer = myNetwork;
  }
  return trainer;
}

exports.formatInput = (
  networkType,
  board,
  playerIdToPlay
) => {
  let formattedBoard;
  if (networkType === 'CNN') {
    formattedBoard = Helper.boardToConvolutionalVol(board, playerIdToPlay);
  } else if (networkType === 'NN') {
    formattedBoard = Helper.boardTo1DArrayFormatted(board, playerIdToPlay);
  }
  return formattedBoard;
}

exports.predict = (
  networkType,
  myNetwork,
  board,
) => {
  let output = [];
  let boardFormatted;
  if (networkType === 'CNN') {
    output = myNetwork.forward(board).w;
  } else if (networkType === 'NN') {
    output = myNetwork.activate(board);
  }
  return output;
}

exports.backPropagate = (
  networkType,
  trainer,
  board,
  reward,
  columnIndex,
  learningRate
) => {
  const outputArray = Helper.getArrayFromIndex(columnIndex, reward);
  if (networkType === 'CNN') {
    trainer.learning_rate = learningRate;
    trainer.train(board, outputArray);
  } else if (networkType === 'NN') {
    trainer.activate(board);
    trainer.propagate(learningRate, outputArray);
  }
}

exports.evaluate = (
  networkType,
  myNetwork,
) => {
  if (networkType === 'CNN') {
    return Helper.evaluateLearningCNN(myNetwork);
  } else if (networkType === 'NN') {
    return Helper.evaluateLearning(myNetwork);
  }
}

},{"./Helper":3,"convnetjs":9,"convnetjs/build/deepqlearn":10,"synaptic":11}],5:[function(require,module,exports){
//      
const noughtsAndCrosses = require('./Game');
const Helper = require('./Helper');
const NeuralNetwork = require('./NeuralNetwork');

exports.playGame = (networkType, epsilon, myNetwork, display) => {
  let winnerBoardStates = [];
  let winnerPlays = [];
  let loserBoardStates = [];
  let loserPlays = [];
  const game = new noughtsAndCrosses.Game();
  const boardStatesAsPlayer1 = [];
  const boardStatesAsPlayer2 = [];
  const playsAsPlayer1 = [];
  const playsAsPlayer2 = [];

  let playerIdToPlay = 1;
  let pat = false;
  let winner = 0;
  while (!pat && !winner) {
    // Play
    const explore = !(Math.random() < epsilon);
    let spaceId;
    let output = [];
    let formattedBoard = NeuralNetwork.formatInput(networkType, game.board, playerIdToPlay);
    if (!explore) {
      
      output = NeuralNetwork.predict(networkType, myNetwork, formattedBoard);
      spaceId = output.indexOf(Math.max(...output));
    } else {
      
      spaceId = Helper.randomChoice(game.getAvailableIds());
    }

    let playAgain = game.chooseSpace(playerIdToPlay, spaceId);
    if(playsAsPlayer1.includes(spaceId) || playsAsPlayer2.includes(spaceId)){playAgain=true};
    // The same player may have to play again if the column he chose was full
    if (!playAgain) {
      // Save board states and plays
      if (playerIdToPlay === 1) {
        boardStatesAsPlayer1.push(formattedBoard);
        playsAsPlayer1.push(spaceId);
      } else if (playerIdToPlay === 2) {
        boardStatesAsPlayer2.push(formattedBoard);
        playsAsPlayer2.push(spaceId);
      }
      
      // Check for wins
      const gameState = game.checkForWin();
      /*
      game.displayBoard()
      console.log(gameState)
      console.log(game.getSpaceById(spaceId).bigid,game.getSpaceById(spaceId).smallid, spaceId, playerIdToPlay)
      console.log("------------------------")
      */
      switch (gameState) {
        case 0:
          // Nobody won, switch player
          playerIdToPlay = playerIdToPlay === 1 ? 2 : 1;
          break;
        case -1:
          // Pat
          pat = true;
          break;
        case 1:
          // Player 1 won
          winner = 1;
          break;
        case 2:
          // Player 2 won
          winner = 2;
          break;
        default:
          break;
      }
    } else {
      
    }
  }
  if (winner > 0) {
    winnerBoardStates = winner === 1 ? boardStatesAsPlayer1 : boardStatesAsPlayer2;
    winnerPlays = winner === 1 ? playsAsPlayer1 : playsAsPlayer2;
    loserBoardStates = winner === 1 ? boardStatesAsPlayer2 : boardStatesAsPlayer1;
    loserPlays = winner === 1 ? playsAsPlayer2 : playsAsPlayer1;
  }
  if (display) game.displayBoard();
  return {
    winnerBoardStates,
    winnerPlays,
    loserBoardStates,
    loserPlays,
  }
}
},{"./Game":2,"./Helper":3,"./NeuralNetwork":4}],6:[function(require,module,exports){

const NeuralNetwork = require('./NeuralNetwork');


exports.trainOnPreviousPlays = (
  networkType,
  myNetwork,
  myTrainer,
  boards,
  plays,
  learningRate,
  reward,
  discount,
  gamma,
) => {
  const playsLength = plays.length;
  let previousQValue = NeuralNetwork.predict(
    networkType,
    myNetwork,
    boards[playsLength - 1],
  );
  
  // backpropagate on the previous winnerPlays
  for (let playIndex = playsLength - 2; playIndex >= 0; playIndex--) {
    NeuralNetwork.backPropagate(
      networkType,
      myTrainer,
      boards[playIndex],
      discount ** (playsLength - playIndex - 1) * reward + gamma * Math.max(...previousQValue),
      plays[playIndex],
      learningRate
    )
    previousQValue = NeuralNetwork.predict(
      networkType,
      myNetwork,
      boards[playIndex],
    );
  }
}
},{"./NeuralNetwork":4}],7:[function(require,module,exports){
// @flow



const noughtsAndCrosses = require('./Game');
const Helper = require('./Helper');
const QLearning = require('./QLearning');
const Play = require('./Play')
const NeuralNetwork = require('./NeuralNetwork');
const fs = require('fs');

// -------------- PARAMETERS ---------------- //
// type of neural network to train
const NETWORK_TYPE = 'CNN';
const networkType = 'CNN';
// number of games to play
const LEARN_TIMES = 1000;
// learningRate is progressively decreased with the number of games until
// the final value LR_INIT/LR_FINAL_FRACTION
const LR_INIT = 0.0001;
const LR_FINAL_FRACTION = 10;
// epsilon is the ratio between exploration and exploitation
// it can evolve along the games played
const EPSILON_INIT = 0.1;
const EPSILON_FINAL = 0.4;
// gamma is the fraction attributed to the maximum Q Value of the next state
const GAMMA = 0.3;
// reward awarded to the final play that led to victory
const REWARD = 100;
// discount applied to the reward awarded to the previous plays that led to victory
const DISCOUNT = 0.8;
// reward (or - reward) awarded to prevent the bot to lose
const SIDE_REWARD = 75;
// network configuration
const NETWORK_CONFIG = {
  CNN: {
    filters_number: 32,
    padding: 2,
    stride: 1,
    size: 4,
    layers : [60, 30]
  },
  NN: {
    layers: [100]
  }
}
// ------------------------------------------ //

const myNetwork = NeuralNetwork.initialize(NETWORK_TYPE, NETWORK_CONFIG);
const epsilon = EPSILON_INIT + (EPSILON_FINAL - EPSILON_INIT) * 1/2;


const chooseMove = (playerIdToPlay, availableMoves, board) => {
      const explore = !(Math.random() < epsilon);
      let spaceId;
      let output = [];
      let formattedBoard = NeuralNetwork.formatInput(networkType, board, playerIdToPlay);
      if (!explore) {
        
        output = NeuralNetwork.predict(networkType, myNetwork, formattedBoard);
        spaceId = output.indexOf(Math.max(...output));
      } else {
        
        spaceId = Helper.randomChoice(availableMoves);
      }
      return spaceId
} 


module.exports.chooseMove = chooseMove
},{"./Game":2,"./Helper":3,"./NeuralNetwork":4,"./Play":5,"./QLearning":6,"fs":1}],8:[function(require,module,exports){
const noughtsAndCrosses = require("./aijs/Game")
const bot = require("./aijs/bot")
const NeuralNetwork = require("./aijs/NeuralNetwork")

const X_TEXT = "X"
const O_TEXT = "O"

const X_COLOUR = 'DodgerBlue'
const O_COLOUR = 'orange'

const playerText = X_TEXT
const botText = O_TEXT

const playerColour = X_COLOUR
const botColour = O_COLOUR

let thinger = document.getElementById('thinger');
let restartBtn = document.getElementById('restartBtn');

let boxes = Array.from(document.getElementsByClassName('box'));

let playsAsPlayer1 = []
let playsAsPlayer2 = []
let currentPlayer = 1


let pat = false;
let winner = 0;

let on = true


const swapSign = () => {
    playerText = playerText==X_TEXT ? O_TEXT : X_TEXT
    playerColour = playerColour==X_COLOUR ? O_COLOUR : X_COLOUR

    botText = botText==X_TEXT ? O_TEXT : X_TEXT
    botColour = botColour==X_COLOUR ? O_COLOUR : X_COLOUR


}

const update = () => {
    for(let i=0;i<81;i++){
            item=game.getSpaceById(i)
            check = document.getElementById(i.toString())
            
            if(item.available){
                check.style.backgroundColor = "#223444";
                check.classList.add('available');
                
            } else {
                check.style.backgroundColor = "";
                check.classList.remove('available');
            }
            
            if(item.player==1){
                check.style.color = playerColour
                check.innerText = playerText
            } else if(item.player==2){
                 check.style.color = botColour
                 check.innerText = botText
            }

            if(item.lastPlaced){
                check.style.backgroundColor = "#FF0152";
                
                
            } else if(item.available){
                check.style.backgroundColor = "#223444";
                check.classList.add('available');
                
            } else {
                check.style.backgroundColor = "";
                check.classList.remove('available');
            }
            
    }
}



const  boxClicked = (e) => {
    
    if (!on || currentPlayer==2 || game.getSpaceById(e.target.id).player != 0 || !game.getSpaceById(e.target.id).available) return;
    restartBtn.innerText = 'Restart Game';
    tempid = parseInt(e.target.id);
    game.chooseSpace(1,tempid)
    playsAsPlayer1.push(tempid)
    botPlace()
    update()
}

const botPlace = () => {
  let spaceId = bot.chooseMove(2, game.getAvailableIds(), game.board)
  if(game.getSpaceById(spaceId).player != 0 || !game.getSpaceById(spaceId).available){return botPlace()}
  
  game.setNewLastSpace(spaceId)
  let playAgain = game.chooseSpace(2, spaceId);
  if(!playsAsPlayer1.includes(spaceId) && !playsAsPlayer2.includes(spaceId)){
    // The same player may have to play again if the column he chose was full
  
      // Save board states and plays
      

      
   
      playsAsPlayer2.push(spaceId);
      
      
      // Check for wins
      const gameState = game.checkForWin();
      /*
      game.displayBoard()
      console.log(gameState)
      console.log(game.getSpaceById(spaceId).bigid,game.getSpaceById(spaceId).smallid, spaceId, playerIdToPlay)
      console.log("------------------------")
      */

      
      switch (gameState) {
        
          // Nobody won, switch player
          
          
        case -1:
          // Pat
          pat = true;
          on = false
        case 1:
          // Player 1 won
          winner = 1;
          on = false
        case 2:
          // Player 2 won
          winner = 2;
          on = false
        default:
          break;
      }
    } 
}


boxes.forEach(box => box.addEventListener('click', boxClicked));

const game = new noughtsAndCrosses.Game();

update();

},{"./aijs/Game":2,"./aijs/NeuralNetwork":4,"./aijs/bot":7}],9:[function(require,module,exports){
var convnetjs = convnetjs || { REVISION: 'ALPHA' };
(function(global) {
  "use strict";

  // Random number utilities
  var return_v = false;
  var v_val = 0.0;
  var gaussRandom = function() {
    if(return_v) { 
      return_v = false;
      return v_val; 
    }
    var u = 2*Math.random()-1;
    var v = 2*Math.random()-1;
    var r = u*u + v*v;
    if(r == 0 || r > 1) return gaussRandom();
    var c = Math.sqrt(-2*Math.log(r)/r);
    v_val = v*c; // cache this
    return_v = true;
    return u*c;
  }
  var randf = function(a, b) { return Math.random()*(b-a)+a; }
  var randi = function(a, b) { return Math.floor(Math.random()*(b-a)+a); }
  var randn = function(mu, std){ return mu+gaussRandom()*std; }

  // Array utilities
  var zeros = function(n) {
    if(typeof(n)==='undefined' || isNaN(n)) { return []; }
    if(typeof ArrayBuffer === 'undefined') {
      // lacking browser support
      var arr = new Array(n);
      for(var i=0;i<n;i++) { arr[i]= 0; }
      return arr;
    } else {
      return new Float64Array(n);
    }
  }

  var arrContains = function(arr, elt) {
    for(var i=0,n=arr.length;i<n;i++) {
      if(arr[i]===elt) return true;
    }
    return false;
  }

  var arrUnique = function(arr) {
    var b = [];
    for(var i=0,n=arr.length;i<n;i++) {
      if(!arrContains(b, arr[i])) {
        b.push(arr[i]);
      }
    }
    return b;
  }

  // return max and min of a given non-empty array.
  var maxmin = function(w) {
    if(w.length === 0) { return {}; } // ... ;s
    var maxv = w[0];
    var minv = w[0];
    var maxi = 0;
    var mini = 0;
    var n = w.length;
    for(var i=1;i<n;i++) {
      if(w[i] > maxv) { maxv = w[i]; maxi = i; } 
      if(w[i] < minv) { minv = w[i]; mini = i; } 
    }
    return {maxi: maxi, maxv: maxv, mini: mini, minv: minv, dv:maxv-minv};
  }

  // create random permutation of numbers, in range [0...n-1]
  var randperm = function(n) {
    var i = n,
        j = 0,
        temp;
    var array = [];
    for(var q=0;q<n;q++)array[q]=q;
    while (i--) {
        j = Math.floor(Math.random() * (i+1));
        temp = array[i];
        array[i] = array[j];
        array[j] = temp;
    }
    return array;
  }

  // sample from list lst according to probabilities in list probs
  // the two lists are of same size, and probs adds up to 1
  var weightedSample = function(lst, probs) {
    var p = randf(0, 1.0);
    var cumprob = 0.0;
    for(var k=0,n=lst.length;k<n;k++) {
      cumprob += probs[k];
      if(p < cumprob) { return lst[k]; }
    }
  }

  // syntactic sugar function for getting default parameter values
  var getopt = function(opt, field_name, default_value) {
    return typeof opt[field_name] !== 'undefined' ? opt[field_name] : default_value;
  }

  global.randf = randf;
  global.randi = randi;
  global.randn = randn;
  global.zeros = zeros;
  global.maxmin = maxmin;
  global.randperm = randperm;
  global.weightedSample = weightedSample;
  global.arrUnique = arrUnique;
  global.arrContains = arrContains;
  global.getopt = getopt;
  
})(convnetjs);
(function(global) {
  "use strict";

  // Vol is the basic building block of all data in a net.
  // it is essentially just a 3D volume of numbers, with a
  // width (sx), height (sy), and depth (depth).
  // it is used to hold data for all filters, all volumes,
  // all weights, and also stores all gradients w.r.t. 
  // the data. c is optionally a value to initialize the volume
  // with. If c is missing, fills the Vol with random numbers.
  var Vol = function(sx, sy, depth, c) {
    // this is how you check if a variable is an array. Oh, Javascript :)
    if(Object.prototype.toString.call(sx) === '[object Array]') {
      // we were given a list in sx, assume 1D volume and fill it up
      this.sx = 1;
      this.sy = 1;
      this.depth = sx.length;
      // we have to do the following copy because we want to use
      // fast typed arrays, not an ordinary javascript array
      this.w = global.zeros(this.depth);
      this.dw = global.zeros(this.depth);
      for(var i=0;i<this.depth;i++) {
        this.w[i] = sx[i];
      }
    } else {
      // we were given dimensions of the vol
      this.sx = sx;
      this.sy = sy;
      this.depth = depth;
      var n = sx*sy*depth;
      this.w = global.zeros(n);
      this.dw = global.zeros(n);
      if(typeof c === 'undefined') {
        // weight normalization is done to equalize the output
        // variance of every neuron, otherwise neurons with a lot
        // of incoming connections have outputs of larger variance
        var scale = Math.sqrt(1.0/(sx*sy*depth));
        for(var i=0;i<n;i++) { 
          this.w[i] = global.randn(0.0, scale);
        }
      } else {
        for(var i=0;i<n;i++) { 
          this.w[i] = c;
        }
      }
    }
  }

  Vol.prototype = {
    get: function(x, y, d) { 
      var ix=((this.sx * y)+x)*this.depth+d;
      return this.w[ix];
    },
    set: function(x, y, d, v) { 
      var ix=((this.sx * y)+x)*this.depth+d;
      this.w[ix] = v; 
    },
    add: function(x, y, d, v) { 
      var ix=((this.sx * y)+x)*this.depth+d;
      this.w[ix] += v; 
    },
    get_grad: function(x, y, d) { 
      var ix = ((this.sx * y)+x)*this.depth+d;
      return this.dw[ix]; 
    },
    set_grad: function(x, y, d, v) { 
      var ix = ((this.sx * y)+x)*this.depth+d;
      this.dw[ix] = v; 
    },
    add_grad: function(x, y, d, v) { 
      var ix = ((this.sx * y)+x)*this.depth+d;
      this.dw[ix] += v; 
    },
    cloneAndZero: function() { return new Vol(this.sx, this.sy, this.depth, 0.0)},
    clone: function() {
      var V = new Vol(this.sx, this.sy, this.depth, 0.0);
      var n = this.w.length;
      for(var i=0;i<n;i++) { V.w[i] = this.w[i]; }
      return V;
    },
    addFrom: function(V) { for(var k=0;k<this.w.length;k++) { this.w[k] += V.w[k]; }},
    addFromScaled: function(V, a) { for(var k=0;k<this.w.length;k++) { this.w[k] += a*V.w[k]; }},
    setConst: function(a) { for(var k=0;k<this.w.length;k++) { this.w[k] = a; }},

    toJSON: function() {
      // todo: we may want to only save d most significant digits to save space
      var json = {}
      json.sx = this.sx; 
      json.sy = this.sy;
      json.depth = this.depth;
      json.w = this.w;
      return json;
      // we wont back up gradients to save space
    },
    fromJSON: function(json) {
      this.sx = json.sx;
      this.sy = json.sy;
      this.depth = json.depth;

      var n = this.sx*this.sy*this.depth;
      this.w = global.zeros(n);
      this.dw = global.zeros(n);
      // copy over the elements.
      for(var i=0;i<n;i++) {
        this.w[i] = json.w[i];
      }
    }
  }

  global.Vol = Vol;
})(convnetjs);
(function(global) {
  "use strict";
  var Vol = global.Vol; // convenience

  // Volume utilities
  // intended for use with data augmentation
  // crop is the size of output
  // dx,dy are offset wrt incoming volume, of the shift
  // fliplr is boolean on whether we also want to flip left<->right
  var augment = function(V, crop, dx, dy, fliplr) {
    // note assumes square outputs of size crop x crop
    if(typeof(fliplr)==='undefined') var fliplr = false;
    if(typeof(dx)==='undefined') var dx = global.randi(0, V.sx - crop);
    if(typeof(dy)==='undefined') var dy = global.randi(0, V.sy - crop);
    
    // randomly sample a crop in the input volume
    var W;
    if(crop !== V.sx || dx!==0 || dy!==0) {
      W = new Vol(crop, crop, V.depth, 0.0);
      for(var x=0;x<crop;x++) {
        for(var y=0;y<crop;y++) {
          if(x+dx<0 || x+dx>=V.sx || y+dy<0 || y+dy>=V.sy) continue; // oob
          for(var d=0;d<V.depth;d++) {
           W.set(x,y,d,V.get(x+dx,y+dy,d)); // copy data over
          }
        }
      }
    } else {
      W = V;
    }

    if(fliplr) {
      // flip volume horziontally
      var W2 = W.cloneAndZero();
      for(var x=0;x<W.sx;x++) {
        for(var y=0;y<W.sy;y++) {
          for(var d=0;d<W.depth;d++) {
           W2.set(x,y,d,W.get(W.sx - x - 1,y,d)); // copy data over
          }
        }
      }
      W = W2; //swap
    }
    return W;
  }

  // img is a DOM element that contains a loaded image
  // returns a Vol of size (W, H, 4). 4 is for RGBA
  var img_to_vol = function(img, convert_grayscale) {

    if(typeof(convert_grayscale)==='undefined') var convert_grayscale = false;

    var canvas = document.createElement('canvas');
    canvas.width = img.width;
    canvas.height = img.height;
    var ctx = canvas.getContext("2d");

    // due to a Firefox bug
    try {
      ctx.drawImage(img, 0, 0);
    } catch (e) {
      if (e.name === "NS_ERROR_NOT_AVAILABLE") {
        // sometimes happens, lets just abort
        return false;
      } else {
        throw e;
      }
    }

    try {
      var img_data = ctx.getImageData(0, 0, canvas.width, canvas.height);
    } catch (e) {
      if(e.name === 'IndexSizeError') {
        return false; // not sure what causes this sometimes but okay abort
      } else {
        throw e;
      }
    }

    // prepare the input: get pixels and normalize them
    var p = img_data.data;
    var W = img.width;
    var H = img.height;
    var pv = []
    for(var i=0;i<p.length;i++) {
      pv.push(p[i]/255.0-0.5); // normalize image pixels to [-0.5, 0.5]
    }
    var x = new Vol(W, H, 4, 0.0); //input volume (image)
    x.w = pv;

    if(convert_grayscale) {
      // flatten into depth=1 array
      var x1 = new Vol(W, H, 1, 0.0);
      for(var i=0;i<W;i++) {
        for(var j=0;j<H;j++) {
          x1.set(i,j,0,x.get(i,j,0));
        }
      }
      x = x1;
    }

    return x;
  }
  
  global.augment = augment;
  global.img_to_vol = img_to_vol;

})(convnetjs);
(function(global) {
  "use strict";
  var Vol = global.Vol; // convenience

  // This file contains all layers that do dot products with input,
  // but usually in a different connectivity pattern and weight sharing
  // schemes: 
  // - FullyConn is fully connected dot products 
  // - ConvLayer does convolutions (so weight sharing spatially)
  // putting them together in one file because they are very similar
  var ConvLayer = function(opt) {
    var opt = opt || {};

    // required
    this.out_depth = opt.filters;
    this.sx = opt.sx; // filter size. Should be odd if possible, it's cleaner.
    this.in_depth = opt.in_depth;
    this.in_sx = opt.in_sx;
    this.in_sy = opt.in_sy;
    
    // optional
    this.sy = typeof opt.sy !== 'undefined' ? opt.sy : this.sx;
    this.stride = typeof opt.stride !== 'undefined' ? opt.stride : 1; // stride at which we apply filters to input volume
    this.pad = typeof opt.pad !== 'undefined' ? opt.pad : 0; // amount of 0 padding to add around borders of input volume
    this.l1_decay_mul = typeof opt.l1_decay_mul !== 'undefined' ? opt.l1_decay_mul : 0.0;
    this.l2_decay_mul = typeof opt.l2_decay_mul !== 'undefined' ? opt.l2_decay_mul : 1.0;

    // computed
    // note we are doing floor, so if the strided convolution of the filter doesnt fit into the input
    // volume exactly, the output volume will be trimmed and not contain the (incomplete) computed
    // final application.
    this.out_sx = Math.floor((this.in_sx + this.pad * 2 - this.sx) / this.stride + 1);
    this.out_sy = Math.floor((this.in_sy + this.pad * 2 - this.sy) / this.stride + 1);
    this.layer_type = 'conv';

    // initializations
    var bias = typeof opt.bias_pref !== 'undefined' ? opt.bias_pref : 0.0;
    this.filters = [];
    for(var i=0;i<this.out_depth;i++) { this.filters.push(new Vol(this.sx, this.sy, this.in_depth)); }
    this.biases = new Vol(1, 1, this.out_depth, bias);
  }
  ConvLayer.prototype = {
    forward: function(V, is_training) {
      this.in_act = V;

      var A = new Vol(this.out_sx, this.out_sy, this.out_depth, 0.0);
      for(var d=0;d<this.out_depth;d++) {
        var f = this.filters[d];
        var x = -this.pad;
        var y = -this.pad;
        for(var ax=0; ax<this.out_sx; x+=this.stride,ax++) {
          y = -this.pad;
          for(var ay=0; ay<this.out_sy; y+=this.stride,ay++) {

            // convolve centered at this particular location
            // could be bit more efficient, going for correctness first
            var a = 0.0;
            for(var fx=0;fx<f.sx;fx++) {
              for(var fy=0;fy<f.sy;fy++) {
                for(var fd=0;fd<f.depth;fd++) {
                  var oy = y+fy; // coordinates in the original input array coordinates
                  var ox = x+fx;
                  if(oy>=0 && oy<V.sy && ox>=0 && ox<V.sx) {
                    //a += f.get(fx, fy, fd) * V.get(ox, oy, fd);
                    // avoid function call overhead for efficiency, compromise modularity :(
                    a += f.w[((f.sx * fy)+fx)*f.depth+fd] * V.w[((V.sx * oy)+ox)*V.depth+fd];
                  }
                }
              }
            }
            a += this.biases.w[d];
            A.set(ax, ay, d, a);
          }
        }
      }
      this.out_act = A;
      return this.out_act;
    },
    backward: function() { 

      // compute gradient wrt weights, biases and input data
      var V = this.in_act;
      V.dw = global.zeros(V.w.length); // zero out gradient wrt bottom data, we're about to fill it
      for(var d=0;d<this.out_depth;d++) {
        var f = this.filters[d];
        var x = -this.pad;
        var y = -this.pad;
        for(var ax=0; ax<this.out_sx; x+=this.stride,ax++) {
          y = -this.pad;
          for(var ay=0; ay<this.out_sy; y+=this.stride,ay++) {
            // convolve and add up the gradients. 
            // could be more efficient, going for correctness first
            var chain_grad = this.out_act.get_grad(ax,ay,d); // gradient from above, from chain rule
            for(var fx=0;fx<f.sx;fx++) {
              for(var fy=0;fy<f.sy;fy++) {
                for(var fd=0;fd<f.depth;fd++) {
                  var oy = y+fy;
                  var ox = x+fx;
                  if(oy>=0 && oy<V.sy && ox>=0 && ox<V.sx) {
                    // forward prop calculated: a += f.get(fx, fy, fd) * V.get(ox, oy, fd);
                    //f.add_grad(fx, fy, fd, V.get(ox, oy, fd) * chain_grad);
                    //V.add_grad(ox, oy, fd, f.get(fx, fy, fd) * chain_grad);

                    // avoid function call overhead and use Vols directly for efficiency
                    var ix1 = ((V.sx * oy)+ox)*V.depth+fd;
                    var ix2 = ((f.sx * fy)+fx)*f.depth+fd;
                    f.dw[ix2] += V.w[ix1]*chain_grad;
                    V.dw[ix1] += f.w[ix2]*chain_grad;
                  }
                }
              }
            }
            this.biases.dw[d] += chain_grad;
          }
        }
      }
    },
    getParamsAndGrads: function() {
      var response = [];
      for(var i=0;i<this.out_depth;i++) {
        response.push({params: this.filters[i].w, grads: this.filters[i].dw, l2_decay_mul: this.l2_decay_mul, l1_decay_mul: this.l1_decay_mul});
      }
      response.push({params: this.biases.w, grads: this.biases.dw, l1_decay_mul: 0.0, l2_decay_mul: 0.0});
      return response;
    },
    toJSON: function() {
      var json = {};
      json.sx = this.sx; // filter size in x, y dims
      json.sy = this.sy;
      json.stride = this.stride;
      json.in_depth = this.in_depth;
      json.out_depth = this.out_depth;
      json.out_sx = this.out_sx;
      json.out_sy = this.out_sy;
      json.layer_type = this.layer_type;
      json.l1_decay_mul = this.l1_decay_mul;
      json.l2_decay_mul = this.l2_decay_mul;
      json.pad = this.pad;
      json.filters = [];
      for(var i=0;i<this.filters.length;i++) {
        json.filters.push(this.filters[i].toJSON());
      }
      json.biases = this.biases.toJSON();
      return json;
    },
    fromJSON: function(json) {
      this.out_depth = json.out_depth;
      this.out_sx = json.out_sx;
      this.out_sy = json.out_sy;
      this.layer_type = json.layer_type;
      this.sx = json.sx; // filter size in x, y dims
      this.sy = json.sy;
      this.stride = json.stride;
      this.in_depth = json.in_depth; // depth of input volume
      this.filters = [];
      this.l1_decay_mul = typeof json.l1_decay_mul !== 'undefined' ? json.l1_decay_mul : 1.0;
      this.l2_decay_mul = typeof json.l2_decay_mul !== 'undefined' ? json.l2_decay_mul : 1.0;
      this.pad = typeof json.pad !== 'undefined' ? json.pad : 0;
      for(var i=0;i<json.filters.length;i++) {
        var v = new Vol(0,0,0,0);
        v.fromJSON(json.filters[i]);
        this.filters.push(v);
      }
      this.biases = new Vol(0,0,0,0);
      this.biases.fromJSON(json.biases);
    }
  }

  var FullyConnLayer = function(opt) {
    var opt = opt || {};

    // required
    // ok fine we will allow 'filters' as the word as well
    this.out_depth = typeof opt.num_neurons !== 'undefined' ? opt.num_neurons : opt.filters;

    // optional 
    this.l1_decay_mul = typeof opt.l1_decay_mul !== 'undefined' ? opt.l1_decay_mul : 0.0;
    this.l2_decay_mul = typeof opt.l2_decay_mul !== 'undefined' ? opt.l2_decay_mul : 1.0;

    // computed
    this.num_inputs = opt.in_sx * opt.in_sy * opt.in_depth;
    this.out_sx = 1;
    this.out_sy = 1;
    this.layer_type = 'fc';

    // initializations
    var bias = typeof opt.bias_pref !== 'undefined' ? opt.bias_pref : 0.0;
    this.filters = [];
    for(var i=0;i<this.out_depth ;i++) { this.filters.push(new Vol(1, 1, this.num_inputs)); }
    this.biases = new Vol(1, 1, this.out_depth, bias);
  }

  FullyConnLayer.prototype = {
    forward: function(V, is_training) {
      this.in_act = V;
      var A = new Vol(1, 1, this.out_depth, 0.0);
      var Vw = V.w;
      for(var i=0;i<this.out_depth;i++) {
        var a = 0.0;
        var wi = this.filters[i].w;
        for(var d=0;d<this.num_inputs;d++) {
          a += Vw[d] * wi[d]; // for efficiency use Vols directly for now
        }
        a += this.biases.w[i];
        A.w[i] = a;
      }
      this.out_act = A;
      return this.out_act;
    },
    backward: function() {
      var V = this.in_act;
      V.dw = global.zeros(V.w.length); // zero out the gradient in input Vol
      
      // compute gradient wrt weights and data
      for(var i=0;i<this.out_depth;i++) {
        var tfi = this.filters[i];
        var chain_grad = this.out_act.dw[i];
        for(var d=0;d<this.num_inputs;d++) {
          V.dw[d] += tfi.w[d]*chain_grad; // grad wrt input data
          tfi.dw[d] += V.w[d]*chain_grad; // grad wrt params
        }
        this.biases.dw[i] += chain_grad;
      }
    },
    getParamsAndGrads: function() {
      var response = [];
      for(var i=0;i<this.out_depth;i++) {
        response.push({params: this.filters[i].w, grads: this.filters[i].dw, l1_decay_mul: this.l1_decay_mul, l2_decay_mul: this.l2_decay_mul});
      }
      response.push({params: this.biases.w, grads: this.biases.dw, l1_decay_mul: 0.0, l2_decay_mul: 0.0});
      return response;
    },
    toJSON: function() {
      var json = {};
      json.out_depth = this.out_depth;
      json.out_sx = this.out_sx;
      json.out_sy = this.out_sy;
      json.layer_type = this.layer_type;
      json.num_inputs = this.num_inputs;
      json.l1_decay_mul = this.l1_decay_mul;
      json.l2_decay_mul = this.l2_decay_mul;
      json.filters = [];
      for(var i=0;i<this.filters.length;i++) {
        json.filters.push(this.filters[i].toJSON());
      }
      json.biases = this.biases.toJSON();
      return json;
    },
    fromJSON: function(json) {
      this.out_depth = json.out_depth;
      this.out_sx = json.out_sx;
      this.out_sy = json.out_sy;
      this.layer_type = json.layer_type;
      this.num_inputs = json.num_inputs;
      this.l1_decay_mul = typeof json.l1_decay_mul !== 'undefined' ? json.l1_decay_mul : 1.0;
      this.l2_decay_mul = typeof json.l2_decay_mul !== 'undefined' ? json.l2_decay_mul : 1.0;
      this.filters = [];
      for(var i=0;i<json.filters.length;i++) {
        var v = new Vol(0,0,0,0);
        v.fromJSON(json.filters[i]);
        this.filters.push(v);
      }
      this.biases = new Vol(0,0,0,0);
      this.biases.fromJSON(json.biases);
    }
  }

  global.ConvLayer = ConvLayer;
  global.FullyConnLayer = FullyConnLayer;
  
})(convnetjs);
(function(global) {
  "use strict";
  var Vol = global.Vol; // convenience
  
  var PoolLayer = function(opt) {

    var opt = opt || {};

    // required
    this.sx = opt.sx; // filter size
    this.in_depth = opt.in_depth;
    this.in_sx = opt.in_sx;
    this.in_sy = opt.in_sy;

    // optional
    this.sy = typeof opt.sy !== 'undefined' ? opt.sy : this.sx;
    this.stride = typeof opt.stride !== 'undefined' ? opt.stride : 2;
    this.pad = typeof opt.pad !== 'undefined' ? opt.pad : 0; // amount of 0 padding to add around borders of input volume

    // computed
    this.out_depth = this.in_depth;
    this.out_sx = Math.floor((this.in_sx + this.pad * 2 - this.sx) / this.stride + 1);
    this.out_sy = Math.floor((this.in_sy + this.pad * 2 - this.sy) / this.stride + 1);
    this.layer_type = 'pool';
    // store switches for x,y coordinates for where the max comes from, for each output neuron
    this.switchx = global.zeros(this.out_sx*this.out_sy*this.out_depth);
    this.switchy = global.zeros(this.out_sx*this.out_sy*this.out_depth);
  }

  PoolLayer.prototype = {
    forward: function(V, is_training) {
      this.in_act = V;

      var A = new Vol(this.out_sx, this.out_sy, this.out_depth, 0.0);
      
      var n=0; // a counter for switches
      for(var d=0;d<this.out_depth;d++) {
        var x = -this.pad;
        var y = -this.pad;
        for(var ax=0; ax<this.out_sx; x+=this.stride,ax++) {
          y = -this.pad;
          for(var ay=0; ay<this.out_sy; y+=this.stride,ay++) {

            // convolve centered at this particular location
            var a = -99999; // hopefully small enough ;\
            var winx=-1,winy=-1;
            for(var fx=0;fx<this.sx;fx++) {
              for(var fy=0;fy<this.sy;fy++) {
                var oy = y+fy;
                var ox = x+fx;
                if(oy>=0 && oy<V.sy && ox>=0 && ox<V.sx) {
                  var v = V.get(ox, oy, d);
                  // perform max pooling and store pointers to where
                  // the max came from. This will speed up backprop 
                  // and can help make nice visualizations in future
                  if(v > a) { a = v; winx=ox; winy=oy;}
                }
              }
            }
            this.switchx[n] = winx;
            this.switchy[n] = winy;
            n++;
            A.set(ax, ay, d, a);
          }
        }
      }
      this.out_act = A;
      return this.out_act;
    },
    backward: function() { 
      // pooling layers have no parameters, so simply compute 
      // gradient wrt data here
      var V = this.in_act;
      V.dw = global.zeros(V.w.length); // zero out gradient wrt data
      var A = this.out_act; // computed in forward pass 

      var n = 0;
      for(var d=0;d<this.out_depth;d++) {
        var x = -this.pad;
        var y = -this.pad;
        for(var ax=0; ax<this.out_sx; x+=this.stride,ax++) {
          y = -this.pad;
          for(var ay=0; ay<this.out_sy; y+=this.stride,ay++) {

            var chain_grad = this.out_act.get_grad(ax,ay,d);
            V.add_grad(this.switchx[n], this.switchy[n], d, chain_grad);
            n++;

          }
        }
      }
    },
    getParamsAndGrads: function() {
      return [];
    },
    toJSON: function() {
      var json = {};
      json.sx = this.sx;
      json.sy = this.sy;
      json.stride = this.stride;
      json.in_depth = this.in_depth;
      json.out_depth = this.out_depth;
      json.out_sx = this.out_sx;
      json.out_sy = this.out_sy;
      json.layer_type = this.layer_type;
      json.pad = this.pad;
      return json;
    },
    fromJSON: function(json) {
      this.out_depth = json.out_depth;
      this.out_sx = json.out_sx;
      this.out_sy = json.out_sy;
      this.layer_type = json.layer_type;
      this.sx = json.sx;
      this.sy = json.sy;
      this.stride = json.stride;
      this.in_depth = json.in_depth;
      this.pad = typeof json.pad !== 'undefined' ? json.pad : 0; // backwards compatibility
      this.switchx = global.zeros(this.out_sx*this.out_sy*this.out_depth); // need to re-init these appropriately
      this.switchy = global.zeros(this.out_sx*this.out_sy*this.out_depth);
    }
  }

  global.PoolLayer = PoolLayer;

})(convnetjs);

(function(global) {
  "use strict";
  var Vol = global.Vol; // convenience
  
  var InputLayer = function(opt) {
    var opt = opt || {};

    // this is a bit silly but lets allow people to specify either ins or outs
    this.out_sx = typeof opt.out_sx !== 'undefined' ? opt.out_sx : opt.in_sx;
    this.out_sy = typeof opt.out_sy !== 'undefined' ? opt.out_sy : opt.in_sy;
    this.out_depth = typeof opt.out_depth !== 'undefined' ? opt.out_depth : opt.in_depth;
    this.layer_type = 'input';
  }
  InputLayer.prototype = {
    forward: function(V, is_training) {
      this.in_act = V;
      this.out_act = V;
      return this.out_act; // dummy identity function for now
    },
    backward: function() { },
    getParamsAndGrads: function() {
      return [];
    },
    toJSON: function() {
      var json = {};
      json.out_depth = this.out_depth;
      json.out_sx = this.out_sx;
      json.out_sy = this.out_sy;
      json.layer_type = this.layer_type;
      return json;
    },
    fromJSON: function(json) {
      this.out_depth = json.out_depth;
      this.out_sx = json.out_sx;
      this.out_sy = json.out_sy;
      this.layer_type = json.layer_type; 
    }
  }

  global.InputLayer = InputLayer;
})(convnetjs);
(function(global) {
  "use strict";
  var Vol = global.Vol; // convenience
  
  // Layers that implement a loss. Currently these are the layers that 
  // can initiate a backward() pass. In future we probably want a more 
  // flexible system that can accomodate multiple losses to do multi-task
  // learning, and stuff like that. But for now, one of the layers in this
  // file must be the final layer in a Net.

  // This is a classifier, with N discrete classes from 0 to N-1
  // it gets a stream of N incoming numbers and computes the softmax
  // function (exponentiate and normalize to sum to 1 as probabilities should)
  var SoftmaxLayer = function(opt) {
    var opt = opt || {};

    // computed
    this.num_inputs = opt.in_sx * opt.in_sy * opt.in_depth;
    this.out_depth = this.num_inputs;
    this.out_sx = 1;
    this.out_sy = 1;
    this.layer_type = 'softmax';
  }

  SoftmaxLayer.prototype = {
    forward: function(V, is_training) {
      this.in_act = V;

      var A = new Vol(1, 1, this.out_depth, 0.0);

      // compute max activation
      var as = V.w;
      var amax = V.w[0];
      for(var i=1;i<this.out_depth;i++) {
        if(as[i] > amax) amax = as[i];
      }

      // compute exponentials (carefully to not blow up)
      var es = global.zeros(this.out_depth);
      var esum = 0.0;
      for(var i=0;i<this.out_depth;i++) {
        var e = Math.exp(as[i] - amax);
        esum += e;
        es[i] = e;
      }

      // normalize and output to sum to one
      for(var i=0;i<this.out_depth;i++) {
        es[i] /= esum;
        A.w[i] = es[i];
      }

      this.es = es; // save these for backprop
      this.out_act = A;
      return this.out_act;
    },
    backward: function(y) {

      // compute and accumulate gradient wrt weights and bias of this layer
      var x = this.in_act;
      x.dw = global.zeros(x.w.length); // zero out the gradient of input Vol

      for(var i=0;i<this.out_depth;i++) {
        var indicator = i === y ? 1.0 : 0.0;
        var mul = -(indicator - this.es[i]);
        x.dw[i] = mul;
      }

      // loss is the class negative log likelihood
      return -Math.log(this.es[y]);
    },
    getParamsAndGrads: function() { 
      return [];
    },
    toJSON: function() {
      var json = {};
      json.out_depth = this.out_depth;
      json.out_sx = this.out_sx;
      json.out_sy = this.out_sy;
      json.layer_type = this.layer_type;
      json.num_inputs = this.num_inputs;
      return json;
    },
    fromJSON: function(json) {
      this.out_depth = json.out_depth;
      this.out_sx = json.out_sx;
      this.out_sy = json.out_sy;
      this.layer_type = json.layer_type;
      this.num_inputs = json.num_inputs;
    }
  }

  // implements an L2 regression cost layer,
  // so penalizes \sum_i(||x_i - y_i||^2), where x is its input
  // and y is the user-provided array of "correct" values.
  var RegressionLayer = function(opt) {
    var opt = opt || {};

    // computed
    this.num_inputs = opt.in_sx * opt.in_sy * opt.in_depth;
    this.out_depth = this.num_inputs;
    this.out_sx = 1;
    this.out_sy = 1;
    this.layer_type = 'regression';
  }

  RegressionLayer.prototype = {
    forward: function(V, is_training) {
      this.in_act = V;
      this.out_act = V;
      return V; // identity function
    },
    // y is a list here of size num_inputs
    backward: function(y) { 

      // compute and accumulate gradient wrt weights and bias of this layer
      var x = this.in_act;
      x.dw = global.zeros(x.w.length); // zero out the gradient of input Vol
      var loss = 0.0;
      if(y instanceof Array || y instanceof Float64Array) {
        for(var i=0;i<this.out_depth;i++) {
          var dy = x.w[i] - y[i];
          x.dw[i] = dy;
          loss += 2*dy*dy;
        }
      } else {
        // assume it is a struct with entries .dim and .val
        // and we pass gradient only along dimension dim to be equal to val
        var i = y.dim;
        var yi = y.val;
        var dy = x.w[i] - yi;
        x.dw[i] = dy;
        loss += 2*dy*dy;
      }
      return loss;
    },
    getParamsAndGrads: function() { 
      return [];
    },
    toJSON: function() {
      var json = {};
      json.out_depth = this.out_depth;
      json.out_sx = this.out_sx;
      json.out_sy = this.out_sy;
      json.layer_type = this.layer_type;
      json.num_inputs = this.num_inputs;
      return json;
    },
    fromJSON: function(json) {
      this.out_depth = json.out_depth;
      this.out_sx = json.out_sx;
      this.out_sy = json.out_sy;
      this.layer_type = json.layer_type;
      this.num_inputs = json.num_inputs;
    }
  }

  var SVMLayer = function(opt) {
    var opt = opt || {};

    // computed
    this.num_inputs = opt.in_sx * opt.in_sy * opt.in_depth;
    this.out_depth = this.num_inputs;
    this.out_sx = 1;
    this.out_sy = 1;
    this.layer_type = 'svm';
  }

  SVMLayer.prototype = {
    forward: function(V, is_training) {
      this.in_act = V;
      this.out_act = V; // nothing to do, output raw scores
      return V;
    },
    backward: function(y) {

      // compute and accumulate gradient wrt weights and bias of this layer
      var x = this.in_act;
      x.dw = global.zeros(x.w.length); // zero out the gradient of input Vol

      var yscore = x.w[y]; // score of ground truth
      var margin = 1.0;
      var loss = 0.0;
      for(var i=0;i<this.out_depth;i++) {
        if(-yscore + x.w[i] + margin > 0) {
          // violating example, apply loss
          // I love hinge loss, by the way. Truly.
          // Seriously, compare this SVM code with Softmax forward AND backprop code above
          // it's clear which one is superior, not only in code, simplicity
          // and beauty, but also in practice.
          x.dw[i] += 1;
          x.dw[y] -= 1;
          loss += -yscore + x.w[i] + margin;
        }
      }

      return loss;
    },
    getParamsAndGrads: function() { 
      return [];
    },
    toJSON: function() {
      var json = {};
      json.out_depth = this.out_depth;
      json.out_sx = this.out_sx;
      json.out_sy = this.out_sy;
      json.layer_type = this.layer_type;
      json.num_inputs = this.num_inputs;
      return json;
    },
    fromJSON: function(json) {
      this.out_depth = json.out_depth;
      this.out_sx = json.out_sx;
      this.out_sy = json.out_sy;
      this.layer_type = json.layer_type;
      this.num_inputs = json.num_inputs;
    }
  }
  
  global.RegressionLayer = RegressionLayer;
  global.SoftmaxLayer = SoftmaxLayer;
  global.SVMLayer = SVMLayer;

})(convnetjs);

(function(global) {
  "use strict";
  var Vol = global.Vol; // convenience
  
  // Implements ReLU nonlinearity elementwise
  // x -> max(0, x)
  // the output is in [0, inf)
  var ReluLayer = function(opt) {
    var opt = opt || {};

    // computed
    this.out_sx = opt.in_sx;
    this.out_sy = opt.in_sy;
    this.out_depth = opt.in_depth;
    this.layer_type = 'relu';
  }
  ReluLayer.prototype = {
    forward: function(V, is_training) {
      this.in_act = V;
      var V2 = V.clone();
      var N = V.w.length;
      var V2w = V2.w;
      for(var i=0;i<N;i++) { 
        if(V2w[i] < 0) V2w[i] = 0; // threshold at 0
      }
      this.out_act = V2;
      return this.out_act;
    },
    backward: function() {
      var V = this.in_act; // we need to set dw of this
      var V2 = this.out_act;
      var N = V.w.length;
      V.dw = global.zeros(N); // zero out gradient wrt data
      for(var i=0;i<N;i++) {
        if(V2.w[i] <= 0) V.dw[i] = 0; // threshold
        else V.dw[i] = V2.dw[i];
      }
    },
    getParamsAndGrads: function() {
      return [];
    },
    toJSON: function() {
      var json = {};
      json.out_depth = this.out_depth;
      json.out_sx = this.out_sx;
      json.out_sy = this.out_sy;
      json.layer_type = this.layer_type;
      return json;
    },
    fromJSON: function(json) {
      this.out_depth = json.out_depth;
      this.out_sx = json.out_sx;
      this.out_sy = json.out_sy;
      this.layer_type = json.layer_type; 
    }
  }

  // Implements Sigmoid nnonlinearity elementwise
  // x -> 1/(1+e^(-x))
  // so the output is between 0 and 1.
  var SigmoidLayer = function(opt) {
    var opt = opt || {};

    // computed
    this.out_sx = opt.in_sx;
    this.out_sy = opt.in_sy;
    this.out_depth = opt.in_depth;
    this.layer_type = 'sigmoid';
  }
  SigmoidLayer.prototype = {
    forward: function(V, is_training) {
      this.in_act = V;
      var V2 = V.cloneAndZero();
      var N = V.w.length;
      var V2w = V2.w;
      var Vw = V.w;
      for(var i=0;i<N;i++) { 
        V2w[i] = 1.0/(1.0+Math.exp(-Vw[i]));
      }
      this.out_act = V2;
      return this.out_act;
    },
    backward: function() {
      var V = this.in_act; // we need to set dw of this
      var V2 = this.out_act;
      var N = V.w.length;
      V.dw = global.zeros(N); // zero out gradient wrt data
      for(var i=0;i<N;i++) {
        var v2wi = V2.w[i];
        V.dw[i] =  v2wi * (1.0 - v2wi) * V2.dw[i];
      }
    },
    getParamsAndGrads: function() {
      return [];
    },
    toJSON: function() {
      var json = {};
      json.out_depth = this.out_depth;
      json.out_sx = this.out_sx;
      json.out_sy = this.out_sy;
      json.layer_type = this.layer_type;
      return json;
    },
    fromJSON: function(json) {
      this.out_depth = json.out_depth;
      this.out_sx = json.out_sx;
      this.out_sy = json.out_sy;
      this.layer_type = json.layer_type; 
    }
  }

  // Implements Maxout nnonlinearity that computes
  // x -> max(x)
  // where x is a vector of size group_size. Ideally of course,
  // the input size should be exactly divisible by group_size
  var MaxoutLayer = function(opt) {
    var opt = opt || {};

    // required
    this.group_size = typeof opt.group_size !== 'undefined' ? opt.group_size : 2;

    // computed
    this.out_sx = opt.in_sx;
    this.out_sy = opt.in_sy;
    this.out_depth = Math.floor(opt.in_depth / this.group_size);
    this.layer_type = 'maxout';

    this.switches = global.zeros(this.out_sx*this.out_sy*this.out_depth); // useful for backprop
  }
  MaxoutLayer.prototype = {
    forward: function(V, is_training) {
      this.in_act = V;
      var N = this.out_depth; 
      var V2 = new Vol(this.out_sx, this.out_sy, this.out_depth, 0.0);

      // optimization branch. If we're operating on 1D arrays we dont have
      // to worry about keeping track of x,y,d coordinates inside
      // input volumes. In convnets we do :(
      if(this.out_sx === 1 && this.out_sy === 1) {
        for(var i=0;i<N;i++) {
          var ix = i * this.group_size; // base index offset
          var a = V.w[ix];
          var ai = 0;
          for(var j=1;j<this.group_size;j++) {
            var a2 = V.w[ix+j];
            if(a2 > a) {
              a = a2;
              ai = j;
            }
          }
          V2.w[i] = a;
          this.switches[i] = ix + ai;
        }
      } else {
        var n=0; // counter for switches
        for(var x=0;x<V.sx;x++) {
          for(var y=0;y<V.sy;y++) {
            for(var i=0;i<N;i++) {
              var ix = i * this.group_size;
              var a = V.get(x, y, ix);
              var ai = 0;
              for(var j=1;j<this.group_size;j++) {
                var a2 = V.get(x, y, ix+j);
                if(a2 > a) {
                  a = a2;
                  ai = j;
                }
              }
              V2.set(x,y,i,a);
              this.switches[n] = ix + ai;
              n++;
            }
          }
        }

      }
      this.out_act = V2;
      return this.out_act;
    },
    backward: function() {
      var V = this.in_act; // we need to set dw of this
      var V2 = this.out_act;
      var N = this.out_depth;
      V.dw = global.zeros(V.w.length); // zero out gradient wrt data

      // pass the gradient through the appropriate switch
      if(this.out_sx === 1 && this.out_sy === 1) {
        for(var i=0;i<N;i++) {
          var chain_grad = V2.dw[i];
          V.dw[this.switches[i]] = chain_grad;
        }
      } else {
        // bleh okay, lets do this the hard way
        var n=0; // counter for switches
        for(var x=0;x<V2.sx;x++) {
          for(var y=0;y<V2.sy;y++) {
            for(var i=0;i<N;i++) {
              var chain_grad = V2.get_grad(x,y,i);
              V.set_grad(x,y,this.switches[n],chain_grad);
              n++;
            }
          }
        }
      }
    },
    getParamsAndGrads: function() {
      return [];
    },
    toJSON: function() {
      var json = {};
      json.out_depth = this.out_depth;
      json.out_sx = this.out_sx;
      json.out_sy = this.out_sy;
      json.layer_type = this.layer_type;
      json.group_size = this.group_size;
      return json;
    },
    fromJSON: function(json) {
      this.out_depth = json.out_depth;
      this.out_sx = json.out_sx;
      this.out_sy = json.out_sy;
      this.layer_type = json.layer_type; 
      this.group_size = json.group_size;
      this.switches = global.zeros(this.group_size);
    }
  }

  // a helper function, since tanh is not yet part of ECMAScript. Will be in v6.
  function tanh(x) {
    var y = Math.exp(2 * x);
    return (y - 1) / (y + 1);
  }
  // Implements Tanh nnonlinearity elementwise
  // x -> tanh(x) 
  // so the output is between -1 and 1.
  var TanhLayer = function(opt) {
    var opt = opt || {};

    // computed
    this.out_sx = opt.in_sx;
    this.out_sy = opt.in_sy;
    this.out_depth = opt.in_depth;
    this.layer_type = 'tanh';
  }
  TanhLayer.prototype = {
    forward: function(V, is_training) {
      this.in_act = V;
      var V2 = V.cloneAndZero();
      var N = V.w.length;
      for(var i=0;i<N;i++) { 
        V2.w[i] = tanh(V.w[i]);
      }
      this.out_act = V2;
      return this.out_act;
    },
    backward: function() {
      var V = this.in_act; // we need to set dw of this
      var V2 = this.out_act;
      var N = V.w.length;
      V.dw = global.zeros(N); // zero out gradient wrt data
      for(var i=0;i<N;i++) {
        var v2wi = V2.w[i];
        V.dw[i] = (1.0 - v2wi * v2wi) * V2.dw[i];
      }
    },
    getParamsAndGrads: function() {
      return [];
    },
    toJSON: function() {
      var json = {};
      json.out_depth = this.out_depth;
      json.out_sx = this.out_sx;
      json.out_sy = this.out_sy;
      json.layer_type = this.layer_type;
      return json;
    },
    fromJSON: function(json) {
      this.out_depth = json.out_depth;
      this.out_sx = json.out_sx;
      this.out_sy = json.out_sy;
      this.layer_type = json.layer_type; 
    }
  }
  
  global.TanhLayer = TanhLayer;
  global.MaxoutLayer = MaxoutLayer;
  global.ReluLayer = ReluLayer;
  global.SigmoidLayer = SigmoidLayer;

})(convnetjs);

(function(global) {
  "use strict";
  var Vol = global.Vol; // convenience

  // An inefficient dropout layer
  // Note this is not most efficient implementation since the layer before
  // computed all these activations and now we're just going to drop them :(
  // same goes for backward pass. Also, if we wanted to be efficient at test time
  // we could equivalently be clever and upscale during train and copy pointers during test
  // todo: make more efficient.
  var DropoutLayer = function(opt) {
    var opt = opt || {};

    // computed
    this.out_sx = opt.in_sx;
    this.out_sy = opt.in_sy;
    this.out_depth = opt.in_depth;
    this.layer_type = 'dropout';
    this.drop_prob = typeof opt.drop_prob !== 'undefined' ? opt.drop_prob : 0.5;
    this.dropped = global.zeros(this.out_sx*this.out_sy*this.out_depth);
  }
  DropoutLayer.prototype = {
    forward: function(V, is_training) {
      this.in_act = V;
      if(typeof(is_training)==='undefined') { is_training = false; } // default is prediction mode
      var V2 = V.clone();
      var N = V.w.length;
      if(is_training) {
        // do dropout
        for(var i=0;i<N;i++) {
          if(Math.random()<this.drop_prob) { V2.w[i]=0; this.dropped[i] = true; } // drop!
          else {this.dropped[i] = false;}
        }
      } else {
        // scale the activations during prediction
        for(var i=0;i<N;i++) { V2.w[i]*=this.drop_prob; }
      }
      this.out_act = V2;
      return this.out_act; // dummy identity function for now
    },
    backward: function() {
      var V = this.in_act; // we need to set dw of this
      var chain_grad = this.out_act;
      var N = V.w.length;
      V.dw = global.zeros(N); // zero out gradient wrt data
      for(var i=0;i<N;i++) {
        if(!(this.dropped[i])) { 
          V.dw[i] = chain_grad.dw[i]; // copy over the gradient
        }
      }
    },
    getParamsAndGrads: function() {
      return [];
    },
    toJSON: function() {
      var json = {};
      json.out_depth = this.out_depth;
      json.out_sx = this.out_sx;
      json.out_sy = this.out_sy;
      json.layer_type = this.layer_type;
      json.drop_prob = this.drop_prob;
      return json;
    },
    fromJSON: function(json) {
      this.out_depth = json.out_depth;
      this.out_sx = json.out_sx;
      this.out_sy = json.out_sy;
      this.layer_type = json.layer_type; 
      this.drop_prob = json.drop_prob;
    }
  }
  

  global.DropoutLayer = DropoutLayer;
})(convnetjs);
(function(global) {
  "use strict";
  var Vol = global.Vol; // convenience
  
  // a bit experimental layer for now. I think it works but I'm not 100%
  // the gradient check is a bit funky. I'll look into this a bit later.
  // Local Response Normalization in window, along depths of volumes
  var LocalResponseNormalizationLayer = function(opt) {
    var opt = opt || {};

    // required
    this.k = opt.k;
    this.n = opt.n;
    this.alpha = opt.alpha;
    this.beta = opt.beta;

    // computed
    this.out_sx = opt.in_sx;
    this.out_sy = opt.in_sy;
    this.out_depth = opt.in_depth;
    this.layer_type = 'lrn';

    // checks
    if(this.n%2 === 0) { console.log('WARNING n should be odd for LRN layer'); }
  }
  LocalResponseNormalizationLayer.prototype = {
    forward: function(V, is_training) {
      this.in_act = V;

      var A = V.cloneAndZero();
      this.S_cache_ = V.cloneAndZero();
      var n2 = Math.floor(this.n/2);
      for(var x=0;x<V.sx;x++) {
        for(var y=0;y<V.sy;y++) {
          for(var i=0;i<V.depth;i++) {

            var ai = V.get(x,y,i);

            // normalize in a window of size n
            var den = 0.0;
            for(var j=Math.max(0,i-n2);j<=Math.min(i+n2,V.depth-1);j++) {
              var aa = V.get(x,y,j);
              den += aa*aa;
            }
            den *= this.alpha / this.n;
            den += this.k;
            this.S_cache_.set(x,y,i,den); // will be useful for backprop
            den = Math.pow(den, this.beta);
            A.set(x,y,i,ai/den);
          }
        }
      }

      this.out_act = A;
      return this.out_act; // dummy identity function for now
    },
    backward: function() { 
      // evaluate gradient wrt data
      var V = this.in_act; // we need to set dw of this
      V.dw = global.zeros(V.w.length); // zero out gradient wrt data
      var A = this.out_act; // computed in forward pass 

      var n2 = Math.floor(this.n/2);
      for(var x=0;x<V.sx;x++) {
        for(var y=0;y<V.sy;y++) {
          for(var i=0;i<V.depth;i++) {

            var chain_grad = this.out_act.get_grad(x,y,i);
            var S = this.S_cache_.get(x,y,i);
            var SB = Math.pow(S, this.beta);
            var SB2 = SB*SB;

            // normalize in a window of size n
            for(var j=Math.max(0,i-n2);j<=Math.min(i+n2,V.depth-1);j++) {              
              var aj = V.get(x,y,j); 
              var g = -aj*this.beta*Math.pow(S,this.beta-1)*this.alpha/this.n*2*aj;
              if(j===i) g+= SB;
              g /= SB2;
              g *= chain_grad;
              V.add_grad(x,y,j,g);
            }

          }
        }
      }
    },
    getParamsAndGrads: function() { return []; },
    toJSON: function() {
      var json = {};
      json.k = this.k;
      json.n = this.n;
      json.alpha = this.alpha; // normalize by size
      json.beta = this.beta;
      json.out_sx = this.out_sx; 
      json.out_sy = this.out_sy;
      json.out_depth = this.out_depth;
      json.layer_type = this.layer_type;
      return json;
    },
    fromJSON: function(json) {
      this.k = json.k;
      this.n = json.n;
      this.alpha = json.alpha; // normalize by size
      this.beta = json.beta;
      this.out_sx = json.out_sx; 
      this.out_sy = json.out_sy;
      this.out_depth = json.out_depth;
      this.layer_type = json.layer_type;
    }
  }
  

  global.LocalResponseNormalizationLayer = LocalResponseNormalizationLayer;
})(convnetjs);
(function(global) {
  "use strict";
  var Vol = global.Vol; // convenience

  // transforms x-> [x, x_i*x_j forall i,j]
  // so the fully connected layer afters will essentially be doing tensor multiplies
  var QuadTransformLayer = function(opt) {
    var opt = opt || {};

    // computed
    this.out_sx = opt.in_sx;
    this.out_sy = opt.in_sy;
    // linear terms, and then quadratic terms, of which there are 1/2*n*(n+1),
    // (offdiagonals and the diagonal total) and arithmetic series.
    // Actually never mind, lets not be fancy here yet and just include
    // terms x_ix_j and x_jx_i twice. Half as efficient but much less
    // headache.
    this.out_depth = opt.in_depth + opt.in_depth * opt.in_depth;
    this.layer_type = 'quadtransform';

  }
  QuadTransformLayer.prototype = {
    forward: function(V, is_training) {
      this.in_act = V;
      var N = this.out_depth;
      var Ni = V.depth;
      var V2 = new Vol(this.out_sx, this.out_sy, this.out_depth, 0.0);
      for(var x=0;x<V.sx;x++) {
        for(var y=0;y<V.sy;y++) {
          for(var i=0;i<N;i++) {
            if(i<Ni) {
              V2.set(x,y,i,V.get(x,y,i)); // copy these over (linear terms)
            } else {
              var i0 = Math.floor((i-Ni)/Ni);
              var i1 = (i-Ni) - i0*Ni;
              V2.set(x,y,i,V.get(x,y,i0) * V.get(x,y,i1)); // quadratic
            }
          }
        }
      }
      this.out_act = V2;
      return this.out_act; // dummy identity function for now
    },
    backward: function() {
      var V = this.in_act;
      V.dw = global.zeros(V.w.length); // zero out gradient wrt data
      var V2 = this.out_act;
      var N = this.out_depth;
      var Ni = V.depth;
      for(var x=0;x<V.sx;x++) {
        for(var y=0;y<V.sy;y++) {
          for(var i=0;i<N;i++) {
            var chain_grad = V2.get_grad(x,y,i);
            if(i<Ni) {
              V.add_grad(x,y,i,chain_grad);
            } else {
              var i0 = Math.floor((i-Ni)/Ni);
              var i1 = (i-Ni) - i0*Ni;
              V.add_grad(x,y,i0,V.get(x,y,i1)*chain_grad);
              V.add_grad(x,y,i1,V.get(x,y,i0)*chain_grad);
            }
          }
        }
      }
    },
    getParamsAndGrads: function() {
      return [];
    },
    toJSON: function() {
      var json = {};
      json.out_depth = this.out_depth;
      json.out_sx = this.out_sx;
      json.out_sy = this.out_sy;
      json.layer_type = this.layer_type;
      return json;
    },
    fromJSON: function(json) {
      this.out_depth = json.out_depth;
      this.out_sx = json.out_sx;
      this.out_sy = json.out_sy;
      this.layer_type = json.layer_type; 
    }
  }
  

  global.QuadTransformLayer = QuadTransformLayer;
})(convnetjs);
(function(global) {
  "use strict";
  var Vol = global.Vol; // convenience
  
  // Net manages a set of layers
  // For now constraints: Simple linear order of layers, first layer input last layer a cost layer
  var Net = function(options) {
    this.layers = [];
  }

  Net.prototype = {
    
    // takes a list of layer definitions and creates the network layer objects
    makeLayers: function(defs) {

      // few checks for now
      if(defs.length<2) {console.log('ERROR! For now at least have input and softmax layers.');}
      if(defs[0].type !== 'input') {console.log('ERROR! For now first layer should be input.');}

      // desugar syntactic for adding activations and dropouts
      var desugar = function() {
        var new_defs = [];
        for(var i=0;i<defs.length;i++) {
          var def = defs[i];
          
          if(def.type==='softmax' || def.type==='svm') {
            // add an fc layer here, there is no reason the user should
            // have to worry about this and we almost always want to
            new_defs.push({type:'fc', num_neurons: def.num_classes});
          }

          if(def.type==='regression') {
            // add an fc layer here, there is no reason the user should
            // have to worry about this and we almost always want to
            new_defs.push({type:'fc', num_neurons: def.num_neurons});
          }

          if((def.type==='fc' || def.type==='conv') 
              && typeof(def.bias_pref) === 'undefined'){
            def.bias_pref = 0.0;
            if(typeof def.activation !== 'undefined' && def.activation === 'relu') {
              def.bias_pref = 0.1; // relus like a bit of positive bias to get gradients early
              // otherwise it's technically possible that a relu unit will never turn on (by chance)
              // and will never get any gradient and never contribute any computation. Dead relu.
            }
          }
          
          if(typeof def.tensor !== 'undefined') {
            // apply quadratic transform so that the upcoming multiply will include
            // quadratic terms, equivalent to doing a tensor product
            if(def.tensor) {
              new_defs.push({type: 'quadtransform'});
            }
          }

          new_defs.push(def);

          if(typeof def.activation !== 'undefined') {
            if(def.activation==='relu') { new_defs.push({type:'relu'}); }
            else if (def.activation==='sigmoid') { new_defs.push({type:'sigmoid'}); }
            else if (def.activation==='tanh') { new_defs.push({type:'tanh'}); }
            else if (def.activation==='maxout') {
              // create maxout activation, and pass along group size, if provided
              var gs = def.group_size !== 'undefined' ? def.group_size : 2;
              new_defs.push({type:'maxout', group_size:gs});
            }
            else { console.log('ERROR unsupported activation ' + def.activation); }
          }
          if(typeof def.drop_prob !== 'undefined' && def.type !== 'dropout') {
            new_defs.push({type:'dropout', drop_prob: def.drop_prob});
          }

        }
        return new_defs;
      }
      defs = desugar(defs);

      // create the layers
      this.layers = [];
      for(var i=0;i<defs.length;i++) {
        var def = defs[i];
        if(i>0) {
          var prev = this.layers[i-1];
          def.in_sx = prev.out_sx;
          def.in_sy = prev.out_sy;
          def.in_depth = prev.out_depth;
        }

        switch(def.type) {
          case 'fc': this.layers.push(new global.FullyConnLayer(def)); break;
          case 'lrn': this.layers.push(new global.LocalResponseNormalizationLayer(def)); break;
          case 'dropout': this.layers.push(new global.DropoutLayer(def)); break;
          case 'input': this.layers.push(new global.InputLayer(def)); break;
          case 'softmax': this.layers.push(new global.SoftmaxLayer(def)); break;
          case 'regression': this.layers.push(new global.RegressionLayer(def)); break;
          case 'conv': this.layers.push(new global.ConvLayer(def)); break;
          case 'pool': this.layers.push(new global.PoolLayer(def)); break;
          case 'relu': this.layers.push(new global.ReluLayer(def)); break;
          case 'sigmoid': this.layers.push(new global.SigmoidLayer(def)); break;
          case 'tanh': this.layers.push(new global.TanhLayer(def)); break;
          case 'maxout': this.layers.push(new global.MaxoutLayer(def)); break;
          case 'quadtransform': this.layers.push(new global.QuadTransformLayer(def)); break;
          case 'svm': this.layers.push(new global.SVMLayer(def)); break;
          default: console.log('ERROR: UNRECOGNIZED LAYER TYPE!');
        }
      }
    },

    // forward prop the network. A trainer will pass in is_training = true
    forward: function(V, is_training) {
      if(typeof(is_training)==='undefined') is_training = false;
      var act = this.layers[0].forward(V, is_training);
      for(var i=1;i<this.layers.length;i++) {
        act = this.layers[i].forward(act, is_training);
      }
      return act;
    },
    
    // backprop: compute gradients wrt all parameters
    backward: function(y) {
      var N = this.layers.length;
      var loss = this.layers[N-1].backward(y); // last layer assumed softmax
      for(var i=N-2;i>=0;i--) { // first layer assumed input
        this.layers[i].backward();
      }
      return loss;
    },
    getParamsAndGrads: function() {
      // accumulate parameters and gradients for the entire network
      var response = [];
      for(var i=0;i<this.layers.length;i++) {
        var layer_reponse = this.layers[i].getParamsAndGrads();
        for(var j=0;j<layer_reponse.length;j++) {
          response.push(layer_reponse[j]);
        }
      }
      return response;
    },
    getPrediction: function() {
      var S = this.layers[this.layers.length-1]; // softmax layer
      var p = S.out_act.w;
      var maxv = p[0];
      var maxi = 0;
      for(var i=1;i<p.length;i++) {
        if(p[i] > maxv) { maxv = p[i]; maxi = i;}
      }
      return maxi;
    },
    toJSON: function() {
      var json = {};
      json.layers = [];
      for(var i=0;i<this.layers.length;i++) {
        json.layers.push(this.layers[i].toJSON());
      }
      return json;
    },
    fromJSON: function(json) {
      this.layers = [];
      for(var i=0;i<json.layers.length;i++) {
        var Lj = json.layers[i]
        var t = Lj.layer_type;
        var L;
        if(t==='input') { L = new global.InputLayer(); }
        if(t==='relu') { L = new global.ReluLayer(); }
        if(t==='sigmoid') { L = new global.SigmoidLayer(); }
        if(t==='tanh') { L = new global.TanhLayer(); }
        if(t==='dropout') { L = new global.DropoutLayer(); }
        if(t==='conv') { L = new global.ConvLayer(); }
        if(t==='pool') { L = new global.PoolLayer(); }
        if(t==='lrn') { L = new global.LocalResponseNormalizationLayer(); }
        if(t==='softmax') { L = new global.SoftmaxLayer(); }
        if(t==='regression') { L = new global.RegressionLayer(); }
        if(t==='fc') { L = new global.FullyConnLayer(); }
        if(t==='maxout') { L = new global.MaxoutLayer(); }
        if(t==='quadtransform') { L = new global.QuadTransformLayer(); }
        if(t==='svm') { L = new global.SVMLayer(); }
        L.fromJSON(Lj);
        this.layers.push(L);
      }
    }
  }
  

  global.Net = Net;
})(convnetjs);
(function(global) {
  "use strict";
  var Vol = global.Vol; // convenience

  var Trainer = function(net, options) {

    this.net = net;

    var options = options || {};
    this.learning_rate = typeof options.learning_rate !== 'undefined' ? options.learning_rate : 0.01;
    this.l1_decay = typeof options.l1_decay !== 'undefined' ? options.l1_decay : 0.0;
    this.l2_decay = typeof options.l2_decay !== 'undefined' ? options.l2_decay : 0.0;
    this.batch_size = typeof options.batch_size !== 'undefined' ? options.batch_size : 1;
    this.method = typeof options.method !== 'undefined' ? options.method : 'sgd'; // sgd/adagrad/adadelta/windowgrad

    this.momentum = typeof options.momentum !== 'undefined' ? options.momentum : 0.9;
    this.ro = typeof options.ro !== 'undefined' ? options.ro : 0.95; // used in adadelta
    this.eps = typeof options.eps !== 'undefined' ? options.eps : 1e-6; // used in adadelta

    this.k = 0; // iteration counter
    this.gsum = []; // last iteration gradients (used for momentum calculations)
    this.xsum = []; // used in adadelta
  }

  Trainer.prototype = {
    train: function(x, y) {

      var start = new Date().getTime();
      this.net.forward(x, true); // also set the flag that lets the net know we're just training
      var end = new Date().getTime();
      var fwd_time = end - start;

      var start = new Date().getTime();
      var cost_loss = this.net.backward(y);
      var l2_decay_loss = 0.0;
      var l1_decay_loss = 0.0;
      var end = new Date().getTime();
      var bwd_time = end - start;
      
      this.k++;
      if(this.k % this.batch_size === 0) {

        var pglist = this.net.getParamsAndGrads();

        // initialize lists for accumulators. Will only be done once on first iteration
        if(this.gsum.length === 0 && (this.method !== 'sgd' || this.momentum > 0.0)) {
          // only vanilla sgd doesnt need either lists
          // momentum needs gsum
          // adagrad needs gsum
          // adadelta needs gsum and xsum
          for(var i=0;i<pglist.length;i++) {
            this.gsum.push(global.zeros(pglist[i].params.length));
            if(this.method === 'adadelta') {
              this.xsum.push(global.zeros(pglist[i].params.length));
            } else {
              this.xsum.push([]); // conserve memory
            }
          }
        }

        // perform an update for all sets of weights
        for(var i=0;i<pglist.length;i++) {
          var pg = pglist[i]; // param, gradient, other options in future (custom learning rate etc)
          var p = pg.params;
          var g = pg.grads;

          // learning rate for some parameters.
          var l2_decay_mul = typeof pg.l2_decay_mul !== 'undefined' ? pg.l2_decay_mul : 1.0;
          var l1_decay_mul = typeof pg.l1_decay_mul !== 'undefined' ? pg.l1_decay_mul : 1.0;
          var l2_decay = this.l2_decay * l2_decay_mul;
          var l1_decay = this.l1_decay * l1_decay_mul;

          var plen = p.length;
          for(var j=0;j<plen;j++) {
            l2_decay_loss += l2_decay*p[j]*p[j]/2; // accumulate weight decay loss
            l1_decay_loss += l1_decay*Math.abs(p[j]);
            var l1grad = l1_decay * (p[j] > 0 ? 1 : -1);
            var l2grad = l2_decay * (p[j]);

            var gij = (l2grad + l1grad + g[j]) / this.batch_size; // raw batch gradient

            var gsumi = this.gsum[i];
            var xsumi = this.xsum[i];
            if(this.method === 'adagrad') {
              // adagrad update
              gsumi[j] = gsumi[j] + gij * gij;
              var dx = - this.learning_rate / Math.sqrt(gsumi[j] + this.eps) * gij;
              p[j] += dx;
            } else if(this.method === 'windowgrad') {
              // this is adagrad but with a moving window weighted average
              // so the gradient is not accumulated over the entire history of the run. 
              // it's also referred to as Idea #1 in Zeiler paper on Adadelta. Seems reasonable to me!
              gsumi[j] = this.ro * gsumi[j] + (1-this.ro) * gij * gij;
              var dx = - this.learning_rate / Math.sqrt(gsumi[j] + this.eps) * gij; // eps added for better conditioning
              p[j] += dx;
            } else if(this.method === 'adadelta') {
              // assume adadelta if not sgd or adagrad
              gsumi[j] = this.ro * gsumi[j] + (1-this.ro) * gij * gij;
              var dx = - Math.sqrt((xsumi[j] + this.eps)/(gsumi[j] + this.eps)) * gij;
              xsumi[j] = this.ro * xsumi[j] + (1-this.ro) * dx * dx; // yes, xsum lags behind gsum by 1.
              p[j] += dx;
            } else {
              // assume SGD
              if(this.momentum > 0.0) {
                // momentum update
                var dx = this.momentum * gsumi[j] - this.learning_rate * gij; // step
                gsumi[j] = dx; // back this up for next iteration of momentum
                p[j] += dx; // apply corrected gradient
              } else {
                // vanilla sgd
                p[j] +=  - this.learning_rate * gij;
              }
            }
            g[j] = 0.0; // zero out gradient so that we can begin accumulating anew
          }
        }
      }

      // appending softmax_loss for backwards compatibility, but from now on we will always use cost_loss
      // in future, TODO: have to completely redo the way loss is done around the network as currently 
      // loss is a bit of a hack. Ideally, user should specify arbitrary number of loss functions on any layer
      // and it should all be computed correctly and automatically. 
      return {fwd_time: fwd_time, bwd_time: bwd_time, 
              l2_decay_loss: l2_decay_loss, l1_decay_loss: l1_decay_loss,
              cost_loss: cost_loss, softmax_loss: cost_loss, 
              loss: cost_loss + l1_decay_loss + l2_decay_loss}
    }
  }
  
  global.Trainer = Trainer;
  global.SGDTrainer = Trainer; // backwards compatibility
})(convnetjs);

(function(global) {
  "use strict";

  // used utilities, make explicit local references
  var randf = global.randf;
  var randi = global.randi;
  var Net = global.Net;
  var Trainer = global.Trainer;
  var maxmin = global.maxmin;
  var randperm = global.randperm;
  var weightedSample = global.weightedSample;
  var getopt = global.getopt;
  var arrUnique = global.arrUnique;

  /*
  A MagicNet takes data: a list of convnetjs.Vol(), and labels
  which for now are assumed to be class indeces 0..K. MagicNet then:
  - creates data folds for cross-validation
  - samples candidate networks
  - evaluates candidate networks on all data folds
  - produces predictions by model-averaging the best networks
  */
  var MagicNet = function(data, labels, opt) {
    var opt = opt || {};
    if(typeof data === 'undefined') { data = []; }
    if(typeof labels === 'undefined') { labels = []; }

    // required inputs
    this.data = data; // store these pointers to data
    this.labels = labels;

    // optional inputs
    this.train_ratio = getopt(opt, 'train_ratio', 0.7);
    this.num_folds = getopt(opt, 'num_folds', 10);
    this.num_candidates = getopt(opt, 'num_candidates', 50); // we evaluate several in parallel
    // how many epochs of data to train every network? for every fold?
    // higher values mean higher accuracy in final results, but more expensive
    this.num_epochs = getopt(opt, 'num_epochs', 50); 
    // number of best models to average during prediction. Usually higher = better
    this.ensemble_size = getopt(opt, 'ensemble_size', 10);

    // candidate parameters
    this.batch_size_min = getopt(opt, 'batch_size_min', 10);
    this.batch_size_max = getopt(opt, 'batch_size_max', 300);
    this.l2_decay_min = getopt(opt, 'l2_decay_min', -4);
    this.l2_decay_max = getopt(opt, 'l2_decay_max', 2);
    this.learning_rate_min = getopt(opt, 'learning_rate_min', -4);
    this.learning_rate_max = getopt(opt, 'learning_rate_max', 0);
    this.momentum_min = getopt(opt, 'momentum_min', 0.9);
    this.momentum_max = getopt(opt, 'momentum_max', 0.9);
    this.neurons_min = getopt(opt, 'neurons_min', 5);
    this.neurons_max = getopt(opt, 'neurons_max', 30);

    // computed
    this.folds = []; // data fold indices, gets filled by sampleFolds()
    this.candidates = []; // candidate networks that are being currently evaluated
    this.evaluated_candidates = []; // history of all candidates that were fully evaluated on all folds
    this.unique_labels = arrUnique(labels);
    this.iter = 0; // iteration counter, goes from 0 -> num_epochs * num_training_data
    this.foldix = 0; // index of active fold

    // callbacks
    this.finish_fold_callback = null;
    this.finish_batch_callback = null;

    // initializations
    if(this.data.length > 0) {
      this.sampleFolds();
      this.sampleCandidates();
    }
  };

  MagicNet.prototype = {

    // sets this.folds to a sampling of this.num_folds folds
    sampleFolds: function() {
      var N = this.data.length;
      var num_train = Math.floor(this.train_ratio * N);
      this.folds = []; // flush folds, if any
      for(var i=0;i<this.num_folds;i++) {
        var p = randperm(N);
        this.folds.push({train_ix: p.slice(0, num_train), test_ix: p.slice(num_train, N)});
      }
    },

    // returns a random candidate network
    sampleCandidate: function() {
      var input_depth = this.data[0].w.length;
      var num_classes = this.unique_labels.length;

      // sample network topology and hyperparameters
      var layer_defs = [];
      layer_defs.push({type:'input', out_sx:1, out_sy:1, out_depth: input_depth});
      var nl = weightedSample([0,1,2,3], [0.2, 0.3, 0.3, 0.2]); // prefer nets with 1,2 hidden layers
      for(var q=0;q<nl;q++) {
        var ni = randi(this.neurons_min, this.neurons_max);
        var act = ['tanh','maxout','relu'][randi(0,3)];
        if(randf(0,1)<0.5) {
          var dp = Math.random();
          layer_defs.push({type:'fc', num_neurons: ni, activation: act, drop_prob: dp});
        } else {
          layer_defs.push({type:'fc', num_neurons: ni, activation: act});
        }
      }
      layer_defs.push({type:'softmax', num_classes: num_classes});
      var net = new Net();
      net.makeLayers(layer_defs);

      // sample training hyperparameters
      var bs = randi(this.batch_size_min, this.batch_size_max); // batch size
      var l2 = Math.pow(10, randf(this.l2_decay_min, this.l2_decay_max)); // l2 weight decay
      var lr = Math.pow(10, randf(this.learning_rate_min, this.learning_rate_max)); // learning rate
      var mom = randf(this.momentum_min, this.momentum_max); // momentum. Lets just use 0.9, works okay usually ;p
      var tp = randf(0,1); // trainer type
      var trainer_def;
      if(tp<0.33) {
        trainer_def = {method:'adadelta', batch_size:bs, l2_decay:l2};
      } else if(tp<0.66) {
        trainer_def = {method:'adagrad', learning_rate: lr, batch_size:bs, l2_decay:l2};
      } else {
        trainer_def = {method:'sgd', learning_rate: lr, momentum: mom, batch_size:bs, l2_decay:l2};
      }
      
      var trainer = new Trainer(net, trainer_def);

      var cand = {};
      cand.acc = [];
      cand.accv = 0; // this will maintained as sum(acc) for convenience
      cand.layer_defs = layer_defs;
      cand.trainer_def = trainer_def;
      cand.net = net;
      cand.trainer = trainer;
      return cand;
    },

    // sets this.candidates with this.num_candidates candidate nets
    sampleCandidates: function() {
      this.candidates = []; // flush, if any
      for(var i=0;i<this.num_candidates;i++) {
        var cand = this.sampleCandidate();
        this.candidates.push(cand);
      }
    },

    step: function() {
      
      // run an example through current candidate
      this.iter++;

      // step all candidates on a random data point
      var fold = this.folds[this.foldix]; // active fold
      var dataix = fold.train_ix[randi(0, fold.train_ix.length)];
      for(var k=0;k<this.candidates.length;k++) {
        var x = this.data[dataix];
        var l = this.labels[dataix];
        this.candidates[k].trainer.train(x, l);
      }

      // process consequences: sample new folds, or candidates
      var lastiter = this.num_epochs * fold.train_ix.length;
      if(this.iter >= lastiter) {
        // finished evaluation of this fold. Get final validation
        // accuracies, record them, and go on to next fold.
        var val_acc = this.evalValErrors();
        for(var k=0;k<this.candidates.length;k++) {
          var c = this.candidates[k];
          c.acc.push(val_acc[k]);
          c.accv += val_acc[k];
        }
        this.iter = 0; // reset step number
        this.foldix++; // increment fold

        if(this.finish_fold_callback !== null) {
          this.finish_fold_callback();
        }

        if(this.foldix >= this.folds.length) {
          // we finished all folds as well! Record these candidates
          // and sample new ones to evaluate.
          for(var k=0;k<this.candidates.length;k++) {
            this.evaluated_candidates.push(this.candidates[k]);
          }
          // sort evaluated candidates according to accuracy achieved
          this.evaluated_candidates.sort(function(a, b) { 
            return (a.accv / a.acc.length) 
                 > (b.accv / b.acc.length) 
                 ? -1 : 1;
          });
          // and clip only to the top few ones (lets place limit at 3*ensemble_size)
          // otherwise there are concerns with keeping these all in memory 
          // if MagicNet is being evaluated for a very long time
          if(this.evaluated_candidates.length > 3 * this.ensemble_size) {
            this.evaluated_candidates = this.evaluated_candidates.slice(0, 3 * this.ensemble_size);
          }
          if(this.finish_batch_callback !== null) {
            this.finish_batch_callback();
          }
          this.sampleCandidates(); // begin with new candidates
          this.foldix = 0; // reset this
        } else {
          // we will go on to another fold. reset all candidates nets
          for(var k=0;k<this.candidates.length;k++) {
            var c = this.candidates[k];
            var net = new Net();
            net.makeLayers(c.layer_defs);
            var trainer = new Trainer(net, c.trainer_def);
            c.net = net;
            c.trainer = trainer;
          }
        }
      }
    },

    evalValErrors: function() {
      // evaluate candidates on validation data and return performance of current networks
      // as simple list
      var vals = [];
      var fold = this.folds[this.foldix]; // active fold
      for(var k=0;k<this.candidates.length;k++) {
        var net = this.candidates[k].net;
        var v = 0.0;
        for(var q=0;q<fold.test_ix.length;q++) {
          var x = this.data[fold.test_ix[q]];
          var l = this.labels[fold.test_ix[q]];
          net.forward(x);
          var yhat = net.getPrediction();
          v += (yhat === l ? 1.0 : 0.0); // 0 1 loss
        }
        v /= fold.test_ix.length; // normalize
        vals.push(v);
      }
      return vals;
    },

    // returns prediction scores for given test data point, as Vol
    // uses an averaged prediction from the best ensemble_size models
    // x is a Vol.
    predict_soft: function(data) {
      // forward prop the best networks
      // and accumulate probabilities at last layer into a an output Vol
      var nv = Math.min(this.ensemble_size, this.evaluated_candidates.length);
      if(nv === 0) { return new convnetjs.Vol(0,0,0); } // not sure what to do here? we're not ready yet
      var xout, n;
      for(var j=0;j<nv;j++) {
        var net = this.evaluated_candidates[j].net;
        var x = net.forward(data);
        if(j===0) { 
          xout = x; 
          n = x.w.length; 
        } else {
          // add it on
          for(var d=0;d<n;d++) {
            xout.w[d] += x.w[d];
          }
        }
      }
      // produce average
      for(var d=0;d<n;d++) {
        xout.w[d] /= n;
      }
      return xout;
    },

    predict: function(data) {
      var xout = this.predict_soft(data);
      if(xout.w.length !== 0) {
        var stats = maxmin(xout.w);
        var predicted_label = stats.maxi; 
      } else {
        var predicted_label = -1; // error out
      }
      return predicted_label;

    },

    toJSON: function() {
      // dump the top ensemble_size networks as a list
      var nv = Math.min(this.ensemble_size, this.evaluated_candidates.length);
      var json = {};
      json.nets = [];
      for(var i=0;i<nv;i++) {
        json.nets.push(this.evaluated_candidates[i].net.toJSON());
      }
      return json;
    },

    fromJSON: function(json) {
      this.ensemble_size = json.nets.length;
      this.evaluated_candidates = [];
      for(var i=0;i<this.ensemble_size;i++) {
        var net = new Net();
        net.fromJSON(json.nets[i]);
        var dummy_candidate = {};
        dummy_candidate.net = net;
        this.evaluated_candidates.push(dummy_candidate);
      }
    },

    // callback functions
    // called when a fold is finished, while evaluating a batch
    onFinishFold: function(f) { this.finish_fold_callback = f; },
    // called when a batch of candidates has finished evaluating
    onFinishBatch: function(f) { this.finish_batch_callback = f; }
    
  };

  global.MagicNet = MagicNet;
})(convnetjs);
(function(lib) {
  "use strict";
  if (typeof module === "undefined" || typeof module.exports === "undefined") {
    window.jsfeat = lib; // in ordinary browser attach library to window
  } else {
    module.exports = lib; // in nodejs
  }
})(convnetjs);

},{}],10:[function(require,module,exports){
var deepqlearn = deepqlearn || { REVISION: 'ALPHA' };

(function(global) {
  "use strict";
  
  // An agent is in state0 and does action0
  // environment then assigns reward0 and provides new state, state1
  // Experience nodes store all this information, which is used in the
  // Q-learning update step
  var Experience = function(state0, action0, reward0, state1) {
    this.state0 = state0;
    this.action0 = action0;
    this.reward0 = reward0;
    this.state1 = state1;
  }

  // A Brain object does all the magic.
  // over time it receives some inputs and some rewards
  // and its job is to set the outputs to maximize the expected reward
  var Brain = function(num_states, num_actions, opt) {
    var opt = opt || {};
    // in number of time steps, of temporal memory
    // the ACTUAL input to the net will be (x,a) temporal_window times, and followed by current x
    // so to have no information from previous time step going into value function, set to 0.
    this.temporal_window = typeof opt.temporal_window !== 'undefined' ? opt.temporal_window : 1; 
    // size of experience replay memory
    this.experience_size = typeof opt.experience_size !== 'undefined' ? opt.experience_size : 30000;
    // number of examples in experience replay memory before we begin learning
    this.start_learn_threshold = typeof opt.start_learn_threshold !== 'undefined'? opt.start_learn_threshold : Math.floor(Math.min(this.experience_size*0.1, 1000)); 
    // gamma is a crucial parameter that controls how much plan-ahead the agent does. In [0,1]
    this.gamma = typeof opt.gamma !== 'undefined' ? opt.gamma : 0.8;
    
    // number of steps we will learn for
    this.learning_steps_total = typeof opt.learning_steps_total !== 'undefined' ? opt.learning_steps_total : 100000;
    // how many steps of the above to perform only random actions (in the beginning)?
    this.learning_steps_burnin = typeof opt.learning_steps_burnin !== 'undefined' ? opt.learning_steps_burnin : 3000;
    // what epsilon value do we bottom out on? 0.0 => purely deterministic policy at end
    this.epsilon_min = typeof opt.epsilon_min !== 'undefined' ? opt.epsilon_min : 0.05;
    // what epsilon to use at test time? (i.e. when learning is disabled)
    this.epsilon_test_time = typeof opt.epsilon_test_time !== 'undefined' ? opt.epsilon_test_time : 0.01;
    
    // advanced feature. Sometimes a random action should be biased towards some values
    // for example in flappy bird, we may want to choose to not flap more often
    if(typeof opt.random_action_distribution !== 'undefined') {
      // this better sum to 1 by the way, and be of length this.num_actions
      this.random_action_distribution = opt.random_action_distribution;
      if(this.random_action_distribution.length !== num_actions) {
        console.log('TROUBLE. random_action_distribution should be same length as num_actions.');
      }
      var a = this.random_action_distribution;
      var s = 0.0; for(var k=0;k<a.length;k++) { s+= a[k]; }
      if(Math.abs(s-1.0)>0.0001) { console.log('TROUBLE. random_action_distribution should sum to 1!'); }
    } else {
      this.random_action_distribution = [];
    }
    
    // states that go into neural net to predict optimal action look as
    // x0,a0,x1,a1,x2,a2,...xt
    // this variable controls the size of that temporal window. Actions are
    // encoded as 1-of-k hot vectors
    this.net_inputs = num_states * this.temporal_window + num_actions * this.temporal_window + num_states;
    this.num_states = num_states;
    this.num_actions = num_actions;
    this.window_size = Math.max(this.temporal_window, 2); // must be at least 2, but if we want more context even more
    this.state_window = new Array(this.window_size);
    this.action_window = new Array(this.window_size);
    this.reward_window = new Array(this.window_size);
    this.net_window = new Array(this.window_size);
    
    // create [state -> value of all possible actions] modeling net for the value function
    var layer_defs = [];
    if(typeof opt.layer_defs !== 'undefined') {
      // this is an advanced usage feature, because size of the input to the network, and number of
      // actions must check out. This is not very pretty Object Oriented programming but I can't see
      // a way out of it :(
      layer_defs = opt.layer_defs;
      if(layer_defs.length < 2) { console.log('TROUBLE! must have at least 2 layers'); }
      if(layer_defs[0].type !== 'input') { console.log('TROUBLE! first layer must be input layer!'); }
      if(layer_defs[layer_defs.length-1].type !== 'regression') { console.log('TROUBLE! last layer must be input regression!'); }
      if(layer_defs[0].out_depth * layer_defs[0].out_sx * layer_defs[0].out_sy !== this.net_inputs) {
        console.log('TROUBLE! Number of inputs must be num_states * temporal_window + num_actions * temporal_window + num_states!');
      }
      if(layer_defs[layer_defs.length-1].num_neurons !== this.num_actions) {
        console.log('TROUBLE! Number of regression neurons should be num_actions!');
      }
    } else {
      // create a very simple neural net by default
      layer_defs.push({type:'input', out_sx:1, out_sy:1, out_depth:this.net_inputs});
      if(typeof opt.hidden_layer_sizes !== 'undefined') {
        // allow user to specify this via the option, for convenience
        var hl = opt.hidden_layer_sizes;
        for(var k=0;k<hl.length;k++) {
          layer_defs.push({type:'fc', num_neurons:hl[k], activation:'relu'}); // relu by default
        }
      }
      layer_defs.push({type:'regression', num_neurons:num_actions}); // value function output
    }
    this.value_net = new convnetjs.Net();
    this.value_net.makeLayers(layer_defs);
    
    // and finally we need a Temporal Difference Learning trainer!
    var tdtrainer_options = {learning_rate:0.01, momentum:0.0, batch_size:64, l2_decay:0.01};
    if(typeof opt.tdtrainer_options !== 'undefined') {
      tdtrainer_options = opt.tdtrainer_options; // allow user to overwrite this
    }
    this.tdtrainer = new convnetjs.SGDTrainer(this.value_net, tdtrainer_options);
    
    // experience replay
    this.experience = [];
    
    // various housekeeping variables
    this.age = 0; // incremented every backward()
    this.forward_passes = 0; // incremented every forward()
    this.epsilon = 1.0; // controls exploration exploitation tradeoff. Should be annealed over time
    this.latest_reward = 0;
    this.last_input_array = [];
    this.average_reward_window = new cnnutil.Window(1000, 10);
    this.average_loss_window = new cnnutil.Window(1000, 10);
    this.learning = true;
  }
  Brain.prototype = {
    random_action: function() {
      // a bit of a helper function. It returns a random action
      // we are abstracting this away because in future we may want to 
      // do more sophisticated things. For example some actions could be more
      // or less likely at "rest"/default state.
      if(this.random_action_distribution.length === 0) {
        return convnetjs.randi(0, this.num_actions);
      } else {
        // okay, lets do some fancier sampling:
        var p = convnetjs.randf(0, 1.0);
        var cumprob = 0.0;
        for(var k=0;k<this.num_actions;k++) {
          cumprob += this.random_action_distribution[k];
          if(p < cumprob) { return k; }
        }
      }
    },
    policy: function(s) {
      // compute the value of doing any action in this state
      // and return the argmax action and its value
      var svol = new convnetjs.Vol(1, 1, this.net_inputs);
      svol.w = s;
      var action_values = this.value_net.forward(svol);
      var maxk = 0; 
      var maxval = action_values.w[0];
      for(var k=1;k<this.num_actions;k++) {
        if(action_values.w[k] > maxval) { maxk = k; maxval = action_values.w[k]; }
      }
      return {action:maxk, value:maxval};
    },
    getNetInput: function(xt) {
      // return s = (x,a,x,a,x,a,xt) state vector. 
      // It's a concatenation of last window_size (x,a) pairs and current state x
      var w = [];
      w = w.concat(xt); // start with current state
      // and now go backwards and append states and actions from history temporal_window times
      var n = this.window_size; 
      for(var k=0;k<this.temporal_window;k++) {
        // state
        w = w.concat(this.state_window[n-1-k]);
        // action, encoded as 1-of-k indicator vector. We scale it up a bit because
        // we dont want weight regularization to undervalue this information, as it only exists once
        var action1ofk = new Array(this.num_actions);
        for(var q=0;q<this.num_actions;q++) action1ofk[q] = 0.0;
        action1ofk[this.action_window[n-1-k]] = 1.0*this.num_states;
        w = w.concat(action1ofk);
      }
      return w;
    },
    forward: function(input_array) {
      // compute forward (behavior) pass given the input neuron signals from body
      this.forward_passes += 1;
      this.last_input_array = input_array; // back this up
      
      // create network input
      var action;
      if(this.forward_passes > this.temporal_window) {
        // we have enough to actually do something reasonable
        var net_input = this.getNetInput(input_array);
        if(this.learning) {
          // compute epsilon for the epsilon-greedy policy
          this.epsilon = Math.min(1.0, Math.max(this.epsilon_min, 1.0-(this.age - this.learning_steps_burnin)/(this.learning_steps_total - this.learning_steps_burnin))); 
        } else {
          this.epsilon = this.epsilon_test_time; // use test-time value
        }
        var rf = convnetjs.randf(0,1);
        if(rf < this.epsilon) {
          // choose a random action with epsilon probability
          action = this.random_action();
        } else {
          // otherwise use our policy to make decision
          var maxact = this.policy(net_input);
          action = maxact.action;
       }
      } else {
        // pathological case that happens first few iterations 
        // before we accumulate window_size inputs
        var net_input = [];
        action = this.random_action();
      }
      
      // remember the state and action we took for backward pass
      this.net_window.shift();
      this.net_window.push(net_input);
      this.state_window.shift(); 
      this.state_window.push(input_array);
      this.action_window.shift(); 
      this.action_window.push(action);
      
      return action;
    },
    backward: function(reward) {
      this.latest_reward = reward;
      this.average_reward_window.add(reward);
      this.reward_window.shift();
      this.reward_window.push(reward);
      
      if(!this.learning) { return; } 
      
      // various book-keeping
      this.age += 1;
      
      // it is time t+1 and we have to store (s_t, a_t, r_t, s_{t+1}) as new experience
      // (given that an appropriate number of state measurements already exist, of course)
      if(this.forward_passes > this.temporal_window + 1) {
        var e = new Experience();
        var n = this.window_size;
        e.state0 = this.net_window[n-2];
        e.action0 = this.action_window[n-2];
        e.reward0 = this.reward_window[n-2];
        e.state1 = this.net_window[n-1];
        if(this.experience.length < this.experience_size) {
          this.experience.push(e);
        } else {
          // replace. finite memory!
          var ri = convnetjs.randi(0, this.experience_size);
          this.experience[ri] = e;
        }
      }
      
      // learn based on experience, once we have some samples to go on
      // this is where the magic happens...
      if(this.experience.length > this.start_learn_threshold) {
        var avcost = 0.0;
        for(var k=0;k < this.tdtrainer.batch_size;k++) {
          var re = convnetjs.randi(0, this.experience.length);
          var e = this.experience[re];
          var x = new convnetjs.Vol(1, 1, this.net_inputs);
          x.w = e.state0;
          var maxact = this.policy(e.state1);
          var r = e.reward0 + this.gamma * maxact.value;
          var ystruct = {dim: e.action0, val: r};
          var loss = this.tdtrainer.train(x, ystruct);
          avcost += loss.loss;
        }
        avcost = avcost/this.tdtrainer.batch_size;
        this.average_loss_window.add(avcost);
      }
    },
    visSelf: function(elt) {
      elt.innerHTML = ''; // erase elt first
      
      // elt is a DOM element that this function fills with brain-related information
      var brainvis = document.createElement('div');
      
      // basic information
      var desc = document.createElement('div');
      var t = '';
      t += 'experience replay size: ' + this.experience.length + '<br>';
      t += 'exploration epsilon: ' + this.epsilon + '<br>';
      t += 'age: ' + this.age + '<br>';
      t += 'average Q-learning loss: ' + this.average_loss_window.get_average() + '<br />';
      t += 'smooth-ish reward: ' + this.average_reward_window.get_average() + '<br />';
      desc.innerHTML = t;
      brainvis.appendChild(desc);
      
      elt.appendChild(brainvis);
    }
  }
  
  global.Brain = Brain;
})(deepqlearn);

(function(lib) {
  "use strict";
  if (typeof module === "undefined" || typeof module.exports === "undefined") {
    window.deepqlearn = lib; // in ordinary browser attach library to window
  } else {
    module.exports = lib; // in nodejs
  }
})(deepqlearn);

},{}],11:[function(require,module,exports){
/*!
 * The MIT License (MIT)
 * 
 * Copyright (c) 2017 Juan Cazala - https://caza.la
 * 
 * Permission is hereby granted, free of charge, to any person obtaining a copy
 * of this software and associated documentation files (the "Software"), to deal
 * in the Software without restriction, including without limitation the rights
 * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 * copies of the Software, and to permit persons to whom the Software is
 * furnished to do so, subject to the following conditions:
 * 
 * The above copyright notice and this permission notice shall be included in
 * all copies or substantial portions of the Software.
 * 
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
 * THE SOFTWARE
 * 
 * 
 * 
 * ********************************************************************************************
 *                                   SYNAPTIC (v1.1.4)
 * ********************************************************************************************
 * 
 * Synaptic is a javascript neural network library for node.js and the browser, its generalized
 * algorithm is architecture-free, so you can build and train basically any type of first order
 * or even second order neural network architectures.
 * 
 * http://en.wikipedia.org/wiki/Recurrent_neural_network#Second_Order_Recurrent_Neural_Network
 * 
 * The library includes a few built-in architectures like multilayer perceptrons, multilayer
 * long-short term memory networks (LSTM) or liquid state machines, and a trainer capable of
 * training any given network, and includes built-in training tasks/tests like solving an XOR,
 * passing a Distracted Sequence Recall test or an Embeded Reber Grammar test.
 * 
 * The algorithm implemented by this library has been taken from Derek D. Monner's paper:
 * 
 * 
 * A generalized LSTM-like training algorithm for second-order recurrent neural networks
 * http://www.overcomplete.net/papers/nn2012.pdf
 * 
 * There are references to the equations in that paper commented through the source code.
 * 
 */
(function webpackUniversalModuleDefinition(root, factory) {
	if(typeof exports === 'object' && typeof module === 'object')
		module.exports = factory();
	else if(typeof define === 'function' && define.amd)
		define([], factory);
	else if(typeof exports === 'object')
		exports["synaptic"] = factory();
	else
		root["synaptic"] = factory();
})(this, function() {
return /******/ (function(modules) { // webpackBootstrap
/******/ 	// The module cache
/******/ 	var installedModules = {};
/******/
/******/ 	// The require function
/******/ 	function __webpack_require__(moduleId) {
/******/
/******/ 		// Check if module is in cache
/******/ 		if(installedModules[moduleId]) {
/******/ 			return installedModules[moduleId].exports;
/******/ 		}
/******/ 		// Create a new module (and put it into the cache)
/******/ 		var module = installedModules[moduleId] = {
/******/ 			i: moduleId,
/******/ 			l: false,
/******/ 			exports: {}
/******/ 		};
/******/
/******/ 		// Execute the module function
/******/ 		modules[moduleId].call(module.exports, module, module.exports, __webpack_require__);
/******/
/******/ 		// Flag the module as loaded
/******/ 		module.l = true;
/******/
/******/ 		// Return the exports of the module
/******/ 		return module.exports;
/******/ 	}
/******/
/******/
/******/ 	// expose the modules object (__webpack_modules__)
/******/ 	__webpack_require__.m = modules;
/******/
/******/ 	// expose the module cache
/******/ 	__webpack_require__.c = installedModules;
/******/
/******/ 	// define getter function for harmony exports
/******/ 	__webpack_require__.d = function(exports, name, getter) {
/******/ 		if(!__webpack_require__.o(exports, name)) {
/******/ 			Object.defineProperty(exports, name, {
/******/ 				configurable: false,
/******/ 				enumerable: true,
/******/ 				get: getter
/******/ 			});
/******/ 		}
/******/ 	};
/******/
/******/ 	// getDefaultExport function for compatibility with non-harmony modules
/******/ 	__webpack_require__.n = function(module) {
/******/ 		var getter = module && module.__esModule ?
/******/ 			function getDefault() { return module['default']; } :
/******/ 			function getModuleExports() { return module; };
/******/ 		__webpack_require__.d(getter, 'a', getter);
/******/ 		return getter;
/******/ 	};
/******/
/******/ 	// Object.prototype.hasOwnProperty.call
/******/ 	__webpack_require__.o = function(object, property) { return Object.prototype.hasOwnProperty.call(object, property); };
/******/
/******/ 	// __webpack_public_path__
/******/ 	__webpack_require__.p = "";
/******/
/******/ 	// Load entry module and return exports
/******/ 	return __webpack_require__(__webpack_require__.s = 4);
/******/ })
/************************************************************************/
/******/ ([
/* 0 */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


Object.defineProperty(exports, "__esModule", {
  value: true
});

var _createClass = function () { function defineProperties(target, props) { for (var i = 0; i < props.length; i++) { var descriptor = props[i]; descriptor.enumerable = descriptor.enumerable || false; descriptor.configurable = true; if ("value" in descriptor) descriptor.writable = true; Object.defineProperty(target, descriptor.key, descriptor); } } return function (Constructor, protoProps, staticProps) { if (protoProps) defineProperties(Constructor.prototype, protoProps); if (staticProps) defineProperties(Constructor, staticProps); return Constructor; }; }();

var _LayerConnection = __webpack_require__(6);

var _LayerConnection2 = _interopRequireDefault(_LayerConnection);

var _Neuron = __webpack_require__(2);

var _Neuron2 = _interopRequireDefault(_Neuron);

var _Network = __webpack_require__(1);

var _Network2 = _interopRequireDefault(_Network);

function _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }

function _classCallCheck(instance, Constructor) { if (!(instance instanceof Constructor)) { throw new TypeError("Cannot call a class as a function"); } }

// types of connections
var connectionType = {
  ALL_TO_ALL: "ALL TO ALL",
  ONE_TO_ONE: "ONE TO ONE",
  ALL_TO_ELSE: "ALL TO ELSE"
};

// types of gates
var gateType = {
  INPUT: "INPUT",
  OUTPUT: "OUTPUT",
  ONE_TO_ONE: "ONE TO ONE"
};

var Layer = function () {
  function Layer(size) {
    _classCallCheck(this, Layer);

    this.size = size | 0;
    this.list = [];

    this.connectedTo = [];

    while (size--) {
      var neuron = new _Neuron2.default();
      this.list.push(neuron);
    }
  }

  // activates all the neurons in the layer


  _createClass(Layer, [{
    key: 'activate',
    value: function activate(input) {

      var activations = [];

      if (typeof input != 'undefined') {
        if (input.length != this.size) throw new Error('INPUT size and LAYER size must be the same to activate!');

        for (var id in this.list) {
          var neuron = this.list[id];
          var activation = neuron.activate(input[id]);
          activations.push(activation);
        }
      } else {
        for (var id in this.list) {
          var neuron = this.list[id];
          var activation = neuron.activate();
          activations.push(activation);
        }
      }
      return activations;
    }

    // propagates the error on all the neurons of the layer

  }, {
    key: 'propagate',
    value: function propagate(rate, target) {

      if (typeof target != 'undefined') {
        if (target.length != this.size) throw new Error('TARGET size and LAYER size must be the same to propagate!');

        for (var id = this.list.length - 1; id >= 0; id--) {
          var neuron = this.list[id];
          neuron.propagate(rate, target[id]);
        }
      } else {
        for (var id = this.list.length - 1; id >= 0; id--) {
          var neuron = this.list[id];
          neuron.propagate(rate);
        }
      }
    }

    // projects a connection from this layer to another one

  }, {
    key: 'project',
    value: function project(layer, type, weights) {

      if (layer instanceof _Network2.default) layer = layer.layers.input;

      if (layer instanceof Layer) {
        if (!this.connected(layer)) return new _LayerConnection2.default(this, layer, type, weights);
      } else throw new Error('Invalid argument, you can only project connections to LAYERS and NETWORKS!');
    }

    // gates a connection betwenn two layers

  }, {
    key: 'gate',
    value: function gate(connection, type) {

      if (type == Layer.gateType.INPUT) {
        if (connection.to.size != this.size) throw new Error('GATER layer and CONNECTION.TO layer must be the same size in order to gate!');

        for (var id in connection.to.list) {
          var neuron = connection.to.list[id];
          var gater = this.list[id];
          for (var input in neuron.connections.inputs) {
            var gated = neuron.connections.inputs[input];
            if (gated.ID in connection.connections) gater.gate(gated);
          }
        }
      } else if (type == Layer.gateType.OUTPUT) {
        if (connection.from.size != this.size) throw new Error('GATER layer and CONNECTION.FROM layer must be the same size in order to gate!');

        for (var id in connection.from.list) {
          var neuron = connection.from.list[id];
          var gater = this.list[id];
          for (var projected in neuron.connections.projected) {
            var gated = neuron.connections.projected[projected];
            if (gated.ID in connection.connections) gater.gate(gated);
          }
        }
      } else if (type == Layer.gateType.ONE_TO_ONE) {
        if (connection.size != this.size) throw new Error('The number of GATER UNITS must be the same as the number of CONNECTIONS to gate!');

        for (var id in connection.list) {
          var gater = this.list[id];
          var gated = connection.list[id];
          gater.gate(gated);
        }
      }
      connection.gatedfrom.push({ layer: this, type: type });
    }

    // true or false whether the whole layer is self-connected or not

  }, {
    key: 'selfconnected',
    value: function selfconnected() {

      for (var id in this.list) {
        var neuron = this.list[id];
        if (!neuron.selfconnected()) return false;
      }
      return true;
    }

    // true of false whether the layer is connected to another layer (parameter) or not

  }, {
    key: 'connected',
    value: function connected(layer) {
      // Check if ALL to ALL connection
      var connections = 0;
      for (var here in this.list) {
        for (var there in layer.list) {
          var from = this.list[here];
          var to = layer.list[there];
          var connected = from.connected(to);
          if (connected.type == 'projected') connections++;
        }
      }
      if (connections == this.size * layer.size) return Layer.connectionType.ALL_TO_ALL;

      // Check if ONE to ONE connection
      connections = 0;
      for (var neuron in this.list) {
        var from = this.list[neuron];
        var to = layer.list[neuron];
        var connected = from.connected(to);
        if (connected.type == 'projected') connections++;
      }
      if (connections == this.size) return Layer.connectionType.ONE_TO_ONE;
    }

    // clears all the neuorns in the layer

  }, {
    key: 'clear',
    value: function clear() {
      for (var id in this.list) {
        var neuron = this.list[id];
        neuron.clear();
      }
    }

    // resets all the neurons in the layer

  }, {
    key: 'reset',
    value: function reset() {
      for (var id in this.list) {
        var neuron = this.list[id];
        neuron.reset();
      }
    }

    // returns all the neurons in the layer (array)

  }, {
    key: 'neurons',
    value: function neurons() {
      return this.list;
    }

    // adds a neuron to the layer

  }, {
    key: 'add',
    value: function add(neuron) {
      neuron = neuron || new _Neuron2.default();
      this.list.push(neuron);
      this.size++;
    }
  }, {
    key: 'set',
    value: function set(options) {
      options = options || {};

      for (var i in this.list) {
        var neuron = this.list[i];
        if (options.label) neuron.label = options.label + '_' + neuron.ID;
        if (options.squash) neuron.squash = options.squash;
        if (options.bias) neuron.bias = options.bias;
      }
      return this;
    }
  }]);

  return Layer;
}();

Layer.connectionType = connectionType;
Layer.gateType = gateType;
exports.default = Layer;

/***/ }),
/* 1 */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


Object.defineProperty(exports, "__esModule", {
  value: true
});

var _typeof = typeof Symbol === "function" && typeof Symbol.iterator === "symbol" ? function (obj) { return typeof obj; } : function (obj) { return obj && typeof Symbol === "function" && obj.constructor === Symbol && obj !== Symbol.prototype ? "symbol" : typeof obj; };

var _createClass = function () { function defineProperties(target, props) { for (var i = 0; i < props.length; i++) { var descriptor = props[i]; descriptor.enumerable = descriptor.enumerable || false; descriptor.configurable = true; if ("value" in descriptor) descriptor.writable = true; Object.defineProperty(target, descriptor.key, descriptor); } } return function (Constructor, protoProps, staticProps) { if (protoProps) defineProperties(Constructor.prototype, protoProps); if (staticProps) defineProperties(Constructor, staticProps); return Constructor; }; }();

var _Neuron = __webpack_require__(2);

var _Neuron2 = _interopRequireDefault(_Neuron);

var _Layer = __webpack_require__(0);

var _Layer2 = _interopRequireDefault(_Layer);

var _Trainer = __webpack_require__(3);

var _Trainer2 = _interopRequireDefault(_Trainer);

function _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }

function _classCallCheck(instance, Constructor) { if (!(instance instanceof Constructor)) { throw new TypeError("Cannot call a class as a function"); } }

var Network = function () {
  function Network(layers) {
    _classCallCheck(this, Network);

    if (typeof layers != 'undefined') {
      this.layers = {
        input: layers.input || null,
        hidden: layers.hidden || [],
        output: layers.output || null
      };
      this.optimized = null;
    }
  }

  // feed-forward activation of all the layers to produce an ouput


  _createClass(Network, [{
    key: 'activate',
    value: function activate(input) {
      if (this.optimized === false) {
        this.layers.input.activate(input);
        for (var i = 0; i < this.layers.hidden.length; i++) {
          this.layers.hidden[i].activate();
        }return this.layers.output.activate();
      } else {
        if (this.optimized == null) this.optimize();
        return this.optimized.activate(input);
      }
    }

    // back-propagate the error thru the network

  }, {
    key: 'propagate',
    value: function propagate(rate, target) {
      if (this.optimized === false) {
        this.layers.output.propagate(rate, target);
        for (var i = this.layers.hidden.length - 1; i >= 0; i--) {
          this.layers.hidden[i].propagate(rate);
        }
      } else {
        if (this.optimized == null) this.optimize();
        this.optimized.propagate(rate, target);
      }
    }

    // project a connection to another unit (either a network or a layer)

  }, {
    key: 'project',
    value: function project(unit, type, weights) {
      if (this.optimized) this.optimized.reset();

      if (unit instanceof Network) return this.layers.output.project(unit.layers.input, type, weights);

      if (unit instanceof _Layer2.default) return this.layers.output.project(unit, type, weights);

      throw new Error('Invalid argument, you can only project connections to LAYERS and NETWORKS!');
    }

    // let this network gate a connection

  }, {
    key: 'gate',
    value: function gate(connection, type) {
      if (this.optimized) this.optimized.reset();
      this.layers.output.gate(connection, type);
    }

    // clear all elegibility traces and extended elegibility traces (the network forgets its context, but not what was trained)

  }, {
    key: 'clear',
    value: function clear() {
      this.restore();

      var inputLayer = this.layers.input,
          outputLayer = this.layers.output;

      inputLayer.clear();
      for (var i = 0; i < this.layers.hidden.length; i++) {
        this.layers.hidden[i].clear();
      }
      outputLayer.clear();

      if (this.optimized) this.optimized.reset();
    }

    // reset all weights and clear all traces (ends up like a new network)

  }, {
    key: 'reset',
    value: function reset() {
      this.restore();

      var inputLayer = this.layers.input,
          outputLayer = this.layers.output;

      inputLayer.reset();
      for (var i = 0; i < this.layers.hidden.length; i++) {
        this.layers.hidden[i].reset();
      }
      outputLayer.reset();

      if (this.optimized) this.optimized.reset();
    }

    // hardcodes the behaviour of the whole network into a single optimized function

  }, {
    key: 'optimize',
    value: function optimize() {
      var that = this;
      var optimized = {};
      var neurons = this.neurons();

      for (var i = 0; i < neurons.length; i++) {
        var neuron = neurons[i].neuron;
        var layer = neurons[i].layer;
        while (neuron.neuron) {
          neuron = neuron.neuron;
        }optimized = neuron.optimize(optimized, layer);
      }

      for (var i = 0; i < optimized.propagation_sentences.length; i++) {
        optimized.propagation_sentences[i].reverse();
      }optimized.propagation_sentences.reverse();

      var hardcode = '';
      hardcode += 'var F = Float64Array ? new Float64Array(' + optimized.memory + ') : []; ';
      for (var i in optimized.variables) {
        hardcode += 'F[' + optimized.variables[i].id + '] = ' + (optimized.variables[i].value || 0) + '; ';
      }hardcode += 'var activate = function(input){\n';
      for (var i = 0; i < optimized.inputs.length; i++) {
        hardcode += 'F[' + optimized.inputs[i] + '] = input[' + i + ']; ';
      }for (var i = 0; i < optimized.activation_sentences.length; i++) {
        if (optimized.activation_sentences[i].length > 0) {
          for (var j = 0; j < optimized.activation_sentences[i].length; j++) {
            hardcode += optimized.activation_sentences[i][j].join(' ');
            hardcode += optimized.trace_sentences[i][j].join(' ');
          }
        }
      }
      hardcode += ' var output = []; ';
      for (var i = 0; i < optimized.outputs.length; i++) {
        hardcode += 'output[' + i + '] = F[' + optimized.outputs[i] + ']; ';
      }hardcode += 'return output; }; ';
      hardcode += 'var propagate = function(rate, target){\n';
      hardcode += 'F[' + optimized.variables.rate.id + '] = rate; ';
      for (var i = 0; i < optimized.targets.length; i++) {
        hardcode += 'F[' + optimized.targets[i] + '] = target[' + i + ']; ';
      }for (var i = 0; i < optimized.propagation_sentences.length; i++) {
        for (var j = 0; j < optimized.propagation_sentences[i].length; j++) {
          hardcode += optimized.propagation_sentences[i][j].join(' ') + ' ';
        }
      }hardcode += ' };\n';
      hardcode += 'var ownership = function(memoryBuffer){\nF = memoryBuffer;\nthis.memory = F;\n};\n';
      hardcode += 'return {\nmemory: F,\nactivate: activate,\npropagate: propagate,\nownership: ownership\n};';
      hardcode = hardcode.split(';').join(';\n');

      var constructor = new Function(hardcode);

      var network = constructor();
      network.data = {
        variables: optimized.variables,
        activate: optimized.activation_sentences,
        propagate: optimized.propagation_sentences,
        trace: optimized.trace_sentences,
        inputs: optimized.inputs,
        outputs: optimized.outputs,
        check_activation: this.activate,
        check_propagation: this.propagate
      };

      network.reset = function () {
        if (that.optimized) {
          that.optimized = null;
          that.activate = network.data.check_activation;
          that.propagate = network.data.check_propagation;
        }
      };

      this.optimized = network;
      this.activate = network.activate;
      this.propagate = network.propagate;
    }

    // restores all the values from the optimized network the their respective objects in order to manipulate the network

  }, {
    key: 'restore',
    value: function restore() {
      if (!this.optimized) return;

      var optimized = this.optimized;

      var getValue = function getValue() {
        var args = Array.prototype.slice.call(arguments);

        var unit = args.shift();
        var prop = args.pop();

        var id = prop + '_';
        for (var property in args) {
          id += args[property] + '_';
        }id += unit.ID;

        var memory = optimized.memory;
        var variables = optimized.data.variables;

        if (id in variables) return memory[variables[id].id];
        return 0;
      };

      var list = this.neurons();

      // link id's to positions in the array
      for (var i = 0; i < list.length; i++) {
        var neuron = list[i].neuron;
        while (neuron.neuron) {
          neuron = neuron.neuron;
        }neuron.state = getValue(neuron, 'state');
        neuron.old = getValue(neuron, 'old');
        neuron.activation = getValue(neuron, 'activation');
        neuron.bias = getValue(neuron, 'bias');

        for (var input in neuron.trace.elegibility) {
          neuron.trace.elegibility[input] = getValue(neuron, 'trace', 'elegibility', input);
        }for (var gated in neuron.trace.extended) {
          for (var input in neuron.trace.extended[gated]) {
            neuron.trace.extended[gated][input] = getValue(neuron, 'trace', 'extended', gated, input);
          }
        } // get connections
        for (var j in neuron.connections.projected) {
          var connection = neuron.connections.projected[j];
          connection.weight = getValue(connection, 'weight');
          connection.gain = getValue(connection, 'gain');
        }
      }
    }

    // returns all the neurons in the network

  }, {
    key: 'neurons',
    value: function neurons() {
      var neurons = [];

      var inputLayer = this.layers.input.neurons(),
          outputLayer = this.layers.output.neurons();

      for (var i = 0; i < inputLayer.length; i++) {
        neurons.push({
          neuron: inputLayer[i],
          layer: 'input'
        });
      }

      for (var i = 0; i < this.layers.hidden.length; i++) {
        var hiddenLayer = this.layers.hidden[i].neurons();
        for (var j = 0; j < hiddenLayer.length; j++) {
          neurons.push({
            neuron: hiddenLayer[j],
            layer: i
          });
        }
      }

      for (var i = 0; i < outputLayer.length; i++) {
        neurons.push({
          neuron: outputLayer[i],
          layer: 'output'
        });
      }

      return neurons;
    }

    // returns number of inputs of the network

  }, {
    key: 'inputs',
    value: function inputs() {
      return this.layers.input.size;
    }

    // returns number of outputs of hte network

  }, {
    key: 'outputs',
    value: function outputs() {
      return this.layers.output.size;
    }

    // sets the layers of the network

  }, {
    key: 'set',
    value: function set(layers) {
      this.layers = {
        input: layers.input || null,
        hidden: layers.hidden || [],
        output: layers.output || null
      };
      if (this.optimized) this.optimized.reset();
    }
  }, {
    key: 'setOptimize',
    value: function setOptimize(bool) {
      this.restore();
      if (this.optimized) this.optimized.reset();
      this.optimized = bool ? null : false;
    }

    // returns a json that represents all the neurons and connections of the network

  }, {
    key: 'toJSON',
    value: function toJSON(ignoreTraces) {
      this.restore();

      var list = this.neurons();
      var neurons = [];
      var connections = [];

      // link id's to positions in the array
      var ids = {};
      for (var i = 0; i < list.length; i++) {
        var neuron = list[i].neuron;
        while (neuron.neuron) {
          neuron = neuron.neuron;
        }ids[neuron.ID] = i;

        var copy = {
          trace: {
            elegibility: {},
            extended: {}
          },
          state: neuron.state,
          old: neuron.old,
          activation: neuron.activation,
          bias: neuron.bias,
          layer: list[i].layer
        };

        copy.squash = neuron.squash == _Neuron2.default.squash.LOGISTIC ? 'LOGISTIC' : neuron.squash == _Neuron2.default.squash.TANH ? 'TANH' : neuron.squash == _Neuron2.default.squash.IDENTITY ? 'IDENTITY' : neuron.squash == _Neuron2.default.squash.HLIM ? 'HLIM' : neuron.squash == _Neuron2.default.squash.RELU ? 'RELU' : null;

        neurons.push(copy);
      }

      for (var i = 0; i < list.length; i++) {
        var neuron = list[i].neuron;
        while (neuron.neuron) {
          neuron = neuron.neuron;
        }for (var j in neuron.connections.projected) {
          var connection = neuron.connections.projected[j];
          connections.push({
            from: ids[connection.from.ID],
            to: ids[connection.to.ID],
            weight: connection.weight,
            gater: connection.gater ? ids[connection.gater.ID] : null
          });
        }
        if (neuron.selfconnected()) {
          connections.push({
            from: ids[neuron.ID],
            to: ids[neuron.ID],
            weight: neuron.selfconnection.weight,
            gater: neuron.selfconnection.gater ? ids[neuron.selfconnection.gater.ID] : null
          });
        }
      }

      return {
        neurons: neurons,
        connections: connections
      };
    }

    // export the topology into dot language which can be visualized as graphs using dot
    /* example: ... console.log(net.toDotLang());
                $ node example.js > example.dot
                $ dot example.dot -Tpng > out.png
    */

  }, {
    key: 'toDot',
    value: function toDot(edgeConnection) {
      if (!(typeof edgeConnection === 'undefined' ? 'undefined' : _typeof(edgeConnection))) edgeConnection = false;
      var code = 'digraph nn {\n    rankdir = BT\n';
      var layers = [this.layers.input].concat(this.layers.hidden, this.layers.output);
      for (var i = 0; i < layers.length; i++) {
        for (var j = 0; j < layers[i].connectedTo.length; j++) {
          // projections
          var connection = layers[i].connectedTo[j];
          var layerTo = connection.to;
          var size = connection.size;
          var layerID = layers.indexOf(layers[i]);
          var layerToID = layers.indexOf(layerTo);
          /* http://stackoverflow.com/questions/26845540/connect-edges-with-graph-dot
           * DOT does not support edge-to-edge connections
           * This workaround produces somewhat weird graphs ...
          */
          if (edgeConnection) {
            if (connection.gatedfrom.length) {
              var fakeNode = 'fake' + layerID + '_' + layerToID;
              code += '    ' + fakeNode + ' [label = "", shape = point, width = 0.01, height = 0.01]\n';
              code += '    ' + layerID + ' -> ' + fakeNode + ' [label = ' + size + ', arrowhead = none]\n';
              code += '    ' + fakeNode + ' -> ' + layerToID + '\n';
            } else code += '    ' + layerID + ' -> ' + layerToID + ' [label = ' + size + ']\n';
            for (var from in connection.gatedfrom) {
              // gatings
              var layerfrom = connection.gatedfrom[from].layer;
              var layerfromID = layers.indexOf(layerfrom);
              code += '    ' + layerfromID + ' -> ' + fakeNode + ' [color = blue]\n';
            }
          } else {
            code += '    ' + layerID + ' -> ' + layerToID + ' [label = ' + size + ']\n';
            for (var from in connection.gatedfrom) {
              // gatings
              var layerfrom = connection.gatedfrom[from].layer;
              var layerfromID = layers.indexOf(layerfrom);
              code += '    ' + layerfromID + ' -> ' + layerToID + ' [color = blue]\n';
            }
          }
        }
      }
      code += '}\n';
      return {
        code: code,
        link: 'https://chart.googleapis.com/chart?chl=' + escape(code.replace('/ /g', '+')) + '&cht=gv'
      };
    }

    // returns a function that works as the activation of the network and can be used without depending on the library

  }, {
    key: 'standalone',
    value: function standalone() {
      if (!this.optimized) this.optimize();

      var data = this.optimized.data;

      // build activation function
      var activation = 'function (input) {\n';

      // build inputs
      for (var i = 0; i < data.inputs.length; i++) {
        activation += 'F[' + data.inputs[i] + '] = input[' + i + '];\n';
      } // build network activation
      for (var i = 0; i < data.activate.length; i++) {
        // shouldn't this be layer?
        for (var j = 0; j < data.activate[i].length; j++) {
          activation += data.activate[i][j].join('') + '\n';
        }
      }

      // build outputs
      activation += 'var output = [];\n';
      for (var i = 0; i < data.outputs.length; i++) {
        activation += 'output[' + i + '] = F[' + data.outputs[i] + '];\n';
      }activation += 'return output;\n}';

      // reference all the positions in memory
      var memory = activation.match(/F\[(\d+)\]/g);
      var dimension = 0;
      var ids = {};

      for (var i = 0; i < memory.length; i++) {
        var tmp = memory[i].match(/\d+/)[0];
        if (!(tmp in ids)) {
          ids[tmp] = dimension++;
        }
      }
      var hardcode = 'F = {\n';

      for (var i in ids) {
        hardcode += ids[i] + ': ' + this.optimized.memory[i] + ',\n';
      }hardcode = hardcode.substring(0, hardcode.length - 2) + '\n};\n';
      hardcode = 'var run = ' + activation.replace(/F\[(\d+)]/g, function (index) {
        return 'F[' + ids[index.match(/\d+/)[0]] + ']';
      }).replace('{\n', '{\n' + hardcode + '') + ';\n';
      hardcode += 'return run';

      // return standalone function
      return new Function(hardcode)();
    }

    // Return a HTML5 WebWorker specialized on training the network stored in `memory`.
    // Train based on the given dataSet and options.
    // The worker returns the updated `memory` when done.

  }, {
    key: 'worker',
    value: function worker(memory, set, options) {
      // Copy the options and set defaults (options might be different for each worker)
      var workerOptions = {};
      if (options) workerOptions = options;
      workerOptions.rate = workerOptions.rate || .2;
      workerOptions.iterations = workerOptions.iterations || 100000;
      workerOptions.error = workerOptions.error || .005;
      workerOptions.cost = workerOptions.cost || null;
      workerOptions.crossValidate = workerOptions.crossValidate || null;

      // Cost function might be different for each worker
      var costFunction = '// REPLACED BY WORKER\nvar cost = ' + (options && options.cost || this.cost || _Trainer2.default.cost.MSE) + ';\n';
      var workerFunction = Network.getWorkerSharedFunctions();
      workerFunction = workerFunction.replace(/var cost = options && options\.cost \|\| this\.cost \|\| Trainer\.cost\.MSE;/g, costFunction);

      // Set what we do when training is finished
      workerFunction = workerFunction.replace('return results;', 'postMessage({action: "done", message: results, memoryBuffer: F}, [F.buffer]);');

      // Replace log with postmessage
      workerFunction = workerFunction.replace('console.log(\'iterations\', iterations, \'error\', error, \'rate\', currentRate)', 'postMessage({action: \'log\', message: {\n' + 'iterations: iterations,\n' + 'error: error,\n' + 'rate: currentRate\n' + '}\n' + '})');

      // Replace schedule with postmessage
      workerFunction = workerFunction.replace('abort = this.schedule.do({ error: error, iterations: iterations, rate: currentRate })', 'postMessage({action: \'schedule\', message: {\n' + 'iterations: iterations,\n' + 'error: error,\n' + 'rate: currentRate\n' + '}\n' + '})');

      if (!this.optimized) this.optimize();

      var hardcode = 'var inputs = ' + this.optimized.data.inputs.length + ';\n';
      hardcode += 'var outputs = ' + this.optimized.data.outputs.length + ';\n';
      hardcode += 'var F =  new Float64Array([' + this.optimized.memory.toString() + ']);\n';
      hardcode += 'var activate = ' + this.optimized.activate.toString() + ';\n';
      hardcode += 'var propagate = ' + this.optimized.propagate.toString() + ';\n';
      hardcode += 'onmessage = function(e) {\n' + 'if (e.data.action == \'startTraining\') {\n' + 'train(' + JSON.stringify(set) + ',' + JSON.stringify(workerOptions) + ');\n' + '}\n' + '}';

      var workerSourceCode = workerFunction + '\n' + hardcode;
      var blob = new Blob([workerSourceCode]);
      var blobURL = window.URL.createObjectURL(blob);

      return new Worker(blobURL);
    }

    // returns a copy of the network

  }, {
    key: 'clone',
    value: function clone() {
      return Network.fromJSON(this.toJSON());
    }

    /**
     * Creates a static String to store the source code of the functions
     *  that are identical for all the workers (train, _trainSet, test)
     *
     * @return {String} Source code that can train a network inside a worker.
     * @static
     */

  }], [{
    key: 'getWorkerSharedFunctions',
    value: function getWorkerSharedFunctions() {
      // If we already computed the source code for the shared functions
      if (typeof Network._SHARED_WORKER_FUNCTIONS !== 'undefined') return Network._SHARED_WORKER_FUNCTIONS;

      // Otherwise compute and return the source code
      // We compute them by simply copying the source code of the train, _trainSet and test functions
      //  using the .toString() method

      // Load and name the train function
      var train_f = _Trainer2.default.prototype.train.toString();
      train_f = train_f.replace(/this._trainSet/g, '_trainSet');
      train_f = train_f.replace(/this.test/g, 'test');
      train_f = train_f.replace(/this.crossValidate/g, 'crossValidate');
      train_f = train_f.replace('crossValidate = true', '// REMOVED BY WORKER');

      // Load and name the _trainSet function
      var _trainSet_f = _Trainer2.default.prototype._trainSet.toString().replace(/this.network./g, '');

      // Load and name the test function
      var test_f = _Trainer2.default.prototype.test.toString().replace(/this.network./g, '');

      return Network._SHARED_WORKER_FUNCTIONS = train_f + '\n' + _trainSet_f + '\n' + test_f;
    }
  }, {
    key: 'fromJSON',
    value: function fromJSON(json) {
      var neurons = [];

      var layers = {
        input: new _Layer2.default(),
        hidden: [],
        output: new _Layer2.default()
      };

      for (var i = 0; i < json.neurons.length; i++) {
        var config = json.neurons[i];

        var neuron = new _Neuron2.default();
        neuron.trace.elegibility = {};
        neuron.trace.extended = {};
        neuron.state = config.state;
        neuron.old = config.old;
        neuron.activation = config.activation;
        neuron.bias = config.bias;
        neuron.squash = config.squash in _Neuron2.default.squash ? _Neuron2.default.squash[config.squash] : _Neuron2.default.squash.LOGISTIC;
        neurons.push(neuron);

        if (config.layer == 'input') layers.input.add(neuron);else if (config.layer == 'output') layers.output.add(neuron);else {
          if (typeof layers.hidden[config.layer] == 'undefined') layers.hidden[config.layer] = new _Layer2.default();
          layers.hidden[config.layer].add(neuron);
        }
      }

      for (var i = 0; i < json.connections.length; i++) {
        var config = json.connections[i];
        var from = neurons[config.from];
        var to = neurons[config.to];
        var weight = config.weight;
        var gater = neurons[config.gater];

        var connection = from.project(to, weight);
        if (gater) gater.gate(connection);
      }

      return new Network(layers);
    }
  }]);

  return Network;
}();

exports.default = Network;

/***/ }),
/* 2 */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


Object.defineProperty(exports, "__esModule", {
  value: true
});

var _createClass = function () { function defineProperties(target, props) { for (var i = 0; i < props.length; i++) { var descriptor = props[i]; descriptor.enumerable = descriptor.enumerable || false; descriptor.configurable = true; if ("value" in descriptor) descriptor.writable = true; Object.defineProperty(target, descriptor.key, descriptor); } } return function (Constructor, protoProps, staticProps) { if (protoProps) defineProperties(Constructor.prototype, protoProps); if (staticProps) defineProperties(Constructor, staticProps); return Constructor; }; }();

var _Connection = __webpack_require__(5);

var _Connection2 = _interopRequireDefault(_Connection);

function _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }

function _classCallCheck(instance, Constructor) { if (!(instance instanceof Constructor)) { throw new TypeError("Cannot call a class as a function"); } }

var neurons = 0;

// squashing functions
var squash = {
  // eq. 5 & 5'
  LOGISTIC: function LOGISTIC(x, derivate) {
    var fx = 1 / (1 + Math.exp(-x));
    if (!derivate) return fx;
    return fx * (1 - fx);
  },
  TANH: function TANH(x, derivate) {
    if (derivate) return 1 - Math.pow(Math.tanh(x), 2);
    return Math.tanh(x);
  },
  IDENTITY: function IDENTITY(x, derivate) {
    return derivate ? 1 : x;
  },
  HLIM: function HLIM(x, derivate) {
    return derivate ? 1 : x > 0 ? 1 : 0;
  },
  RELU: function RELU(x, derivate) {
    if (derivate) return x > 0 ? 1 : 0;
    return x > 0 ? x : 0;
  }
};

var Neuron = function () {
  function Neuron() {
    _classCallCheck(this, Neuron);

    this.ID = Neuron.uid();

    this.connections = {
      inputs: {},
      projected: {},
      gated: {}
    };
    this.error = {
      responsibility: 0,
      projected: 0,
      gated: 0
    };
    this.trace = {
      elegibility: {},
      extended: {},
      influences: {}
    };
    this.state = 0;
    this.old = 0;
    this.activation = 0;
    this.selfconnection = new _Connection2.default(this, this, 0); // weight = 0 -> not connected
    this.squash = Neuron.squash.LOGISTIC;
    this.neighboors = {};
    this.bias = Math.random() * .2 - .1;
  }

  // activate the neuron


  _createClass(Neuron, [{
    key: 'activate',
    value: function activate(input) {
      // activation from enviroment (for input neurons)
      if (typeof input != 'undefined') {
        this.activation = input;
        this.derivative = 0;
        this.bias = 0;
        return this.activation;
      }

      // old state
      this.old = this.state;

      // eq. 15
      this.state = this.selfconnection.gain * this.selfconnection.weight * this.state + this.bias;

      for (var i in this.connections.inputs) {
        var input = this.connections.inputs[i];
        this.state += input.from.activation * input.weight * input.gain;
      }

      // eq. 16
      this.activation = this.squash(this.state);

      // f'(s)
      this.derivative = this.squash(this.state, true);

      // update traces
      var influences = [];
      for (var id in this.trace.extended) {
        // extended elegibility trace
        var neuron = this.neighboors[id];

        // if gated neuron's selfconnection is gated by this unit, the influence keeps track of the neuron's old state
        var influence = neuron.selfconnection.gater == this ? neuron.old : 0;

        // index runs over all the incoming connections to the gated neuron that are gated by this unit
        for (var incoming in this.trace.influences[neuron.ID]) {
          // captures the effect that has an input connection to this unit, on a neuron that is gated by this unit
          influence += this.trace.influences[neuron.ID][incoming].weight * this.trace.influences[neuron.ID][incoming].from.activation;
        }
        influences[neuron.ID] = influence;
      }

      for (var i in this.connections.inputs) {
        var input = this.connections.inputs[i];

        // elegibility trace - Eq. 17
        this.trace.elegibility[input.ID] = this.selfconnection.gain * this.selfconnection.weight * this.trace.elegibility[input.ID] + input.gain * input.from.activation;

        for (var id in this.trace.extended) {
          // extended elegibility trace
          var xtrace = this.trace.extended[id];
          var neuron = this.neighboors[id];
          var influence = influences[neuron.ID];

          // eq. 18
          xtrace[input.ID] = neuron.selfconnection.gain * neuron.selfconnection.weight * xtrace[input.ID] + this.derivative * this.trace.elegibility[input.ID] * influence;
        }
      }

      //  update gated connection's gains
      for (var connection in this.connections.gated) {
        this.connections.gated[connection].gain = this.activation;
      }

      return this.activation;
    }

    // back-propagate the error

  }, {
    key: 'propagate',
    value: function propagate(rate, target) {
      // error accumulator
      var error = 0;

      // whether or not this neuron is in the output layer
      var isOutput = typeof target != 'undefined';

      // output neurons get their error from the enviroment
      if (isOutput) this.error.responsibility = this.error.projected = target - this.activation; // Eq. 10

      else // the rest of the neuron compute their error responsibilities by backpropagation
        {
          // error responsibilities from all the connections projected from this neuron
          for (var id in this.connections.projected) {
            var connection = this.connections.projected[id];
            var neuron = connection.to;
            // Eq. 21
            error += neuron.error.responsibility * connection.gain * connection.weight;
          }

          // projected error responsibility
          this.error.projected = this.derivative * error;

          error = 0;
          // error responsibilities from all the connections gated by this neuron
          for (var id in this.trace.extended) {
            var neuron = this.neighboors[id]; // gated neuron
            var influence = neuron.selfconnection.gater == this ? neuron.old : 0; // if gated neuron's selfconnection is gated by this neuron

            // index runs over all the connections to the gated neuron that are gated by this neuron
            for (var input in this.trace.influences[id]) {
              // captures the effect that the input connection of this neuron have, on a neuron which its input/s is/are gated by this neuron
              influence += this.trace.influences[id][input].weight * this.trace.influences[neuron.ID][input].from.activation;
            }
            // eq. 22
            error += neuron.error.responsibility * influence;
          }

          // gated error responsibility
          this.error.gated = this.derivative * error;

          // error responsibility - Eq. 23
          this.error.responsibility = this.error.projected + this.error.gated;
        }

      // learning rate
      rate = rate || .1;

      // adjust all the neuron's incoming connections
      for (var id in this.connections.inputs) {
        var input = this.connections.inputs[id];

        // Eq. 24
        var gradient = this.error.projected * this.trace.elegibility[input.ID];
        for (var id in this.trace.extended) {
          var neuron = this.neighboors[id];
          gradient += neuron.error.responsibility * this.trace.extended[neuron.ID][input.ID];
        }
        input.weight += rate * gradient; // adjust weights - aka learn
      }

      // adjust bias
      this.bias += rate * this.error.responsibility;
    }
  }, {
    key: 'project',
    value: function project(neuron, weight) {
      // self-connection
      if (neuron == this) {
        this.selfconnection.weight = 1;
        return this.selfconnection;
      }

      // check if connection already exists
      var connected = this.connected(neuron);
      if (connected && connected.type == 'projected') {
        // update connection
        if (typeof weight != 'undefined') connected.connection.weight = weight;
        // return existing connection
        return connected.connection;
      } else {
        // create a new connection
        var connection = new _Connection2.default(this, neuron, weight);
      }

      // reference all the connections and traces
      this.connections.projected[connection.ID] = connection;
      this.neighboors[neuron.ID] = neuron;
      neuron.connections.inputs[connection.ID] = connection;
      neuron.trace.elegibility[connection.ID] = 0;

      for (var id in neuron.trace.extended) {
        var trace = neuron.trace.extended[id];
        trace[connection.ID] = 0;
      }

      return connection;
    }
  }, {
    key: 'gate',
    value: function gate(connection) {
      // add connection to gated list
      this.connections.gated[connection.ID] = connection;

      var neuron = connection.to;
      if (!(neuron.ID in this.trace.extended)) {
        // extended trace
        this.neighboors[neuron.ID] = neuron;
        var xtrace = this.trace.extended[neuron.ID] = {};
        for (var id in this.connections.inputs) {
          var input = this.connections.inputs[id];
          xtrace[input.ID] = 0;
        }
      }

      // keep track
      if (neuron.ID in this.trace.influences) this.trace.influences[neuron.ID].push(connection);else this.trace.influences[neuron.ID] = [connection];

      // set gater
      connection.gater = this;
    }

    // returns true or false whether the neuron is self-connected or not

  }, {
    key: 'selfconnected',
    value: function selfconnected() {
      return this.selfconnection.weight !== 0;
    }

    // returns true or false whether the neuron is connected to another neuron (parameter)

  }, {
    key: 'connected',
    value: function connected(neuron) {
      var result = {
        type: null,
        connection: false
      };

      if (this == neuron) {
        if (this.selfconnected()) {
          result.type = 'selfconnection';
          result.connection = this.selfconnection;
          return result;
        } else return false;
      }

      for (var type in this.connections) {
        for (var connection in this.connections[type]) {
          var connection = this.connections[type][connection];
          if (connection.to == neuron) {
            result.type = type;
            result.connection = connection;
            return result;
          } else if (connection.from == neuron) {
            result.type = type;
            result.connection = connection;
            return result;
          }
        }
      }

      return false;
    }

    // clears all the traces (the neuron forgets it's context, but the connections remain intact)

  }, {
    key: 'clear',
    value: function clear() {
      for (var trace in this.trace.elegibility) {
        this.trace.elegibility[trace] = 0;
      }

      for (var trace in this.trace.extended) {
        for (var extended in this.trace.extended[trace]) {
          this.trace.extended[trace][extended] = 0;
        }
      }

      this.error.responsibility = this.error.projected = this.error.gated = 0;
    }

    // all the connections are randomized and the traces are cleared

  }, {
    key: 'reset',
    value: function reset() {
      this.clear();

      for (var type in this.connections) {
        for (var connection in this.connections[type]) {
          this.connections[type][connection].weight = Math.random() * .2 - .1;
        }
      }

      this.bias = Math.random() * .2 - .1;
      this.old = this.state = this.activation = 0;
    }

    // hardcodes the behaviour of the neuron into an optimized function

  }, {
    key: 'optimize',
    value: function optimize(optimized, layer) {

      optimized = optimized || {};
      var store_activation = [];
      var store_trace = [];
      var store_propagation = [];
      var varID = optimized.memory || 0;
      var neurons = optimized.neurons || 1;
      var inputs = optimized.inputs || [];
      var targets = optimized.targets || [];
      var outputs = optimized.outputs || [];
      var variables = optimized.variables || {};
      var activation_sentences = optimized.activation_sentences || [];
      var trace_sentences = optimized.trace_sentences || [];
      var propagation_sentences = optimized.propagation_sentences || [];
      var layers = optimized.layers || { __count: 0, __neuron: 0 };

      // allocate sentences
      var allocate = function allocate(store) {
        var allocated = layer in layers && store[layers.__count];
        if (!allocated) {
          layers.__count = store.push([]) - 1;
          layers[layer] = layers.__count;
        }
      };
      allocate(activation_sentences);
      allocate(trace_sentences);
      allocate(propagation_sentences);
      var currentLayer = layers.__count;

      // get/reserve space in memory by creating a unique ID for a variablel
      var getVar = function getVar() {
        var args = Array.prototype.slice.call(arguments);

        if (args.length == 1) {
          if (args[0] == 'target') {
            var id = 'target_' + targets.length;
            targets.push(varID);
          } else var id = args[0];
          if (id in variables) return variables[id];
          return variables[id] = {
            value: 0,
            id: varID++
          };
        } else {
          var extended = args.length > 2;
          if (extended) var value = args.pop();

          var unit = args.shift();
          var prop = args.pop();

          if (!extended) var value = unit[prop];

          var id = prop + '_';
          for (var i = 0; i < args.length; i++) {
            id += args[i] + '_';
          }id += unit.ID;
          if (id in variables) return variables[id];

          return variables[id] = {
            value: value,
            id: varID++
          };
        }
      };

      // build sentence
      var buildSentence = function buildSentence() {
        var args = Array.prototype.slice.call(arguments);
        var store = args.pop();
        var sentence = '';
        for (var i = 0; i < args.length; i++) {
          if (typeof args[i] == 'string') sentence += args[i];else sentence += 'F[' + args[i].id + ']';
        }store.push(sentence + ';');
      };

      // helper to check if an object is empty
      var isEmpty = function isEmpty(obj) {
        for (var prop in obj) {
          if (obj.hasOwnProperty(prop)) return false;
        }
        return true;
      };

      // characteristics of the neuron
      var noProjections = isEmpty(this.connections.projected);
      var noGates = isEmpty(this.connections.gated);
      var isInput = layer == 'input' ? true : isEmpty(this.connections.inputs);
      var isOutput = layer == 'output' ? true : noProjections && noGates;

      // optimize neuron's behaviour
      var rate = getVar('rate');
      var activation = getVar(this, 'activation');
      if (isInput) inputs.push(activation.id);else {
        activation_sentences[currentLayer].push(store_activation);
        trace_sentences[currentLayer].push(store_trace);
        propagation_sentences[currentLayer].push(store_propagation);
        var old = getVar(this, 'old');
        var state = getVar(this, 'state');
        var bias = getVar(this, 'bias');
        if (this.selfconnection.gater) var self_gain = getVar(this.selfconnection, 'gain');
        if (this.selfconnected()) var self_weight = getVar(this.selfconnection, 'weight');
        buildSentence(old, ' = ', state, store_activation);
        if (this.selfconnected()) {
          if (this.selfconnection.gater) buildSentence(state, ' = ', self_gain, ' * ', self_weight, ' * ', state, ' + ', bias, store_activation);else buildSentence(state, ' = ', self_weight, ' * ', state, ' + ', bias, store_activation);
        } else buildSentence(state, ' = ', bias, store_activation);
        for (var i in this.connections.inputs) {
          var input = this.connections.inputs[i];
          var input_activation = getVar(input.from, 'activation');
          var input_weight = getVar(input, 'weight');
          if (input.gater) var input_gain = getVar(input, 'gain');
          if (this.connections.inputs[i].gater) buildSentence(state, ' += ', input_activation, ' * ', input_weight, ' * ', input_gain, store_activation);else buildSentence(state, ' += ', input_activation, ' * ', input_weight, store_activation);
        }
        var derivative = getVar(this, 'derivative');
        switch (this.squash) {
          case Neuron.squash.LOGISTIC:
            buildSentence(activation, ' = (1 / (1 + Math.exp(-', state, ')))', store_activation);
            buildSentence(derivative, ' = ', activation, ' * (1 - ', activation, ')', store_activation);
            break;
          case Neuron.squash.TANH:
            var eP = getVar('aux');
            var eN = getVar('aux_2');
            buildSentence(eP, ' = Math.exp(', state, ')', store_activation);
            buildSentence(eN, ' = 1 / ', eP, store_activation);
            buildSentence(activation, ' = (', eP, ' - ', eN, ') / (', eP, ' + ', eN, ')', store_activation);
            buildSentence(derivative, ' = 1 - (', activation, ' * ', activation, ')', store_activation);
            break;
          case Neuron.squash.IDENTITY:
            buildSentence(activation, ' = ', state, store_activation);
            buildSentence(derivative, ' = 1', store_activation);
            break;
          case Neuron.squash.HLIM:
            buildSentence(activation, ' = +(', state, ' > 0)', store_activation);
            buildSentence(derivative, ' = 1', store_activation);
            break;
          case Neuron.squash.RELU:
            buildSentence(activation, ' = ', state, ' > 0 ? ', state, ' : 0', store_activation);
            buildSentence(derivative, ' = ', state, ' > 0 ? 1 : 0', store_activation);
            break;
        }

        for (var id in this.trace.extended) {
          // calculate extended elegibility traces in advance
          var neuron = this.neighboors[id];
          var influence = getVar('influences[' + neuron.ID + ']');
          var neuron_old = getVar(neuron, 'old');
          var initialized = false;
          if (neuron.selfconnection.gater == this) {
            buildSentence(influence, ' = ', neuron_old, store_trace);
            initialized = true;
          }
          for (var incoming in this.trace.influences[neuron.ID]) {
            var incoming_weight = getVar(this.trace.influences[neuron.ID][incoming], 'weight');
            var incoming_activation = getVar(this.trace.influences[neuron.ID][incoming].from, 'activation');

            if (initialized) buildSentence(influence, ' += ', incoming_weight, ' * ', incoming_activation, store_trace);else {
              buildSentence(influence, ' = ', incoming_weight, ' * ', incoming_activation, store_trace);
              initialized = true;
            }
          }
        }

        for (var i in this.connections.inputs) {
          var input = this.connections.inputs[i];
          if (input.gater) var input_gain = getVar(input, 'gain');
          var input_activation = getVar(input.from, 'activation');
          var trace = getVar(this, 'trace', 'elegibility', input.ID, this.trace.elegibility[input.ID]);
          if (this.selfconnected()) {
            if (this.selfconnection.gater) {
              if (input.gater) buildSentence(trace, ' = ', self_gain, ' * ', self_weight, ' * ', trace, ' + ', input_gain, ' * ', input_activation, store_trace);else buildSentence(trace, ' = ', self_gain, ' * ', self_weight, ' * ', trace, ' + ', input_activation, store_trace);
            } else {
              if (input.gater) buildSentence(trace, ' = ', self_weight, ' * ', trace, ' + ', input_gain, ' * ', input_activation, store_trace);else buildSentence(trace, ' = ', self_weight, ' * ', trace, ' + ', input_activation, store_trace);
            }
          } else {
            if (input.gater) buildSentence(trace, ' = ', input_gain, ' * ', input_activation, store_trace);else buildSentence(trace, ' = ', input_activation, store_trace);
          }
          for (var id in this.trace.extended) {
            // extended elegibility trace
            var neuron = this.neighboors[id];
            var influence = getVar('influences[' + neuron.ID + ']');

            var trace = getVar(this, 'trace', 'elegibility', input.ID, this.trace.elegibility[input.ID]);
            var xtrace = getVar(this, 'trace', 'extended', neuron.ID, input.ID, this.trace.extended[neuron.ID][input.ID]);
            if (neuron.selfconnected()) var neuron_self_weight = getVar(neuron.selfconnection, 'weight');
            if (neuron.selfconnection.gater) var neuron_self_gain = getVar(neuron.selfconnection, 'gain');
            if (neuron.selfconnected()) {
              if (neuron.selfconnection.gater) buildSentence(xtrace, ' = ', neuron_self_gain, ' * ', neuron_self_weight, ' * ', xtrace, ' + ', derivative, ' * ', trace, ' * ', influence, store_trace);else buildSentence(xtrace, ' = ', neuron_self_weight, ' * ', xtrace, ' + ', derivative, ' * ', trace, ' * ', influence, store_trace);
            } else buildSentence(xtrace, ' = ', derivative, ' * ', trace, ' * ', influence, store_trace);
          }
        }
        for (var connection in this.connections.gated) {
          var gated_gain = getVar(this.connections.gated[connection], 'gain');
          buildSentence(gated_gain, ' = ', activation, store_activation);
        }
      }
      if (!isInput) {
        var responsibility = getVar(this, 'error', 'responsibility', this.error.responsibility);
        if (isOutput) {
          var target = getVar('target');
          buildSentence(responsibility, ' = ', target, ' - ', activation, store_propagation);
          for (var id in this.connections.inputs) {
            var input = this.connections.inputs[id];
            var trace = getVar(this, 'trace', 'elegibility', input.ID, this.trace.elegibility[input.ID]);
            var input_weight = getVar(input, 'weight');
            buildSentence(input_weight, ' += ', rate, ' * (', responsibility, ' * ', trace, ')', store_propagation);
          }
          outputs.push(activation.id);
        } else {
          if (!noProjections && !noGates) {
            var error = getVar('aux');
            for (var id in this.connections.projected) {
              var connection = this.connections.projected[id];
              var neuron = connection.to;
              var connection_weight = getVar(connection, 'weight');
              var neuron_responsibility = getVar(neuron, 'error', 'responsibility', neuron.error.responsibility);
              if (connection.gater) {
                var connection_gain = getVar(connection, 'gain');
                buildSentence(error, ' += ', neuron_responsibility, ' * ', connection_gain, ' * ', connection_weight, store_propagation);
              } else buildSentence(error, ' += ', neuron_responsibility, ' * ', connection_weight, store_propagation);
            }
            var projected = getVar(this, 'error', 'projected', this.error.projected);
            buildSentence(projected, ' = ', derivative, ' * ', error, store_propagation);
            buildSentence(error, ' = 0', store_propagation);
            for (var id in this.trace.extended) {
              var neuron = this.neighboors[id];
              var influence = getVar('aux_2');
              var neuron_old = getVar(neuron, 'old');
              if (neuron.selfconnection.gater == this) buildSentence(influence, ' = ', neuron_old, store_propagation);else buildSentence(influence, ' = 0', store_propagation);
              for (var input in this.trace.influences[neuron.ID]) {
                var connection = this.trace.influences[neuron.ID][input];
                var connection_weight = getVar(connection, 'weight');
                var neuron_activation = getVar(connection.from, 'activation');
                buildSentence(influence, ' += ', connection_weight, ' * ', neuron_activation, store_propagation);
              }
              var neuron_responsibility = getVar(neuron, 'error', 'responsibility', neuron.error.responsibility);
              buildSentence(error, ' += ', neuron_responsibility, ' * ', influence, store_propagation);
            }
            var gated = getVar(this, 'error', 'gated', this.error.gated);
            buildSentence(gated, ' = ', derivative, ' * ', error, store_propagation);
            buildSentence(responsibility, ' = ', projected, ' + ', gated, store_propagation);
            for (var id in this.connections.inputs) {
              var input = this.connections.inputs[id];
              var gradient = getVar('aux');
              var trace = getVar(this, 'trace', 'elegibility', input.ID, this.trace.elegibility[input.ID]);
              buildSentence(gradient, ' = ', projected, ' * ', trace, store_propagation);
              for (var id in this.trace.extended) {
                var neuron = this.neighboors[id];
                var neuron_responsibility = getVar(neuron, 'error', 'responsibility', neuron.error.responsibility);
                var xtrace = getVar(this, 'trace', 'extended', neuron.ID, input.ID, this.trace.extended[neuron.ID][input.ID]);
                buildSentence(gradient, ' += ', neuron_responsibility, ' * ', xtrace, store_propagation);
              }
              var input_weight = getVar(input, 'weight');
              buildSentence(input_weight, ' += ', rate, ' * ', gradient, store_propagation);
            }
          } else if (noGates) {
            buildSentence(responsibility, ' = 0', store_propagation);
            for (var id in this.connections.projected) {
              var connection = this.connections.projected[id];
              var neuron = connection.to;
              var connection_weight = getVar(connection, 'weight');
              var neuron_responsibility = getVar(neuron, 'error', 'responsibility', neuron.error.responsibility);
              if (connection.gater) {
                var connection_gain = getVar(connection, 'gain');
                buildSentence(responsibility, ' += ', neuron_responsibility, ' * ', connection_gain, ' * ', connection_weight, store_propagation);
              } else buildSentence(responsibility, ' += ', neuron_responsibility, ' * ', connection_weight, store_propagation);
            }
            buildSentence(responsibility, ' *= ', derivative, store_propagation);
            for (var id in this.connections.inputs) {
              var input = this.connections.inputs[id];
              var trace = getVar(this, 'trace', 'elegibility', input.ID, this.trace.elegibility[input.ID]);
              var input_weight = getVar(input, 'weight');
              buildSentence(input_weight, ' += ', rate, ' * (', responsibility, ' * ', trace, ')', store_propagation);
            }
          } else if (noProjections) {
            buildSentence(responsibility, ' = 0', store_propagation);
            for (var id in this.trace.extended) {
              var neuron = this.neighboors[id];
              var influence = getVar('aux');
              var neuron_old = getVar(neuron, 'old');
              if (neuron.selfconnection.gater == this) buildSentence(influence, ' = ', neuron_old, store_propagation);else buildSentence(influence, ' = 0', store_propagation);
              for (var input in this.trace.influences[neuron.ID]) {
                var connection = this.trace.influences[neuron.ID][input];
                var connection_weight = getVar(connection, 'weight');
                var neuron_activation = getVar(connection.from, 'activation');
                buildSentence(influence, ' += ', connection_weight, ' * ', neuron_activation, store_propagation);
              }
              var neuron_responsibility = getVar(neuron, 'error', 'responsibility', neuron.error.responsibility);
              buildSentence(responsibility, ' += ', neuron_responsibility, ' * ', influence, store_propagation);
            }
            buildSentence(responsibility, ' *= ', derivative, store_propagation);
            for (var id in this.connections.inputs) {
              var input = this.connections.inputs[id];
              var gradient = getVar('aux');
              buildSentence(gradient, ' = 0', store_propagation);
              for (var id in this.trace.extended) {
                var neuron = this.neighboors[id];
                var neuron_responsibility = getVar(neuron, 'error', 'responsibility', neuron.error.responsibility);
                var xtrace = getVar(this, 'trace', 'extended', neuron.ID, input.ID, this.trace.extended[neuron.ID][input.ID]);
                buildSentence(gradient, ' += ', neuron_responsibility, ' * ', xtrace, store_propagation);
              }
              var input_weight = getVar(input, 'weight');
              buildSentence(input_weight, ' += ', rate, ' * ', gradient, store_propagation);
            }
          }
        }
        buildSentence(bias, ' += ', rate, ' * ', responsibility, store_propagation);
      }
      return {
        memory: varID,
        neurons: neurons + 1,
        inputs: inputs,
        outputs: outputs,
        targets: targets,
        variables: variables,
        activation_sentences: activation_sentences,
        trace_sentences: trace_sentences,
        propagation_sentences: propagation_sentences,
        layers: layers
      };
    }
  }], [{
    key: 'uid',
    value: function uid() {
      return neurons++;
    }
  }, {
    key: 'quantity',
    value: function quantity() {
      return {
        neurons: neurons,
        connections: _Connection.connections
      };
    }
  }]);

  return Neuron;
}();

Neuron.squash = squash;
exports.default = Neuron;

/***/ }),
/* 3 */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


Object.defineProperty(exports, "__esModule", {
  value: true
});

var _createClass = function () { function defineProperties(target, props) { for (var i = 0; i < props.length; i++) { var descriptor = props[i]; descriptor.enumerable = descriptor.enumerable || false; descriptor.configurable = true; if ("value" in descriptor) descriptor.writable = true; Object.defineProperty(target, descriptor.key, descriptor); } } return function (Constructor, protoProps, staticProps) { if (protoProps) defineProperties(Constructor.prototype, protoProps); if (staticProps) defineProperties(Constructor, staticProps); return Constructor; }; }();

function _classCallCheck(instance, Constructor) { if (!(instance instanceof Constructor)) { throw new TypeError("Cannot call a class as a function"); } }

//+ Jonas Raoni Soares Silva
//@ http://jsfromhell.com/array/shuffle [v1.0]
function shuffleInplace(o) {
  //v1.0
  for (var j, x, i = o.length; i; j = Math.floor(Math.random() * i), x = o[--i], o[i] = o[j], o[j] = x) {}
  return o;
};

// Built-in cost functions
var cost = {
  // Eq. 9
  CROSS_ENTROPY: function CROSS_ENTROPY(target, output) {
    var crossentropy = 0;
    for (var i in output) {
      crossentropy -= target[i] * Math.log(output[i] + 1e-15) + (1 - target[i]) * Math.log(1 + 1e-15 - output[i]);
    } // +1e-15 is a tiny push away to avoid Math.log(0)
    return crossentropy;
  },
  MSE: function MSE(target, output) {
    var mse = 0;
    for (var i = 0; i < output.length; i++) {
      mse += Math.pow(target[i] - output[i], 2);
    }return mse / output.length;
  },
  BINARY: function BINARY(target, output) {
    var misses = 0;
    for (var i = 0; i < output.length; i++) {
      misses += Math.round(target[i] * 2) != Math.round(output[i] * 2);
    }return misses;
  }
};

var Trainer = function () {
  function Trainer(network, options) {
    _classCallCheck(this, Trainer);

    options = options || {};
    this.network = network;
    this.rate = options.rate || .2;
    this.iterations = options.iterations || 100000;
    this.error = options.error || .005;
    this.cost = options.cost || null;
    this.crossValidate = options.crossValidate || null;
  }

  // trains any given set to a network


  _createClass(Trainer, [{
    key: 'train',
    value: function train(set, options) {
      var error = 1;
      var iterations = bucketSize = 0;
      var abort = false;
      var currentRate;
      var cost = options && options.cost || this.cost || Trainer.cost.MSE;
      var crossValidate = false,
          testSet,
          trainSet;

      var start = Date.now();

      if (options) {
        if (options.iterations) this.iterations = options.iterations;
        if (options.error) this.error = options.error;
        if (options.rate) this.rate = options.rate;
        if (options.cost) this.cost = options.cost;
        if (options.schedule) this.schedule = options.schedule;
        if (options.customLog) {
          // for backward compatibility with code that used customLog
          console.log('Deprecated: use schedule instead of customLog');
          this.schedule = options.customLog;
        }
        if (this.crossValidate || options.crossValidate) {
          if (!this.crossValidate) this.crossValidate = {};
          crossValidate = true;
          if (options.crossValidate.testSize) this.crossValidate.testSize = options.crossValidate.testSize;
          if (options.crossValidate.testError) this.crossValidate.testError = options.crossValidate.testError;
        }
      }

      currentRate = this.rate;
      if (Array.isArray(this.rate)) {
        var bucketSize = Math.floor(this.iterations / this.rate.length);
      }

      if (crossValidate) {
        var numTrain = Math.ceil((1 - this.crossValidate.testSize) * set.length);
        trainSet = set.slice(0, numTrain);
        testSet = set.slice(numTrain);
      }

      var lastError = 0;
      while (!abort && iterations < this.iterations && error > this.error) {
        if (crossValidate && error <= this.crossValidate.testError) {
          break;
        }

        var currentSetSize = set.length;
        error = 0;
        iterations++;

        if (bucketSize > 0) {
          var currentBucket = Math.floor(iterations / bucketSize);
          currentRate = this.rate[currentBucket] || currentRate;
        }

        if (typeof this.rate === 'function') {
          currentRate = this.rate(iterations, lastError);
        }

        if (crossValidate) {
          this._trainSet(trainSet, currentRate, cost);
          error += this.test(testSet).error;
          currentSetSize = 1;
        } else {
          error += this._trainSet(set, currentRate, cost);
          currentSetSize = set.length;
        }

        // check error
        error /= currentSetSize;
        lastError = error;

        if (options) {
          if (this.schedule && this.schedule.every && iterations % this.schedule.every == 0) abort = this.schedule.do({ error: error, iterations: iterations, rate: currentRate });else if (options.log && iterations % options.log == 0) {
            console.log('iterations', iterations, 'error', error, 'rate', currentRate);
          }
          ;
          if (options.shuffle) shuffleInplace(set);
        }
      }

      var results = {
        error: error,
        iterations: iterations,
        time: Date.now() - start
      };

      return results;
    }

    // trains any given set to a network, using a WebWorker (only for the browser). Returns a Promise of the results.

  }, {
    key: 'trainAsync',
    value: function trainAsync(set, options) {
      var train = this.workerTrain.bind(this);
      return new Promise(function (resolve, reject) {
        try {
          train(set, resolve, options, true);
        } catch (e) {
          reject(e);
        }
      });
    }

    // preforms one training epoch and returns the error (private function used in this.train)

  }, {
    key: '_trainSet',
    value: function _trainSet(set, currentRate, costFunction) {
      var errorSum = 0;
      for (var i = 0; i < set.length; i++) {
        var input = set[i].input;
        var target = set[i].output;

        var output = this.network.activate(input);
        this.network.propagate(currentRate, target);

        errorSum += costFunction(target, output);
      }
      return errorSum;
    }

    // tests a set and returns the error and elapsed time

  }, {
    key: 'test',
    value: function test(set, options) {
      var error = 0;
      var input, output, target;
      var cost = options && options.cost || this.cost || Trainer.cost.MSE;

      var start = Date.now();

      for (var i = 0; i < set.length; i++) {
        input = set[i].input;
        target = set[i].output;
        output = this.network.activate(input);
        error += cost(target, output);
      }

      error /= set.length;

      var results = {
        error: error,
        time: Date.now() - start
      };

      return results;
    }

    // trains any given set to a network using a WebWorker [deprecated: use trainAsync instead]

  }, {
    key: 'workerTrain',
    value: function workerTrain(set, callback, options, suppressWarning) {
      if (!suppressWarning) {
        console.warn('Deprecated: do not use `workerTrain`, use `trainAsync` instead.');
      }
      var that = this;

      if (!this.network.optimized) this.network.optimize();

      // Create a new worker
      var worker = this.network.worker(this.network.optimized.memory, set, options);

      // train the worker
      worker.onmessage = function (e) {
        switch (e.data.action) {
          case 'done':
            var iterations = e.data.message.iterations;
            var error = e.data.message.error;
            var time = e.data.message.time;

            that.network.optimized.ownership(e.data.memoryBuffer);

            // Done callback
            callback({
              error: error,
              iterations: iterations,
              time: time
            });

            // Delete the worker and all its associated memory
            worker.terminate();
            break;

          case 'log':
            console.log(e.data.message);

          case 'schedule':
            if (options && options.schedule && typeof options.schedule.do === 'function') {
              var scheduled = options.schedule.do;
              scheduled(e.data.message);
            }
            break;
        }
      };

      // Start the worker
      worker.postMessage({ action: 'startTraining' });
    }

    // trains an XOR to the network

  }, {
    key: 'XOR',
    value: function XOR(options) {
      if (this.network.inputs() != 2 || this.network.outputs() != 1) throw new Error('Incompatible network (2 inputs, 1 output)');

      var defaults = {
        iterations: 100000,
        log: false,
        shuffle: true,
        cost: Trainer.cost.MSE
      };

      if (options) for (var i in options) {
        defaults[i] = options[i];
      }return this.train([{
        input: [0, 0],
        output: [0]
      }, {
        input: [1, 0],
        output: [1]
      }, {
        input: [0, 1],
        output: [1]
      }, {
        input: [1, 1],
        output: [0]
      }], defaults);
    }

    // trains the network to pass a Distracted Sequence Recall test

  }, {
    key: 'DSR',
    value: function DSR(options) {
      options = options || {};

      var targets = options.targets || [2, 4, 7, 8];
      var distractors = options.distractors || [3, 5, 6, 9];
      var prompts = options.prompts || [0, 1];
      var length = options.length || 24;
      var criterion = options.success || 0.95;
      var iterations = options.iterations || 100000;
      var rate = options.rate || .1;
      var log = options.log || 0;
      var schedule = options.schedule || {};
      var cost = options.cost || this.cost || Trainer.cost.CROSS_ENTROPY;

      var trial, correct, i, j, success;
      trial = correct = i = j = success = 0;
      var error = 1,
          symbols = targets.length + distractors.length + prompts.length;

      var noRepeat = function noRepeat(range, avoid) {
        var number = Math.random() * range | 0;
        var used = false;
        for (var i in avoid) {
          if (number == avoid[i]) used = true;
        }return used ? noRepeat(range, avoid) : number;
      };

      var equal = function equal(prediction, output) {
        for (var i in prediction) {
          if (Math.round(prediction[i]) != output[i]) return false;
        }return true;
      };

      var start = Date.now();

      while (trial < iterations && (success < criterion || trial % 1000 != 0)) {
        // generate sequence
        var sequence = [],
            sequenceLength = length - prompts.length;
        for (i = 0; i < sequenceLength; i++) {
          var any = Math.random() * distractors.length | 0;
          sequence.push(distractors[any]);
        }
        var indexes = [],
            positions = [];
        for (i = 0; i < prompts.length; i++) {
          indexes.push(Math.random() * targets.length | 0);
          positions.push(noRepeat(sequenceLength, positions));
        }
        positions = positions.sort();
        for (i = 0; i < prompts.length; i++) {
          sequence[positions[i]] = targets[indexes[i]];
          sequence.push(prompts[i]);
        }

        //train sequence
        var distractorsCorrect;
        var targetsCorrect = distractorsCorrect = 0;
        error = 0;
        for (i = 0; i < length; i++) {
          // generate input from sequence
          var input = [];
          for (j = 0; j < symbols; j++) {
            input[j] = 0;
          }input[sequence[i]] = 1;

          // generate target output
          var output = [];
          for (j = 0; j < targets.length; j++) {
            output[j] = 0;
          }if (i >= sequenceLength) {
            var index = i - sequenceLength;
            output[indexes[index]] = 1;
          }

          // check result
          var prediction = this.network.activate(input);

          if (equal(prediction, output)) {
            if (i < sequenceLength) distractorsCorrect++;else targetsCorrect++;
          } else {
            this.network.propagate(rate, output);
          }

          error += cost(output, prediction);

          if (distractorsCorrect + targetsCorrect == length) correct++;
        }

        // calculate error
        if (trial % 1000 == 0) correct = 0;
        trial++;
        var divideError = trial % 1000;
        divideError = divideError == 0 ? 1000 : divideError;
        success = correct / divideError;
        error /= length;

        // log
        if (log && trial % log == 0) console.log('iterations:', trial, ' success:', success, ' correct:', correct, ' time:', Date.now() - start, ' error:', error);
        if (schedule.do && schedule.every && trial % schedule.every == 0) schedule.do({
          iterations: trial,
          success: success,
          error: error,
          time: Date.now() - start,
          correct: correct
        });
      }

      return {
        iterations: trial,
        success: success,
        error: error,
        time: Date.now() - start
      };
    }

    // train the network to learn an Embeded Reber Grammar

  }, {
    key: 'ERG',
    value: function ERG(options) {

      options = options || {};
      var iterations = options.iterations || 150000;
      var criterion = options.error || .05;
      var rate = options.rate || .1;
      var log = options.log || 500;
      var cost = options.cost || this.cost || Trainer.cost.CROSS_ENTROPY;

      // gramar node
      var Node = function Node() {
        this.paths = [];
      };
      Node.prototype = {
        connect: function connect(node, value) {
          this.paths.push({
            node: node,
            value: value
          });
          return this;
        },
        any: function any() {
          if (this.paths.length == 0) return false;
          var index = Math.random() * this.paths.length | 0;
          return this.paths[index];
        },
        test: function test(value) {
          for (var i in this.paths) {
            if (this.paths[i].value == value) return this.paths[i];
          }return false;
        }
      };

      var reberGrammar = function reberGrammar() {

        // build a reber grammar
        var output = new Node();
        var n1 = new Node().connect(output, 'E');
        var n2 = new Node().connect(n1, 'S');
        var n3 = new Node().connect(n1, 'V').connect(n2, 'P');
        var n4 = new Node().connect(n2, 'X');
        n4.connect(n4, 'S');
        var n5 = new Node().connect(n3, 'V');
        n5.connect(n5, 'T');
        n2.connect(n5, 'X');
        var n6 = new Node().connect(n4, 'T').connect(n5, 'P');
        var input = new Node().connect(n6, 'B');

        return {
          input: input,
          output: output
        };
      };

      // build an embeded reber grammar
      var embededReberGrammar = function embededReberGrammar() {
        var reber1 = reberGrammar();
        var reber2 = reberGrammar();

        var output = new Node();
        var n1 = new Node().connect(output, 'E');
        reber1.output.connect(n1, 'T');
        reber2.output.connect(n1, 'P');
        var n2 = new Node().connect(reber1.input, 'P').connect(reber2.input, 'T');
        var input = new Node().connect(n2, 'B');

        return {
          input: input,
          output: output
        };
      };

      // generate an ERG sequence
      var generate = function generate() {
        var node = embededReberGrammar().input;
        var next = node.any();
        var str = '';
        while (next) {
          str += next.value;
          next = next.node.any();
        }
        return str;
      };

      // test if a string matches an embeded reber grammar
      var test = function test(str) {
        var node = embededReberGrammar().input;
        var i = 0;
        var ch = str.charAt(i);
        while (i < str.length) {
          var next = node.test(ch);
          if (!next) return false;
          node = next.node;
          ch = str.charAt(++i);
        }
        return true;
      };

      // helper to check if the output and the target vectors match
      var different = function different(array1, array2) {
        var max1 = 0;
        var i1 = -1;
        var max2 = 0;
        var i2 = -1;
        for (var i in array1) {
          if (array1[i] > max1) {
            max1 = array1[i];
            i1 = i;
          }
          if (array2[i] > max2) {
            max2 = array2[i];
            i2 = i;
          }
        }

        return i1 != i2;
      };

      var iteration = 0;
      var error = 1;
      var table = {
        'B': 0,
        'P': 1,
        'T': 2,
        'X': 3,
        'S': 4,
        'E': 5
      };

      var start = Date.now();
      while (iteration < iterations && error > criterion) {
        var i = 0;
        error = 0;

        // ERG sequence to learn
        var sequence = generate();

        // input
        var read = sequence.charAt(i);
        // target
        var predict = sequence.charAt(i + 1);

        // train
        while (i < sequence.length - 1) {
          var input = [];
          var target = [];
          for (var j = 0; j < 6; j++) {
            input[j] = 0;
            target[j] = 0;
          }
          input[table[read]] = 1;
          target[table[predict]] = 1;

          var output = this.network.activate(input);

          if (different(output, target)) this.network.propagate(rate, target);

          read = sequence.charAt(++i);
          predict = sequence.charAt(i + 1);

          error += cost(target, output);
        }
        error /= sequence.length;
        iteration++;
        if (iteration % log == 0) {
          console.log('iterations:', iteration, ' time:', Date.now() - start, ' error:', error);
        }
      }

      return {
        iterations: iteration,
        error: error,
        time: Date.now() - start,
        test: test,
        generate: generate
      };
    }
  }, {
    key: 'timingTask',
    value: function timingTask(options) {

      if (this.network.inputs() != 2 || this.network.outputs() != 1) throw new Error('Invalid Network: must have 2 inputs and one output');

      if (typeof options == 'undefined') options = {};

      // helper
      function getSamples(trainingSize, testSize) {

        // sample size
        var size = trainingSize + testSize;

        // generate samples
        var t = 0;
        var set = [];
        for (var i = 0; i < size; i++) {
          set.push({ input: [0, 0], output: [0] });
        }
        while (t < size - 20) {
          var n = Math.round(Math.random() * 20);
          set[t].input[0] = 1;
          for (var j = t; j <= t + n; j++) {
            set[j].input[1] = n / 20;
            set[j].output[0] = 0.5;
          }
          t += n;
          n = Math.round(Math.random() * 20);
          for (var k = t + 1; k <= t + n && k < size; k++) {
            set[k].input[1] = set[t].input[1];
          }t += n;
        }

        // separate samples between train and test sets
        var trainingSet = [];
        var testSet = [];
        for (var l = 0; l < size; l++) {
          (l < trainingSize ? trainingSet : testSet).push(set[l]);
        } // return samples
        return {
          train: trainingSet,
          test: testSet
        };
      }

      var iterations = options.iterations || 200;
      var error = options.error || .005;
      var rate = options.rate || [.03, .02];
      var log = options.log === false ? false : options.log || 10;
      var cost = options.cost || this.cost || Trainer.cost.MSE;
      var trainingSamples = options.trainSamples || 7000;
      var testSamples = options.trainSamples || 1000;

      // samples for training and testing
      var samples = getSamples(trainingSamples, testSamples);

      // train
      var result = this.train(samples.train, {
        rate: rate,
        log: log,
        iterations: iterations,
        error: error,
        cost: cost
      });

      return {
        train: result,
        test: this.test(samples.test)
      };
    }
  }]);

  return Trainer;
}();

Trainer.cost = cost;
exports.default = Trainer;

/***/ }),
/* 4 */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


Object.defineProperty(exports, "__esModule", {
  value: true
});
exports.Architect = exports.Network = exports.Trainer = exports.Layer = exports.Neuron = undefined;

var _Neuron = __webpack_require__(2);

Object.defineProperty(exports, 'Neuron', {
  enumerable: true,
  get: function get() {
    return _interopRequireDefault(_Neuron).default;
  }
});

var _Layer = __webpack_require__(0);

Object.defineProperty(exports, 'Layer', {
  enumerable: true,
  get: function get() {
    return _interopRequireDefault(_Layer).default;
  }
});

var _Trainer = __webpack_require__(3);

Object.defineProperty(exports, 'Trainer', {
  enumerable: true,
  get: function get() {
    return _interopRequireDefault(_Trainer).default;
  }
});

var _Network = __webpack_require__(1);

Object.defineProperty(exports, 'Network', {
  enumerable: true,
  get: function get() {
    return _interopRequireDefault(_Network).default;
  }
});

var _architect = __webpack_require__(7);

var Architect = _interopRequireWildcard(_architect);

function _interopRequireWildcard(obj) { if (obj && obj.__esModule) { return obj; } else { var newObj = {}; if (obj != null) { for (var key in obj) { if (Object.prototype.hasOwnProperty.call(obj, key)) newObj[key] = obj[key]; } } newObj.default = obj; return newObj; } }

function _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }

exports.Architect = Architect;

/***/ }),
/* 5 */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


Object.defineProperty(exports, "__esModule", {
  value: true
});

var _createClass = function () { function defineProperties(target, props) { for (var i = 0; i < props.length; i++) { var descriptor = props[i]; descriptor.enumerable = descriptor.enumerable || false; descriptor.configurable = true; if ("value" in descriptor) descriptor.writable = true; Object.defineProperty(target, descriptor.key, descriptor); } } return function (Constructor, protoProps, staticProps) { if (protoProps) defineProperties(Constructor.prototype, protoProps); if (staticProps) defineProperties(Constructor, staticProps); return Constructor; }; }();

function _classCallCheck(instance, Constructor) { if (!(instance instanceof Constructor)) { throw new TypeError("Cannot call a class as a function"); } }

var connections = exports.connections = 0;

var Connection = function () {
  function Connection(from, to, weight) {
    _classCallCheck(this, Connection);

    if (!from || !to) throw new Error("Connection Error: Invalid neurons");

    this.ID = Connection.uid();
    this.from = from;
    this.to = to;
    this.weight = typeof weight == 'undefined' ? Math.random() * .2 - .1 : weight;
    this.gain = 1;
    this.gater = null;
  }

  _createClass(Connection, null, [{
    key: "uid",
    value: function uid() {
      return exports.connections = connections += 1, connections - 1;
    }
  }]);

  return Connection;
}();

exports.default = Connection;

/***/ }),
/* 6 */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


Object.defineProperty(exports, "__esModule", {
  value: true
});
exports.connections = undefined;

var _createClass = function () { function defineProperties(target, props) { for (var i = 0; i < props.length; i++) { var descriptor = props[i]; descriptor.enumerable = descriptor.enumerable || false; descriptor.configurable = true; if ("value" in descriptor) descriptor.writable = true; Object.defineProperty(target, descriptor.key, descriptor); } } return function (Constructor, protoProps, staticProps) { if (protoProps) defineProperties(Constructor.prototype, protoProps); if (staticProps) defineProperties(Constructor, staticProps); return Constructor; }; }();

var _Layer = __webpack_require__(0);

var _Layer2 = _interopRequireDefault(_Layer);

function _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }

function _classCallCheck(instance, Constructor) { if (!(instance instanceof Constructor)) { throw new TypeError("Cannot call a class as a function"); } }

// represents a connection from one layer to another, and keeps track of its weight and gain
var connections = exports.connections = 0;

var LayerConnection = function () {
  function LayerConnection(fromLayer, toLayer, type, weights) {
    _classCallCheck(this, LayerConnection);

    this.ID = LayerConnection.uid();
    this.from = fromLayer;
    this.to = toLayer;
    this.selfconnection = toLayer == fromLayer;
    this.type = type;
    this.connections = {};
    this.list = [];
    this.size = 0;
    this.gatedfrom = [];

    if (typeof this.type == 'undefined') {
      if (fromLayer == toLayer) this.type = _Layer2.default.connectionType.ONE_TO_ONE;else this.type = _Layer2.default.connectionType.ALL_TO_ALL;
    }

    if (this.type == _Layer2.default.connectionType.ALL_TO_ALL || this.type == _Layer2.default.connectionType.ALL_TO_ELSE) {
      for (var here in this.from.list) {
        for (var there in this.to.list) {
          var from = this.from.list[here];
          var to = this.to.list[there];
          if (this.type == _Layer2.default.connectionType.ALL_TO_ELSE && from == to) continue;
          var connection = from.project(to, weights);

          this.connections[connection.ID] = connection;
          this.size = this.list.push(connection);
        }
      }
    } else if (this.type == _Layer2.default.connectionType.ONE_TO_ONE) {

      for (var neuron in this.from.list) {
        var from = this.from.list[neuron];
        var to = this.to.list[neuron];
        var connection = from.project(to, weights);

        this.connections[connection.ID] = connection;
        this.size = this.list.push(connection);
      }
    }

    fromLayer.connectedTo.push(this);
  }

  _createClass(LayerConnection, null, [{
    key: 'uid',
    value: function uid() {
      return exports.connections = connections += 1, connections - 1;
    }
  }]);

  return LayerConnection;
}();

exports.default = LayerConnection;

/***/ }),
/* 7 */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


Object.defineProperty(exports, "__esModule", {
  value: true
});

var _Perceptron = __webpack_require__(8);

Object.defineProperty(exports, 'Perceptron', {
  enumerable: true,
  get: function get() {
    return _interopRequireDefault(_Perceptron).default;
  }
});

var _LSTM = __webpack_require__(9);

Object.defineProperty(exports, 'LSTM', {
  enumerable: true,
  get: function get() {
    return _interopRequireDefault(_LSTM).default;
  }
});

var _Liquid = __webpack_require__(10);

Object.defineProperty(exports, 'Liquid', {
  enumerable: true,
  get: function get() {
    return _interopRequireDefault(_Liquid).default;
  }
});

var _Hopfield = __webpack_require__(11);

Object.defineProperty(exports, 'Hopfield', {
  enumerable: true,
  get: function get() {
    return _interopRequireDefault(_Hopfield).default;
  }
});

function _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }

/***/ }),
/* 8 */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


Object.defineProperty(exports, "__esModule", {
  value: true
});

var _Network2 = __webpack_require__(1);

var _Network3 = _interopRequireDefault(_Network2);

var _Layer = __webpack_require__(0);

var _Layer2 = _interopRequireDefault(_Layer);

function _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }

function _classCallCheck(instance, Constructor) { if (!(instance instanceof Constructor)) { throw new TypeError("Cannot call a class as a function"); } }

function _possibleConstructorReturn(self, call) { if (!self) { throw new ReferenceError("this hasn't been initialised - super() hasn't been called"); } return call && (typeof call === "object" || typeof call === "function") ? call : self; }

function _inherits(subClass, superClass) { if (typeof superClass !== "function" && superClass !== null) { throw new TypeError("Super expression must either be null or a function, not " + typeof superClass); } subClass.prototype = Object.create(superClass && superClass.prototype, { constructor: { value: subClass, enumerable: false, writable: true, configurable: true } }); if (superClass) Object.setPrototypeOf ? Object.setPrototypeOf(subClass, superClass) : subClass.__proto__ = superClass; }

var Perceptron = function (_Network) {
  _inherits(Perceptron, _Network);

  function Perceptron() {
    _classCallCheck(this, Perceptron);

    var _this = _possibleConstructorReturn(this, (Perceptron.__proto__ || Object.getPrototypeOf(Perceptron)).call(this));

    var args = Array.prototype.slice.call(arguments); // convert arguments to Array
    if (args.length < 3) throw new Error('not enough layers (minimum 3) !!');

    var inputs = args.shift(); // first argument
    var outputs = args.pop(); // last argument
    var layers = args; // all the arguments in the middle

    var input = new _Layer2.default(inputs);
    var hidden = [];
    var output = new _Layer2.default(outputs);

    var previous = input;

    // generate hidden layers
    for (var i = 0; i < layers.length; i++) {
      var size = layers[i];
      var layer = new _Layer2.default(size);
      hidden.push(layer);
      previous.project(layer);
      previous = layer;
    }
    previous.project(output);

    // set layers of the neural network
    _this.set({
      input: input,
      hidden: hidden,
      output: output
    });
    return _this;
  }

  return Perceptron;
}(_Network3.default);

exports.default = Perceptron;

/***/ }),
/* 9 */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


Object.defineProperty(exports, "__esModule", {
  value: true
});

var _Network2 = __webpack_require__(1);

var _Network3 = _interopRequireDefault(_Network2);

var _Layer = __webpack_require__(0);

var _Layer2 = _interopRequireDefault(_Layer);

function _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }

function _classCallCheck(instance, Constructor) { if (!(instance instanceof Constructor)) { throw new TypeError("Cannot call a class as a function"); } }

function _possibleConstructorReturn(self, call) { if (!self) { throw new ReferenceError("this hasn't been initialised - super() hasn't been called"); } return call && (typeof call === "object" || typeof call === "function") ? call : self; }

function _inherits(subClass, superClass) { if (typeof superClass !== "function" && superClass !== null) { throw new TypeError("Super expression must either be null or a function, not " + typeof superClass); } subClass.prototype = Object.create(superClass && superClass.prototype, { constructor: { value: subClass, enumerable: false, writable: true, configurable: true } }); if (superClass) Object.setPrototypeOf ? Object.setPrototypeOf(subClass, superClass) : subClass.__proto__ = superClass; }

var LSTM = function (_Network) {
  _inherits(LSTM, _Network);

  function LSTM() {
    _classCallCheck(this, LSTM);

    var _this = _possibleConstructorReturn(this, (LSTM.__proto__ || Object.getPrototypeOf(LSTM)).call(this));

    var args = Array.prototype.slice.call(arguments); // convert arguments to array
    if (args.length < 3) throw new Error("not enough layers (minimum 3) !!");

    var last = args.pop();
    var option = {
      peepholes: _Layer2.default.connectionType.ALL_TO_ALL,
      hiddenToHidden: false,
      outputToHidden: false,
      outputToGates: false,
      inputToOutput: true
    };
    if (typeof last != 'number') {
      var outputs = args.pop();
      if (last.hasOwnProperty('peepholes')) option.peepholes = last.peepholes;
      if (last.hasOwnProperty('hiddenToHidden')) option.hiddenToHidden = last.hiddenToHidden;
      if (last.hasOwnProperty('outputToHidden')) option.outputToHidden = last.outputToHidden;
      if (last.hasOwnProperty('outputToGates')) option.outputToGates = last.outputToGates;
      if (last.hasOwnProperty('inputToOutput')) option.inputToOutput = last.inputToOutput;
    } else {
      var outputs = last;
    }

    var inputs = args.shift();
    var layers = args;

    var inputLayer = new _Layer2.default(inputs);
    var hiddenLayers = [];
    var outputLayer = new _Layer2.default(outputs);

    var previous = null;

    // generate layers
    for (var i = 0; i < layers.length; i++) {
      // generate memory blocks (memory cell and respective gates)
      var size = layers[i];

      var inputGate = new _Layer2.default(size).set({
        bias: 1
      });
      var forgetGate = new _Layer2.default(size).set({
        bias: 1
      });
      var memoryCell = new _Layer2.default(size);
      var outputGate = new _Layer2.default(size).set({
        bias: 1
      });

      hiddenLayers.push(inputGate);
      hiddenLayers.push(forgetGate);
      hiddenLayers.push(memoryCell);
      hiddenLayers.push(outputGate);

      // connections from input layer
      var input = inputLayer.project(memoryCell);
      inputLayer.project(inputGate);
      inputLayer.project(forgetGate);
      inputLayer.project(outputGate);

      // connections from previous memory-block layer to this one
      if (previous != null) {
        var cell = previous.project(memoryCell);
        previous.project(inputGate);
        previous.project(forgetGate);
        previous.project(outputGate);
      }

      // connections from memory cell
      var output = memoryCell.project(outputLayer);

      // self-connection
      var self = memoryCell.project(memoryCell);

      // hidden to hidden recurrent connection
      if (option.hiddenToHidden) memoryCell.project(memoryCell, _Layer2.default.connectionType.ALL_TO_ELSE);

      // out to hidden recurrent connection
      if (option.outputToHidden) outputLayer.project(memoryCell);

      // out to gates recurrent connection
      if (option.outputToGates) {
        outputLayer.project(inputGate);
        outputLayer.project(outputGate);
        outputLayer.project(forgetGate);
      }

      // peepholes
      memoryCell.project(inputGate, option.peepholes);
      memoryCell.project(forgetGate, option.peepholes);
      memoryCell.project(outputGate, option.peepholes);

      // gates
      inputGate.gate(input, _Layer2.default.gateType.INPUT);
      forgetGate.gate(self, _Layer2.default.gateType.ONE_TO_ONE);
      outputGate.gate(output, _Layer2.default.gateType.OUTPUT);
      if (previous != null) inputGate.gate(cell, _Layer2.default.gateType.INPUT);

      previous = memoryCell;
    }

    // input to output direct connection
    if (option.inputToOutput) inputLayer.project(outputLayer);

    // set the layers of the neural network
    _this.set({
      input: inputLayer,
      hidden: hiddenLayers,
      output: outputLayer
    });
    return _this;
  }

  return LSTM;
}(_Network3.default);

exports.default = LSTM;

/***/ }),
/* 10 */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


Object.defineProperty(exports, "__esModule", {
  value: true
});

var _Network2 = __webpack_require__(1);

var _Network3 = _interopRequireDefault(_Network2);

var _Layer = __webpack_require__(0);

var _Layer2 = _interopRequireDefault(_Layer);

function _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }

function _classCallCheck(instance, Constructor) { if (!(instance instanceof Constructor)) { throw new TypeError("Cannot call a class as a function"); } }

function _possibleConstructorReturn(self, call) { if (!self) { throw new ReferenceError("this hasn't been initialised - super() hasn't been called"); } return call && (typeof call === "object" || typeof call === "function") ? call : self; }

function _inherits(subClass, superClass) { if (typeof superClass !== "function" && superClass !== null) { throw new TypeError("Super expression must either be null or a function, not " + typeof superClass); } subClass.prototype = Object.create(superClass && superClass.prototype, { constructor: { value: subClass, enumerable: false, writable: true, configurable: true } }); if (superClass) Object.setPrototypeOf ? Object.setPrototypeOf(subClass, superClass) : subClass.__proto__ = superClass; }

var Liquid = function (_Network) {
  _inherits(Liquid, _Network);

  function Liquid(inputs, hidden, outputs, connections, gates) {
    _classCallCheck(this, Liquid);

    // create layers
    var _this = _possibleConstructorReturn(this, (Liquid.__proto__ || Object.getPrototypeOf(Liquid)).call(this));

    var inputLayer = new _Layer2.default(inputs);
    var hiddenLayer = new _Layer2.default(hidden);
    var outputLayer = new _Layer2.default(outputs);

    // make connections and gates randomly among the neurons
    var neurons = hiddenLayer.neurons();
    var connectionList = [];

    for (var i = 0; i < connections; i++) {
      // connect two random neurons
      var from = Math.random() * neurons.length | 0;
      var to = Math.random() * neurons.length | 0;
      var connection = neurons[from].project(neurons[to]);
      connectionList.push(connection);
    }

    for (var j = 0; j < gates; j++) {
      // pick a random gater neuron
      var gater = Math.random() * neurons.length | 0;
      // pick a random connection to gate
      var connection = Math.random() * connectionList.length | 0;
      // let the gater gate the connection
      neurons[gater].gate(connectionList[connection]);
    }

    // connect the layers
    inputLayer.project(hiddenLayer);
    hiddenLayer.project(outputLayer);

    // set the layers of the network
    _this.set({
      input: inputLayer,
      hidden: [hiddenLayer],
      output: outputLayer
    });
    return _this;
  }

  return Liquid;
}(_Network3.default);

exports.default = Liquid;

/***/ }),
/* 11 */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


Object.defineProperty(exports, "__esModule", {
  value: true
});

var _createClass = function () { function defineProperties(target, props) { for (var i = 0; i < props.length; i++) { var descriptor = props[i]; descriptor.enumerable = descriptor.enumerable || false; descriptor.configurable = true; if ("value" in descriptor) descriptor.writable = true; Object.defineProperty(target, descriptor.key, descriptor); } } return function (Constructor, protoProps, staticProps) { if (protoProps) defineProperties(Constructor.prototype, protoProps); if (staticProps) defineProperties(Constructor, staticProps); return Constructor; }; }();

var _Network2 = __webpack_require__(1);

var _Network3 = _interopRequireDefault(_Network2);

var _Trainer = __webpack_require__(3);

var _Trainer2 = _interopRequireDefault(_Trainer);

var _Layer = __webpack_require__(0);

var _Layer2 = _interopRequireDefault(_Layer);

function _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }

function _classCallCheck(instance, Constructor) { if (!(instance instanceof Constructor)) { throw new TypeError("Cannot call a class as a function"); } }

function _possibleConstructorReturn(self, call) { if (!self) { throw new ReferenceError("this hasn't been initialised - super() hasn't been called"); } return call && (typeof call === "object" || typeof call === "function") ? call : self; }

function _inherits(subClass, superClass) { if (typeof superClass !== "function" && superClass !== null) { throw new TypeError("Super expression must either be null or a function, not " + typeof superClass); } subClass.prototype = Object.create(superClass && superClass.prototype, { constructor: { value: subClass, enumerable: false, writable: true, configurable: true } }); if (superClass) Object.setPrototypeOf ? Object.setPrototypeOf(subClass, superClass) : subClass.__proto__ = superClass; }

var Hopfield = function (_Network) {
  _inherits(Hopfield, _Network);

  function Hopfield(size) {
    _classCallCheck(this, Hopfield);

    var _this = _possibleConstructorReturn(this, (Hopfield.__proto__ || Object.getPrototypeOf(Hopfield)).call(this));

    var inputLayer = new _Layer2.default(size);
    var outputLayer = new _Layer2.default(size);

    inputLayer.project(outputLayer, _Layer2.default.connectionType.ALL_TO_ALL);

    _this.set({
      input: inputLayer,
      hidden: [],
      output: outputLayer
    });

    _this.trainer = new _Trainer2.default(_this);
    return _this;
  }

  _createClass(Hopfield, [{
    key: 'learn',
    value: function learn(patterns) {
      var set = [];
      for (var p in patterns) {
        set.push({
          input: patterns[p],
          output: patterns[p]
        });
      }return this.trainer.train(set, {
        iterations: 500000,
        error: .00005,
        rate: 1
      });
    }
  }, {
    key: 'feed',
    value: function feed(pattern) {
      var output = this.activate(pattern);

      var pattern = [];
      for (var i in output) {
        pattern[i] = output[i] > .5 ? 1 : 0;
      }return pattern;
    }
  }]);

  return Hopfield;
}(_Network3.default);

exports.default = Hopfield;

/***/ })
/******/ ]);
});
},{}]},{},[8]);
